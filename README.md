# Ray Tracing in One Weekend




![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/zfk3Q1tuLECywHO.jpg)






### å¼•è¨€

[ã€ŠRay Tracing In One Weekendã€‹](https://raytracing.github.io/books/RayTracingInOneWeekend.html)ï¼ˆã€Šä¸€å‘¨æœ«æå®šå…‰çº¿è¿½è¸ªã€‹ï¼‰ï¼Œ ç”±Peter Shirleyï¼ˆå°±æ˜¯é‚£æœ¬å›¾å½¢å­¦è™ä¹¦çš„ä½œè€…ï¼‰æ‰€ç¼–å†™çš„çš„è½¯æ¸²å…‰è¿½ä¸‰éƒ¨æ›²ç¬¬ä¸€æœ¬, æ˜¯ä¸€æœ¬éå¸¸å¥½çš„å…¥é—¨çº§ä¹¦ç±, ç¯‡å¹…ä¸å¤š, ä¸€å…±åªæœ‰54é¡µ, é€‚åˆæ–°æ‰‹å­¦ä¹ ã€‚





> åŸæ–‡æºè‡ª[Ray Tracing in One Weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html)
>
> æœ¬æ–‡è½¬è½½è‡ª[ray tracing in one weekend ä¸­æ–‡ç¿»è¯‘](https://matrix4f.com/Graphic/ray-tracing-in-one-weekend/)çš„ç¿»è¯‘,å¦‚æœæ‚¨æ„¿æ„å¸®åŠ©è¯‘è€…æ”¹è¿›è¿™ä¸ªç¿»è¯‘, è¯·ç›´æ¥**å‘é€é‚®ä»¶**åˆ°[zgxmy@126.com](mailto:zgxmy@126.com), ä¸‡åˆ†æ„Ÿæ¿€



### æ¦‚è¿°

> Iâ€™ve taught many graphics classes over the years. Often I do them in ray tracing, because you are forced to write all the code, but you can still get cool images with no API. I decided to adapt my course notes into a how-to, to get you to a cool program as quickly as possible. It will not be a full-featured ray tracer, but it does have the indirect lighting which has made ray tracing a staple in movies. Follow these steps, and the architecture of the ray tracer you produce will be good for extending to a more extensive ray tracer if you get excited and want to pursue that.

è¿™äº›å¹´æ¥æˆ‘å¼€è¿‡ä¸å°‘å›¾å½¢å­¦çš„è¯¾ã€‚æˆ‘å¸¸å¸¸æŠŠå…‰çº¿è¿½è¸ªä½œä¸ºè¯¾å ‚ä¸Šçš„æ•™å­¦å†…å®¹ã€‚å› ä¸ºå¯¹äºå…‰çº¿è¿½è¸ªæ¥è¯´, åœ¨ä¸ä½¿ç”¨ä»»ä½•APIçš„æƒ…å†µä¸‹, ä½ ä¸å¾—ä¸è¢«è¿«æ‰‹æ’¸å…¨éƒ¨çš„ä»£ç , ä½†ä½ ä»ç„¶èƒ½æ¸²æŸ“å‡ºç‚«é…·çš„å›¾ç‰‡ã€‚æˆ‘å†³å®šå°†æˆ‘çš„è¯¾å ‚ç¬”è®°æ”¹å†™æˆæœ¬æ•™ç¨‹, è®©ä½ èƒ½å°½å¯èƒ½å¿«çš„å®ç°ä¸€ä¸ªç‚«é…·çš„å…‰çº¿è¿½è¸ªå™¨ï¼ˆray tracerï¼‰ã€‚è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„å…‰çº¿è¿½è¸ªå™¨ã€‚ä½†æ˜¯å®ƒå´æ‹¥ç”¨è®©å…‰çº¿è¿½è¸ªåœ¨ç”µå½±è¡Œä¸šæˆä¸ºä¸»æµçš„éç›´æ¥å…‰ç…§ï¼ˆindirect lightingï¼‰ã€‚è·Ÿéšæœ¬æ•™ç¨‹å¾ªåºæ¸è¿›, ä½ çš„å…‰çº¿è¿½è¸ªå™¨çš„ä»£ç æ„ç­‘å°†ä¼šå˜å¾—æ˜“äºæ‹“å±•ã€‚å¦‚æœä¹‹åä½ å¯¹è¿™æ–¹é¢ç‡ƒèµ·äº†å…´è¶£, ä½ å¯ä»¥å°†å®ƒæ‹“å±•æˆä¸€ä¸ªæ›´åŠ å®Œå¤‡çš„å…‰çº¿è¿½è¸ªå™¨ã€‚



> When somebody says â€œray tracingâ€ it could mean many things. What I am going to describe is technically a path tracer, and a fairly general one. While the code will be pretty simple (let the computer do the work!) I think youâ€™ll be very happy with the images you can make.

å½“å¤§å®¶æèµ·â€å…‰çº¿è¿½è¸ªâ€, å¯èƒ½æŒ‡çš„æ˜¯å¾ˆå¤šä¸åŒçš„ä¸œè¥¿ã€‚æˆ‘å¯¹è¿™ä¸ªè¯çš„æè¿°æ˜¯, å…‰çº¿è¿½è¸ªåœ¨æŠ€æœ¯ä¸Šå°±æ˜¯ä¸€ä¸ªè·¯å¾„è¿½è¸ªå™¨, äº‹å®ä¸Šå¤§éƒ¨åˆ†æƒ…å†µä¸‹è¿™ä¸ªè¯éƒ½æ˜¯è¿™ä¸ªæ„æ€ã€‚å…‰çº¿è¿½è¸ªå™¨çš„ä»£ç ä¹Ÿæ˜¯ååˆ†çš„ç®€å•ï¼ˆè®©ç”µè„‘å¸®æˆ‘ä»¬ç®—å»å§!ï¼‰ã€‚å½“ä½ çœ‹åˆ°ä½ æ¸²æŸ“çš„å›¾ç‰‡æ—¶, ä½ ä¸€å®šä¼šæ„Ÿåˆ°é«˜å…´çš„ã€‚



> Iâ€™ll take you through writing a ray tracer in the order I do it, along with some debugging tips. By the end, you will have a ray tracer that produces some great images. You should be able to do this in a weekend. If you take longer, donâ€™t worry about it. I use C++ as the driving language, but you donâ€™t need to. However, I suggest you do, because itâ€™s fast, portable, and most production movie and video game renderers are written in C++. Note that I avoid most â€œmodern featuresâ€ of C++, but inheritance and operator overloading are too useful for ray tracers to pass on. I do not provide the code online, but the code is real and I show all of it except for a few straightforward operators in the `vec3` class. I am a big believer in typing in code to learn it, but when code is available I use it, so I only practice what I preach when the code is not available. So donâ€™t ask!



æ¥ä¸‹æ¥æˆ‘ä¼šå¸¦ä½ ä¸€æ­¥æ­¥çš„å®ç°è¿™ä¸ªå…‰çº¿è¿½è¸ª, å¹¶åŠ å…¥ä¸€äº›æˆ‘çš„debugå»ºè®®ã€‚æœ€åä½ ä¼šå¾—åˆ°ä¸€ä¸ªèƒ½æ¸²æŸ“å‡ºæ¼‚äº®å›¾ç‰‡çš„å…‰çº¿è¿½è¸ªå™¨ã€‚ä½ è®¤ä¸ºä½ åº”è¯¥èƒ½åœ¨ä¸€ä¸ªå‘¨æœ«çš„æ—¶é—´å†…æå®šã€‚å¦‚æœä½ èŠ±çš„æ—¶é—´æ¯”è¿™é•¿, åˆ«æ‹…å¿ƒ, ä¹Ÿæ²¡å¤ªå¤§é—®é¢˜ã€‚æˆ‘ä½¿ç”¨C++ä½œä¸ºæœ¬å…‰çº¿è¿½è¸ªå™¨çš„å®ç°è¯­è¨€ã€‚ä½ å…¶å®ä¸å¿…, ä½†æˆ‘è¿˜æ˜¯æ¨èä½ ç”¨C++, å› ä¸ºC++å¿«é€Ÿ, å¹³å°ç§»æ¤æ€§å¥½, å¹¶ä¸”å¤§éƒ¨åˆ†çš„å·¥ä¸šçº§ç”µå½±å’Œæ¸¸æˆæ¸²æŸ“å™¨éƒ½æ˜¯ä½¿ç”¨C++ç¼–å†™çš„ã€‚æ³¨æ„è¿™é‡Œæˆ‘é¿å…äº†å¤§éƒ¨åˆ†C++çš„æ–°ç‰¹æ€§ã€‚ä½†æ˜¯ç»§æ‰¿å’Œé‡è½½è¿ç®—ç¬¦æˆ‘ä»¬ä¿ç•™, å¯¹å…‰çº¿è¿½è¸ªå™¨æ¥è¯´è¿™ä¸ªå¤ªæœ‰ç”¨äº†ã€‚ç½‘ä¸Šçš„é‚£äº›ä»£ç ä¸æ˜¯æˆ‘æä¾›çš„, ä½†æ˜¯è¿™äº›ä»£ç æ˜¯çœŸçš„å¯ä»¥è·‘çš„ã€‚é™¤äº†`vec3`ç±»ä¸­çš„ä¸€äº›ç®€å•çš„æ“ä½œ, æˆ‘å°†æ‰€æœ‰çš„ä»£ç éƒ½å…¬å¼€äº†ã€‚æˆ‘æ˜¯â€å­¦ä¹ ç¼–ç¨‹è¦äº²è‡ªåŠ¨æ‰‹æ•²ä»£ç â€æ´¾ã€‚ä½†æ˜¯å¦‚æœæœ‰ä¸€ä»½ä»£ç æ‘†åœ¨æˆ‘é¢å‰, æˆ‘å¯ä»¥ç›´æ¥ç”¨, æˆ‘è¿˜æ˜¯ä¼šç”¨çš„ã€‚æ‰€ä»¥æˆ‘åªåœ¨æ²¡ä»£ç ç”¨çš„æ—¶å€™, æˆ‘æ‰å¥‰è¡Œæˆ‘åˆšåˆšè¯´çš„è¯ã€‚å¥½å•¦, åˆ«æè¿™ä¸ªäº†!



> I have left that last part in because it is funny what a 180 I have done. Several readers ended up with subtle errors that were helped when we compared code. So please do type in the code, but if you want to look at mine it is at:
>
> https://github.com/RayTracing/raytracing.github.io/



æˆ‘æ²¡æŠŠä¸Šé¢ä¸€æ®µåˆ äº†, å› ä¸ºæˆ‘çš„æ€åº¦180Â°å¤§è½¬å˜å¤ªå¥½ç©äº†ã€‚è¯»è€…ä»¬å¸®æˆ‘ä¿®å¤äº†ä¸€äº›æ¬¡è¦çš„ç¼–è¯‘é”™è¯¯, æ‰€ä»¥è¿˜æ˜¯è¯·ä½ äº²æ‰‹æ¥æ•²ä¸€ä¸‹ä»£ç å§!ä½†æ˜¯ä½ å¦‚æœä½ æƒ³çœ‹çœ‹æˆ‘çš„ä»£ç :

https://github.com/RayTracing/raytracing.github.io/



> I assume a little bit of familiarity with vectors (like dot product and vector addition). If you donâ€™t know that, do a little review. If you need that review, or to learn it for the first time, check out Marschnerâ€™s and my graphics text, Foley, Van Dam, *et al.*, or McGuireâ€™s graphics codex.



æˆ‘å‡å®šä½ æœ‰ä¸€å®šçš„å‘é‡çŸ¥è¯†(æ¯”å¦‚è¯´ç‚¹ä¹˜å’Œå‰ä¹˜)ã€‚å¦‚æœä½ è®°ä¸å¤ªæ¸…æ¥š, å›é¡¾ä¸€ä¸‹å°±è¡Œã€‚å¦‚æœä½ éœ€è¦å›é¡¾, æˆ–è€…ä½ æ˜¯ç¬¬ä¸€æ¬¡å¬è¯´è¿™ä¸ªä¸œè¥¿, ä½ å¯ä»¥çœ‹æˆ‘æˆ–è€…Marschnerçš„å›¾åƒæ•™æ, Foley, Van Damç­‰ä¹Ÿè¡Œã€‚æˆ–è€…McGuireçš„codexã€‚



> If you run into trouble, or do something cool youâ€™d like to show somebody, send me some email at [ptrshrl@gmail.com.](mailto:ptrshrl@gmail.com.)
>
> Iâ€™ll be maintaining a site related to the book including further reading and links to resources at a blog https://in1weekend.blogspot.com/ related to this book.
>
> Thanks to everyone who lent a hand on this project. You can find them in the [acknowledgments](https://raytracing.github.io/books/RayTracingInOneWeekend.html#acknowledgments) section at the end of this book.
>
> Letâ€™s get on with it!

å¦‚æœä½ é‡åˆ°çš„éº»çƒ¦, æˆ–è€…ä½ å¼„å‡ºäº†å¾ˆcoooolçš„ä¸œè¥¿æƒ³è¦åˆ†äº«ç»™å¤§å®¶çœ‹, è¯·ç»™æˆ‘å‘é‚®ä»¶ã€‚æˆ‘çš„é‚®ç®±æ˜¯[ptrshrl@gmail.com](mailto:ptrshrl@gmail.com)[ç‚¹æˆ‘å‘é‚®ä»¶](mailto:ptrshrl@gmail.com)

æˆ‘ä¼šç»´æŠ¤ä¸€ä¸ªæœ‰å…³æœ¬ä¹¦çš„åšå®¢ç½‘ç«™, ç½‘ç«™é‡Œæœ‰ä¸€äº›æ‹“å±•é˜…è¯»å’Œä¸€äº›é“¾æ¥èµ„æºã€‚ç½‘å€æ˜¯[https://in1weekend.blogspot.com](https://in1weekend.blogspot.com/)

å¥½äº†ä¸å¤šBB, è®©æˆ‘ä»¬å¼€å§‹å§!





### è¾“å‡ºå›¾åƒ

#### PPMå›¾åƒæ ¼å¼

> Whenever you start a renderer, you need a way to see an image. The most straightforward way is to write it to a file. The catch is, there are so many formats. Many of those are complex. I always start with a plain text ppm file. Hereâ€™s a nice description from Wikipedia:

å½“ä½ å¼€å§‹å†™æ¸²æŸ“å™¨çš„æ—¶å€™, ä½ é¦–å…ˆå¾—èƒ½æœ‰åŠæ³•çœ‹åˆ°ä½ æ¸²æŸ“çš„å›¾åƒã€‚æœ€ç›´æ¥äº†å½“çš„æ–¹æ³•å°±è¡ŒæŠŠå›¾åƒä¿¡æ¯å†™å…¥æ–‡ä»¶ã€‚é—®é¢˜æ˜¯, æœ‰é‚£ä¹ˆå¤šå›¾ç‰‡æ ¼å¼, è€Œä¸”è®¸å¤šæ ¼å¼éƒ½æŒºå¤æ‚çš„ã€‚åœ¨å¼€å§‹éƒ¨åˆ†, æˆ‘å¸¸å¸¸ä½¿ç”¨æœ€ç®€å•çš„ppmæ–‡ä»¶ï¼Œè¿™é‡Œå¼•ç”¨Wikipediaä¸Šé¢çš„ç®€æ˜ä»‹ç»:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.01-ppm.jpg)

> Letâ€™s make some C++ code to output such a thing:

æˆ‘ä»¬æ¥å†™ä¸€ä¸‹è¾“å‡ºè¿™ç§å›¾ç‰‡æ ¼å¼çš„C++ä»£ç :



```c++
#include <iostream>

int main() {

    // Image

    const int image_width = 256;
    const int image_height = 256;

    // Render

    std::cout << "P3\n" << image_width << ' ' << image_height << "\n255\n";

    for (int j = image_height-1; j >= 0; --j) {
        for (int i = 0; i < image_width; ++i) {
            auto r = double(i) / (image_width-1);
            auto g = double(j) / (image_height-1);
            auto b = 0.25;

            int ir = static_cast<int>(255.999 * r);
            int ig = static_cast<int>(255.999 * g);
            int ib = static_cast<int>(255.999 * b);

            std::cout << ir << ' ' << ig << ' ' << ib << '\n';
        }
    }
}
```





> There are some things to note in that code:
>
> 1. The pixels are written out in rows with pixels left to right.
> 2. The rows are written out from top to bottom.
> 3. By convention, each of the red/green/blue components range from 0.0 to 1.0. We will relax that later when we internally use high dynamic range, but before output we will tone map to the zero to one range, so this code wonâ€™t change.
> 4. Red goes from fully off (black) to fully on (bright red) from left to right, and green goes from black at the bottom to fully on at the top. Red and green together make yellow so we should expect the upper right corner to be yellow.

ä»£ç é‡Œæœ‰ä¸€äº›æˆ‘ä»¬è¦æ³¨æ„çš„äº‹æƒ…:

1. å¯¹äºåƒç´ æ¥è¯´, æ¯ä¸€è¡Œæ˜¯ä»å·¦å¾€å³å†™å…¥çš„ã€‚

2. è¡Œä»ä¸Šå¼€å§‹å¾€ä¸‹å†™å…¥çš„ã€‚

3. é€šå¸¸æˆ‘ä»¬æŠŠRGBé€šé“çš„å€¼é™å®šåœ¨0.0åˆ°1.0ã€‚æˆ‘ä»¬ä¹‹åè®¡ç®—é¢œè‰²å€¼çš„æ—¶å€™å°†ä½¿ç”¨ä¸€ä¸ªåŠ¨æ€çš„èŒƒå›´, è¿™ä¸ªèŒƒå›´å¹¶ä¸æ˜¯0åˆ°1ã€‚ä½†æ˜¯åœ¨ä½¿ç”¨è¿™æ®µä»£ç è¾“å‡ºå›¾åƒä¹‹å‰, æˆ‘ä»¬å°†æŠŠé¢œè‰²æ˜ å°„åˆ°0åˆ°1ã€‚æ‰€ä»¥è¿™éƒ¨åˆ†è¾“å‡ºå›¾åƒä»£ç ä¸ä¼šæ”¹å˜ã€‚

4. ä¸‹æ–¹çš„çº¢è‰²ä»å·¦åˆ°å³ç”±é»‘è¾¹çº¢, å·¦ä¾§çš„ç»¿è‰²ä»ä¸Šåˆ°ä¸‹ç”±é»‘åˆ°ç»¿ã€‚çº¢+ç»¿å˜é»„, æ‰€ä»¥æˆ‘ä»¬çš„å³ä¸Šè§’åº”è¯¥æ˜¯é»„çš„ã€‚



ä¸‹é¢æ˜¯æˆ‘ä¸ªäººçš„æ€»ç»“ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªä¹‹å‰ä¸æ˜¯å¾ˆæ¸…æ¥šçš„ç‚¹ï¼š

+ `auto`å…³é”®å­—å°±æ˜¯å˜é‡çš„è‡ªåŠ¨ç±»å‹æ¨æ–­ï¼Œå¯ä»¥åœ¨å£°æ˜å˜é‡çš„æ—¶å€™æ ¹æ®å˜é‡åˆå§‹å€¼çš„ç±»å‹è‡ªåŠ¨ä¸ºæ­¤å˜é‡é€‰æ‹©åŒ¹é…çš„ç±»å‹ï¼Œç±»ä¼¼çš„å…³é”®å­—è¿˜æœ‰`decltype`

+ `static_cast`ç›¸å½“äºä¼ ç»Ÿçš„Cè¯­è¨€é‡Œçš„å¼ºåˆ¶è½¬æ¢ï¼Œè¯¥è¿ç®—ç¬¦æŠŠ`expression`è½¬æ¢ä¸º`new_type`ç±»å‹ï¼Œç”¨æ¥å¼ºè¿«éšå¼è½¬æ¢ï¼Œä¾‹å¦‚`non-const`å¯¹è±¡è½¬ä¸º`const`å¯¹è±¡ï¼Œç¼–è¯‘æ—¶æ£€æŸ¥ï¼Œç”¨äºéå¤šæ€çš„è½¬æ¢ï¼Œå¯ä»¥è½¬æ¢æŒ‡é’ˆåŠå…¶ä»–ï¼Œä½†æ²¡æœ‰è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥æ¥ä¿è¯è½¬æ¢çš„å®‰å…¨æ€§ã€‚å®ƒä¸»è¦æœ‰å¦‚ä¸‹å‡ ç§ç”¨æ³•ï¼š

  1. ç”¨äºç±»å±‚æ¬¡ç»“æ„ä¸­åŸºç±»ï¼ˆçˆ¶ç±»ï¼‰å’Œæ´¾ç”Ÿç±»ï¼ˆå­ç±»ï¼‰ä¹‹é—´æŒ‡é’ˆæˆ–å¼•ç”¨çš„è½¬æ¢

     **è¿›è¡Œä¸Šè¡Œè½¬æ¢ï¼ˆæŠŠæ´¾ç”Ÿç±»çš„æŒ‡é’ˆæˆ–å¼•ç”¨è½¬æ¢æˆåŸºç±»è¡¨ç¤ºï¼‰æ˜¯å®‰å…¨çš„**

     **è¿›è¡Œä¸‹è¡Œè½¬æ¢ï¼ˆæŠŠåŸºç±»æŒ‡é’ˆæˆ–å¼•ç”¨è½¬æ¢æˆæ´¾ç”Ÿç±»è¡¨ç¤ºï¼‰æ—¶ï¼Œç”±äºæ²¡æœ‰åŠ¨æ€ç±»å‹æ£€æŸ¥ï¼Œæ‰€ä»¥æ˜¯ä¸å®‰å…¨çš„**

  2. ç”¨äºåŸºæœ¬æ•°æ®ç±»å‹ä¹‹é—´çš„è½¬æ¢ï¼Œå¦‚æŠŠ`int`è½¬æ¢æˆ`char`ï¼ŒæŠŠ`int`è½¬æ¢æˆ`enum`ã€‚è¿™ç§è½¬æ¢çš„å®‰å…¨æ€§ä¹Ÿè¦å¼€å‘äººå‘˜æ¥ä¿è¯

  3. æŠŠç©ºæŒ‡é’ˆè½¬æ¢æˆç›®æ ‡ç±»å‹çš„ç©ºæŒ‡é’ˆ

  4. æŠŠä»»ä½•ç±»å‹çš„è¡¨è¾¾å¼è½¬æ¢æˆ`void`ç±»å‹

  æ³¨æ„ï¼š`static_cast`ä¸èƒ½è½¬æ¢æ‰`expression`çš„`const`ã€`volatile`ã€æˆ–è€…`__unaligned`å±æ€§

+ è¿™é‡Œè¡Œåˆ—çš„å˜åŒ–ä»¤äººè´¹è§£ï¼Œç¬¬ä¸€å±‚å¾ªç¯è´Ÿè´£`image_height`ä¹Ÿå°±æ˜¯è¡Œçš„è¯æ˜¾ç„¶æ˜¯è‡ªé¡¶å‘ä¸Šçš„å†™å…¥ï¼Œè€Œæ–‡ä¸­å´è¯´"The rows are written out from top to bottom."ï¼Œè®©æˆ‘æ„Ÿåˆ°å›°æƒ‘ï¼Œå¯èƒ½æ˜¯ä¸ºäº†ç‰¹å®šçš„è‰²å½©è€Œæ•…æ„ä¸ºä¹‹å§



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/image-20210327092205919.png)





#### åˆ›å»ºå›¾åƒæ–‡ä»¶

> Because the file is written to the program output, you'll need to redirect it to an image file. Typically this is done from the command-line by using the `>` redirection operator, like so:

ç°åœ¨æˆ‘ä»¬è¦æŠŠcoutçš„è¾“å‡ºæµå†™å…¥æ–‡ä»¶ä¸­ã€‚å¹¸å¥½æˆ‘ä»¬æœ‰å‘½ä»¤è¡Œæ“ä½œç¬¦`>`æ¥å®šå‘è¾“å‡ºæµã€‚åœ¨windowsæ“ä½œç³»ç»Ÿä¸­å·®ä¸å¤šè¿™æ ·çš„:

```bash
build\Release\inOneWeekend.exe > image.ppm
```



> This is how things would look on Windows. On Mac or Linux, it would look like this:

åœ¨Macæˆ–è€…Linuxæ“ä½œç³»ç»Ÿä¸­, å¤§æ¦‚æ˜¯è¿™ä¸ªæ ·å­çš„:

```bash
build/inOneWeekend > image.ppm
```



è¿™é‡Œæˆ‘å°è¯•ç›´æ¥æŒ‰ç…§ä¸Šé¢çš„ä»£ç è¿è¡Œï¼Œç»“æœå¹¶æ²¡æœ‰æ‰¾åˆ°`build`æŒ‡ä»¤ã€‚æ‰€ä»¥æˆ‘æŒ‰ç…§linuxå¹³å°çš„ç¼–è¯‘cppç¨‹åºçš„æ–¹å¼ï¼Œå¾—åˆ°äº†ppmæ ¼å¼çš„å›¾åƒï¼š

```shell
 g++ -o OneWeekend main.cpp
 ./OneWeekend > image.ppm
```



> Opening the output file (in `ToyViewer` on my Mac, but try it in your favorite viewer and Google â€œppm viewerâ€ if your viewer doesnâ€™t support it) shows this result:

æ‰“å¼€æˆ‘ä»¬è¾“å‡ºçš„æ–‡ä»¶ï¼ˆæˆ‘æ˜¯Macç³»ç»Ÿ, æˆ‘æ˜¯ç”¨ToyVieweræ‰“å¼€çš„, ä½ å¯ä»¥ç”¨ä½ å–œæ¬¢çš„ä»»æ„çœ‹å›¾è½¯ä»¶æ¥æ‰“å¼€ã€‚å¦‚æœä½ é»˜è®¤çš„çœ‹å›¾è½¯ä»¶æ¯”å¦‚windowsä¸‹çš„å›¾ç‰‡ä¸æ”¯æŒppmæ ¼å¼, åªè¦Googleä¸€ä¸‹â€ppm viewerâ€è£…ä¸ªæ–°çš„å°±è¡Œã€‚)æ‰“å¼€åçš„ç»“æœå¦‚ä¸‹:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.01-first-ppm-image.png)



> Hooray! This is the graphics â€œhello worldâ€. If your image doesnâ€™t look like that, open the output file in a text editor and see what it looks like. It should start something like this:

å¥½è€¶!è¿™ä¾¿æ˜¯å›¾å½¢å­¦ä¸­çš„â€hello worldâ€äº†ã€‚å¦‚æœä½ çš„å›¾åƒçœ‹ä¸Šå»ä¸æ˜¯è¿™æ ·çš„, ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ä½ çš„è¾“å‡ºæ–‡ä»¶, çœ‹çœ‹é‡Œé¢å†…å®¹æ˜¯å•¥æ ·çš„ã€‚ä¸å‡ºæ„å¤–çš„è¯, æ­£ç¡®æ ¼å¼åº”è¯¥æ˜¯è¿™æ ·çš„:

```
P3
256 256
255
0 255 63
1 255 63
2 255 63
3 255 63
4 255 63
5 255 63
6 255 63
7 255 63
8 255 63
9 255 63
...
```



> If it doesnâ€™t, then you probably just have some newlines or something similar that is confusing the image reader.
>
> If you want to produce more image types than PPM, I am a fan of `stb_image.h`, a header-only image library available on GitHub at https://github.com/nothings/stb.

å¦‚æœä¸æ˜¯è¿™æ ·çš„, ä½ å¯èƒ½å½“ä¸­å¤šäº†äº›ç©ºè¡Œæˆ–è€…ç±»ä¼¼çš„ä»€ä¹ˆä¸œè¥¿, å› æ­¤ä½ çš„çœ‹å›¾è½¯ä»¶è¯†åˆ«ä¸å‡ºæ¥ã€‚

å¦‚æœä½ æƒ³ç”Ÿæˆåˆ«çš„å›¾åƒæ ¼å¼æ¥ä»£æ›¿åŸºç¡€çš„PPM, æˆ‘å¼ºçƒˆå®‰åˆ©[`stb_image.h`](https://github.com/nothings/stb/blob/master/stb_image.h), ä½ å¯ä»¥å…è´¹åœ¨[github](https://github.com/nothings/stb/blob/master/stb_image.h)ä¸Šè·å–ã€‚



è¿™é‡Œæˆ‘è‡ªä½œèªæ˜å°†`main.cpp`ä¸­çš„è¾“å‡ºè¿›è¡Œäº†æ›´æ”¹ï¼Œåœ¨åé¢é€ æˆäº†å¾ˆå¤§çš„é”™è¯¯ï¼š

```c++
//std::cout << "\nimage width: " << image_width << "    image height: " << image_height << "\n255\n";
std::cout << "P3\n" << image_width << ' ' << image_height << "\n255\n";

```



å¦‚æœä¸æŒ‰è¿™ç§æ ¼å¼ç”Ÿæˆçš„`.ppm`æ‰“å¼€ä¼šå‡ºç°æ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ ¼å¤–æ³¨æ„ã€‚



#### åŠ å…¥ä¸€ä¸ªè¿›åº¦æ¡

> Before we continue, let's add a progress indicator to our output. This is a handy way to track the progress of a long render, and also to possibly identify a run that's stalled out due to an infinite loop or other problem.

åœ¨æˆ‘ä»¬å¾€ä¸‹èµ°ä¹‹å‰, æˆ‘ä»¬å…ˆæ¥åŠ ä¸ªè¾“å‡ºçš„è¿›åº¦æç¤ºã€‚å¯¹äºæŸ¥çœ‹ä¸€æ¬¡é•¿æ—¶é—´æ¸²æŸ“çš„è¿›åº¦æ¥è¯´, è¿™ä¸å¤±ä¸ºä¸€ç§ç®€ä¾¿çš„åšæ³•ã€‚ä¹Ÿå¯ä»¥é€šè¿‡è¿™ä¸ªè¿›åº¦æ¥åˆ¤æ–­ç¨‹åºæ˜¯å¦å¡ä½æˆ–è€…è¿›å…¥ä¸€ä¸ªæ­»å¾ªç¯ã€‚

> Our program outputs the image to the standard output stream (`std::cout`), so leave that alone and instead write to the error output stream (`std::cerr`):

æˆ‘ä»¬çš„ç¨‹åºå°†å›¾ç‰‡ä¿¡æ¯å†™å…¥æ ‡å‡†è¾“å‡ºæµï¼ˆ`std::cout`ï¼‰, æ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç”¨è¿™ä¸ªæµè¾“å‡ºè¿›åº¦ã€‚æˆ‘ä»¬æ¢ç”¨é”™è¯¯è¾“å‡ºæµï¼ˆ`std::cerr`ï¼‰æ¥è¾“å‡ºè¿›åº¦:

```diff
    for (int j = image_height-1; j >= 0; --j) {
+       std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            auto r = double(i) / (image_width-1);
            auto g = double(j) / (image_height-1);
            auto b = 0.25;

            int ir = static_cast<int>(255.999 * r);
            int ig = static_cast<int>(255.999 * g);
            int ib = static_cast<int>(255.999 * b);

            std::cout << ir << ' ' << ig << ' ' << ib << '\n';
        }
    }

+  	std::cerr << "\nDone.\n";
```





### vec3å‘é‡ç±»

> Almost all graphics programs have some class(es) for storing geometric vectors and colors. In many systems these vectors are 4D (3D plus a homogeneous coordinate for geometry, and RGB plus an alpha transparency channel for colors). For our purposes, three coordinates suffices. Weâ€™ll use the same class `vec3` for colors, locations, directions, offsets, whatever. Some people donâ€™t like this because it doesnâ€™t prevent you from doing something silly, like adding a color to a location. They have a good point, but weâ€™re going to always take the â€œless codeâ€ route when not obviously wrong. In spite of this, we do declare two aliases for `vec3`: `point3` and `color`. Since these two types are just aliases for `vec3`, you won't get warnings if you pass a `color` to a function expecting a `point3`, for example. We use them only to clarify intent and use.

å‡ ä¹æ‰€æœ‰çš„å›¾å½¢ç¨‹åºéƒ½ä½¿ç”¨ç±»ä¼¼çš„ç±»æ¥å‚¨å­˜å‡ ä½•å‘é‡å’Œé¢œè‰²ã€‚åœ¨è®¸å¤šç¨‹åºä¸­è¿™äº›å‘é‡æ˜¯å››ç»´çš„(å¯¹äºä½ç½®æˆ–è€…å‡ ä½•å‘é‡æ¥è¯´æ˜¯ä¸‰ç»´çš„é½æ¬¡æ‹“å±•, å¯¹äºé¢œè‰²æ¥è¯´æ˜¯RGBåŠ é€æ˜é€šé“)ã€‚å¯¹æˆ‘ä»¬ç°åœ¨è¿™ä¸ªç¨‹åºæ¥è¯´, ä¸‰ç»´å°±è¶³å¤Ÿäº†ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ª`vec3`ç±»æ¥å‚¨å­˜æ‰€æœ‰çš„é¢œè‰², ä½ç½®, æ–¹å‘, ä½ç½®åç§», æˆ–è€…åˆ«çš„ä»€ä¹ˆä¸œè¥¿ã€‚ä¸€äº›äººå¯èƒ½ä¸å¤ªå–œæ¬¢è¿™æ ·åš, å› ä¸ºå…¨éƒ½ç”¨ä¸€ä¸ªç±», æ²¡æœ‰é™åˆ¶, å†™ä»£ç çš„æ—¶å€™éš¾å…ä¼šçŠ¯äºŒ, æ¯”å¦‚ä½ æŠŠé¢œè‰²å’Œä½ç½®åŠ åœ¨ä¸€èµ·ã€‚ä»–ä»¬çš„æƒ³æ³•æŒºå¥½çš„, ä½†æ˜¯æˆ‘ä»¬æƒ³åœ¨é¿å…æ˜æ˜¾é”™è¯¯çš„åŒæ—¶è®©ä»£ç é‡å°½é‡çš„ç²¾ç®€ã€‚æ‰€ä»¥è¿™é‡Œå°±å…ˆä¸€ä¸ªç±»å§ã€‚ã€è¯‘æ³¨: ä¹‹åæœ‰æ·»åŠ æ–°çš„colorç±»ã€‘



#### å˜é‡å’Œå‡½æ•°

> Hereâ€™s the top part of my `vec3` class:

ä¸‹é¢æ˜¯æˆ‘çš„`vec3`çš„å¤´æ–‡ä»¶:

```c++
#ifndef VEC3_H
#define VEC3_H

#include <cmath>
#include <iostream>

using std::sqrt;

class vec3 {
    public:
        vec3() : e{0,0,0} {}
        vec3(double e0, double e1, double e2) : e{e0, e1, e2} {}

        double x() const { return e[0]; }
        double y() const { return e[1]; }
        double z() const { return e[2]; }

        vec3 operator-() const { return vec3(-e[0], -e[1], -e[2]); }
        double operator[](int i) const { return e[i]; }
        double& operator[](int i) { return e[i]; }

        vec3& operator+=(const vec3 &v) {
            e[0] += v.e[0];
            e[1] += v.e[1];
            e[2] += v.e[2];
            return *this;
        }

        vec3& operator*=(const double t) {
            e[0] *= t;
            e[1] *= t;
            e[2] *= t;
            return *this;
        }

        vec3& operator/=(const double t) {
            return *this *= 1/t;
        }

        double length() const {
            return sqrt(length_squared());
        }

        double length_squared() const {
            return e[0]*e[0] + e[1]*e[1] + e[2]*e[2];
        }

    public:
        double e[3];
};

// Type aliases for vec3
using point3 = vec3;   // 3D point
using color = vec3;    // RGB color

#endif
```



> We use `double` here, but some ray tracers use `float`. Either one is fine â€” follow your own tastes.

æˆ‘ä»¬ä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹`double`, ä½†æ˜¯æœ‰äº›å…‰çº¿è¿½è¸ªå™¨ä½¿ç”¨å•ç²¾åº¦æµ®ç‚¹`float`ã€‚è¿™é‡Œå…¶å®éƒ½è¡Œ, ä½ å–œæ¬¢å“ªä¸ªå°±ç”¨é‚£ä¸ªã€‚



#### vec3å®ç”¨å‡½æ•°

> The second part of the header file contains vector utility functions:

å¤´æ–‡ä»¶çš„ç¬¬äºŒéƒ¨åˆ†åŒ…æ‹¬ä¸€äº›å‘é‡æ“ä½œå·¥å…·å‡½æ•°:

```c++
// vec3 Utility Functions

inline std::ostream& operator<<(std::ostream &out, const vec3 &v) {
    return out << v.e[0] << ' ' << v.e[1] << ' ' << v.e[2];
}

inline vec3 operator+(const vec3 &u, const vec3 &v) {
    return vec3(u.e[0] + v.e[0], u.e[1] + v.e[1], u.e[2] + v.e[2]);
}

inline vec3 operator-(const vec3 &u, const vec3 &v) {
    return vec3(u.e[0] - v.e[0], u.e[1] - v.e[1], u.e[2] - v.e[2]);
}

inline vec3 operator*(const vec3 &u, const vec3 &v) {
    return vec3(u.e[0] * v.e[0], u.e[1] * v.e[1], u.e[2] * v.e[2]);
}

inline vec3 operator*(double t, const vec3 &v) {
    return vec3(t*v.e[0], t*v.e[1], t*v.e[2]);
}

inline vec3 operator*(const vec3 &v, double t) {
    return t * v;
}

inline vec3 operator/(vec3 v, double t) {
    return (1/t) * v;
}

inline double dot(const vec3 &u, const vec3 &v) {
    return u.e[0] * v.e[0]
         + u.e[1] * v.e[1]
         + u.e[2] * v.e[2];
}

inline vec3 cross(const vec3 &u, const vec3 &v) {
    return vec3(u.e[1] * v.e[2] - u.e[2] * v.e[1],
                u.e[2] * v.e[0] - u.e[0] * v.e[2],
                u.e[0] * v.e[1] - u.e[1] * v.e[0]);
}

inline vec3 unit_vector(vec3 v) {
    return v / v.length();
}
```





### å…‰çº¿, ç®€å•æ‘„åƒæœº, ä»¥åŠèƒŒæ™¯



> The one thing that all ray tracers have is a ray class and a computation of what color is seen along a ray. Letâ€™s think of a ray as a function **P**(ğ‘¡)=**A**+ğ‘¡**b**. Here **P** is a 3D position along a line in 3D. **A** is the ray origin and **b** is the ray direction. The ray parameter ğ‘¡t is a real number (`double` in the code). Plug in a different ğ‘¡t and **P**(ğ‘¡) moves the point along the ray. Add in negative ğ‘¡t values and you can go anywhere on the 3D line. For positive ğ‘¡t, you get only the parts in front of **A**, and this is what is often called a half-line or ray.



æ‰€æœ‰çš„å…‰çº¿è¿½è¸ªå™¨éƒ½æœ‰ä¸ªä¸€ä¸ªrayç±», æˆ‘ä»¬å‡å®šå…‰çº¿çš„å…¬å¼ä¸º**P**(ğ‘¡)=**A**+ğ‘¡**b**ã€‚è¿™é‡Œçš„**P**æ˜¯ä¸‰ç»´å°„çº¿ä¸Šçš„ä¸€ä¸ªç‚¹ã€‚**A**æ˜¯å°„çº¿çš„åŸç‚¹, **b**æ˜¯å°„çº¿çš„æ–¹å‘ã€‚ç±»ä¸­çš„å˜é‡**ğ‘¡**æ˜¯ä¸€ä¸ªå®æ•°(ä»£ç ä¸­ä¸º`double`ç±»å‹)ã€‚**P**(**ğ‘¡**)æ¥å—ä»»æ„çš„ğ‘¡åšä¸ºå˜é‡, è¿”å›å°„çº¿ä¸Šçš„å¯¹åº”ç‚¹ã€‚å¦‚æœå…è®¸**ğ‘¡**å–è´Ÿå€¼ä½ å¯ä»¥å¾—åˆ°æ•´æ¡ç›´çº¿ã€‚å¯¹äºä¸€ä¸ªæ­£æ•°**ğ‘¡**, ä½ åªèƒ½å¾—åˆ°åŸç‚¹å‰éƒ¨åˆ†**A**, è¿™å¸¸å¸¸è¢«ç§°ä¸ºåŠæ¡ç›´çº¿, æˆ–è€…è¯´å°„çº¿ã€‚



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.02-lerp.jpg)



> The function **P**(ğ‘¡) in more verbose code form I call `ray::at(t)`:

æˆ‘åœ¨ä»£ç ä¸­ä½¿ç”¨å¤æ‚å‘½å, å°†å‡½æ•°**P**(ğ‘¡)æ‰©å†™ä¸º`ray::at(t)`ï¼š

```c++
#ifndef RAY_H
#define RAY_H

#include "vec3.h"

class ray {
    public:
        ray() {}
        ray(const vec3& origin, const vec3& direction)
            : orig(origin), dir(direction)
        {}

        vec3 origin() const    { return orig; }
        vec3 direction() const { return dir; }

        vec3 at(double t) const {
            return orig + t*dir;
        }

    public:
        vec3 orig;
        vec3 dir;
};

#endif
```





> Now we are ready to turn the corner and make a ray tracer. At the core, the ray tracer sends rays through pixels and computes the color seen in the direction of those rays. The involved steps are (1) calculate the ray from the eye to the pixel, (2) determine which objects the ray intersects, and (3) compute a color for that intersection point. When first developing a ray tracer, I always do a simple camera for getting the code up and running. I also make a simple `ray_color(ray)` function that returns the color of the background (a simple gradient).



ç°åœ¨æˆ‘ä»¬å†æ‹å›æ¥åšæˆ‘ä»¬çš„å…‰çº¿è¿½è¸ªå™¨ã€‚å…‰çº¿è¿½è¸ªå™¨çš„æ ¸å¿ƒæ˜¯ä»åƒç´ å‘å°„å°„çº¿, å¹¶è®¡ç®—è¿™äº›å°„çº¿å¾—åˆ°çš„é¢œè‰²ã€‚è¿™åŒ…æ‹¬å¦‚ä¸‹çš„æ­¥éª¤: (1)å°†å°„çº¿ä»è§†ç‚¹è½¬åŒ–ä¸ºåƒç´ åæ ‡ (2)è®¡ç®—å…‰çº¿æ˜¯å¦ä¸åœºæ™¯ä¸­çš„ç‰©ä½“ç›¸äº¤ (3)å¦‚æœæœ‰, è®¡ç®—äº¤ç‚¹çš„é¢œè‰²ã€‚åœ¨åšå…‰çº¿è¿½è¸ªå™¨çš„åˆæœŸ, æˆ‘ä¼šå…ˆå¼„ä¸ªç®€å•æ‘„åƒæœºè®©ä»£ç èƒ½è·‘èµ·æ¥ã€‚æˆ‘ä¹Ÿä¼šç¼–å†™ä¸€ä¸ªç®€å•çš„`color(ray)`å‡½æ•°æ¥è¿”å›èƒŒæ™¯é¢œè‰²å€¼(ä¸€ä¸ªç®€å•çš„æ¸å˜è‰²)ã€‚



> Iâ€™ve often gotten into trouble using square images for debugging because I transpose ğ‘¥x and ğ‘¦y too often, so Iâ€™ll use a non-square image. For now we'll use a 16:9 aspect ratio, since that's so common.



åœ¨ä½¿ç”¨æ­£æ–¹å½¢çš„å›¾åƒDebugæ—¶æˆ‘æ—¶å¸¸ä¼šé‡åˆ°é—®é¢˜, å› ä¸ºæˆ‘è€æ˜¯æŠŠxå’Œyå¼„åã€‚æ‰€ä»¥æˆ‘åšæŒä½¿ç”¨16:9è¿™æ ·é•¿å®½ä¸ç­‰çš„å›¾åƒã€‚



> In addition to setting up the pixel dimensions for the rendered image, we also need to set up a virtual viewport through which to pass our scene rays. For the standard square pixel spacing, the viewport's aspect ratio should be the same as our rendered image. We'll just pick a viewport two units in height. We'll also set the distance between the projection plane and the projection point to be one unit. This is referred to as the â€œfocal lengthâ€, not to be confused with â€œfocus distanceâ€, which we'll present later.



> Iâ€™ll put the â€œeyeâ€ (or camera center if you think of a camera) at (0,0,0)(0,0,0). I will have the y-axis go up, and the x-axis to the right. In order to respect the convention of a right handed coordinate system, into the screen is the negative z-axis. I will traverse the screen from the upper left hand corner, and use two offset vectors along the screen sides to move the ray endpoint across the screen. Note that I do not make the ray direction a unit length vector because I think not doing that makes for simpler and slightly faster code.



æˆ‘ä¼šæŠŠè§†ç‚¹(æˆ–è€…è¯´æ‘„åƒæœº, å¦‚æœä½ è®¤ä¸ºå®ƒæ˜¯ä¸ªæ‘„åƒæœºçš„è¯)æ”¾åœ¨(0,0,0)(0,0,0)ã€‚è¿™é‡Œyè½´å‘ä¸Š, xè½´å‘å³, ä¸ºäº†å‡†å®ˆä½¿ç”¨å·¦æ‰‹ç³»çš„è§„èŒƒ, æ‘„åƒæœºçœ‹å‘çš„æ–¹å‘ä¸ºzè½´çš„è´Ÿæ–¹å‘ã€‚æˆ‘ä¼šæŠŠå‘å‡ºå°„çº¿çš„åŸç‚¹ä»å›¾åƒçš„å·¦ä¸‹è§’å¼€å§‹æ²¿ç€xyæ–¹å‘åšå¢é‡ç›´è‡³éå†å…¨å›¾ã€‚æ³¨æ„æˆ‘è¿™é‡Œå¹¶æ²¡æœ‰å°†å°„çº¿çš„å‘é‡è®¾ç½®ä¸ºå•ä½å‘é‡, å› ä¸ºæˆ‘è®¤ä¸ºè¿™æ ·ä»£ç ä¼šæ›´åŠ ç®€å•å¿«æ·ã€‚

![](https://tva1.sinaimg.cn/large/008i3skNgy1gqb0up1aq5j30ww0by752.jpg)





> Below in code, the ray `r` goes to approximately the pixel centers (I wonâ€™t worry about exactness for now because weâ€™ll add antialiasing later):



ä¸‹é¢æ˜¯ä»£ç , å°„çº¿rç°åœ¨åªæ˜¯è¿‘ä¼¼çš„ä»å„ä¸ªåƒç´ çš„ä¸­å¿ƒå°„å‡º(ç°åœ¨ä¸å¿…æ‹…å¿ƒç²¾åº¦é—®é¢˜, å› ä¸ºæˆ‘ä»¬ä¸€ä¼šå„¿å°±ä¼šåŠ å…¥æŠ—é”¯é½¿):

```diff
#include "vec3.h"

#include <iostream>

int main() {
    const int image_width = 200;
    const int image_height = 100;

    std::cout << "P3\n" << image_width << ' ' << image_height << "\n255\n";

    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
+            vec3 color(double(i)/image_width, double(j)/image_height, 0.2);
+            color.write_color(std::cout);
        }
    }

    std::cerr << "\nDone.\n";
}
```



> The `ray_color(ray)` function linearly blends white and blue depending on the height of the ğ‘¦ coordinate *after* scaling the ray direction to unit length (so $âˆ’1.0<ğ‘¦<1.0$). Because we're looking at the ğ‘¦ height after normalizing the vector, you'll notice a horizontal gradient to the color in addition to the vertical gradient.
>
> 
>
> I then did a standard graphics trick of scaling that to $0.0â‰¤ğ‘¡â‰¤1.0$. When $ğ‘¡=1.0$ I want blue. When $ğ‘¡=0.0$ I want white. In between, I want a blend. This forms a â€œlinear blendâ€, or â€œlinear interpolationâ€, or â€œlerpâ€ for short, between two things. A lerp is always of the form



`ray_color(ray)`å‡½æ•°æ ¹æ®yå€¼å°†è“ç™½åšäº†ä¸ªçº¿æ€§å·®å€¼çš„æ··åˆ, æˆ‘ä»¬è¿™é‡ŒæŠŠå°„çº¿åšäº†ä¸ªå•ä½åŒ–, ä»¥ä¿è¯yçš„å–å€¼èŒƒå›´($âˆ’1.0<y<1.0$)ã€‚å› ä¸ºæˆ‘ä»¬ä½¿ç”¨yè½´åšæ¸å˜, æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªè“ç™½æ¸å˜ä¹Ÿæ˜¯ç«–ç›´çš„ã€‚

æˆ‘æ¥ä¸‹æ¥ä½¿ç”¨äº†ä¸€ä¸ªæ ‡å‡†çš„å°æŠ€å·§å°†yçš„èŒƒå›´ä»$âˆ’1.0â‰¤yâ‰¤1.0$æ˜ å°„åˆ°äº†$0â‰¤yâ‰¤1.0$ã€‚è¿™æ ·$t=1.0$æ—¶å°±æ˜¯è“è‰², è€Œ$t=0.0$æ—¶å°±æ˜¯ç™½è‰²ã€‚åœ¨è“ç™½ä¹‹é—´æˆ‘æƒ³è¦ä¸€ä¸ªæ··åˆæ•ˆæœ(blend)ã€‚ç°åœ¨æˆ‘ä»¬é‡‡ç”¨çš„æ˜¯çº¿æ€§æ··åˆ(linear blend)æˆ–è€…è¯´çº¿æ€§æ’å€¼(liner interpolation)ã€‚æˆ–è€…ç®€ç§°å…¶ä¸ºlerpã€‚ä¸€ä¸ªlerpä¸€èˆ¬æ¥è¯´ä¼šæ˜¯ä¸‹é¢çš„å½¢å¼:
$$
blendedValue=(1âˆ’t)â‹…startValue+tâ‹…endValue
$$


> with ğ‘¡ going from zero to one. In our case this produces:

å½“$t$ä»0åˆ°1, æˆ‘ä»¬ä¼šæ¸²æŸ“å‡ºè¿™æ ·çš„å›¾åƒ:

![](https://tva1.sinaimg.cn/large/008i3skNgy1gqb0ywiam4j30b40690sm.jpg)



### åŠ å…¥çƒä½“

> Letâ€™s add a single object to our ray tracer. People often use spheres in ray tracers because calculating whether a ray hits a sphere is pretty straightforward.

è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„å…‰çº¿è¿½è¸ªå™¨åŠ å…¥ä¸€ä¸ªç‰©ä½“å§!äººä»¬é€šå¸¸ä½¿ç”¨çš„çƒä½“, å› ä¸ºè®¡ç®—å°„çº¿æ˜¯å¦ä¸çƒä½“ç›¸äº¤æ˜¯ååˆ†ç®€æ´æ˜äº†çš„ã€‚



> Recall that the equation for a sphere centered at the origin of radius ğ‘…R is $x^2+y^2+z^2=R^2$. Put another way, if a given point $(ğ‘¥,ğ‘¦,ğ‘§)$ is on the sphere, then $x^2+y^2+z^2=R^2$. If the given point (ğ‘¥,ğ‘¦,ğ‘§)(x,y,z) is *inside* the sphere, then $x^2+y^2+z^2<R^2$, and if a given point$ (ğ‘¥,ğ‘¦,ğ‘§)$ is *outside* the sphere, then $x^2+y^2+z^2>R^2$.
>
> It gets uglier if the sphere center is at (ğ¶ğ‘¥,ğ¶ğ‘¦,ğ¶ğ‘§):

å›æƒ³ä¸€ä¸‹æˆ‘ä»¬ä¸­å­¦æ—¶æœŸå­¦è¿‡çš„çƒä½“è¡¨é¢æ–¹ç¨‹, å¯¹äºä¸€ä¸ªåŠå¾„ä¸ºrçš„çƒä½“æ¥è¯´, æœ‰æ–¹ç¨‹$x^2+y^2+z^2=R^2$, å…¶ä¸­$(x,y,z)$æ˜¯çƒé¢ä¸Šçš„ç‚¹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦è¡¨ç¤ºç‚¹$(x,y,z)$åœ¨çƒä½“çš„å†…éƒ¨, é‚£ä¾¿æœ‰æ–¹ç¨‹$x^2+y^2+z^2<R^2$, ç±»ä¼¼çš„, å¦‚æœè¦è¡¨ç¤ºçƒä½“å¤–éƒ¨çš„ç‚¹, åˆ™æœ‰$x^2+y^2+z^2>R^2$ã€‚

å¦‚æœçƒä½“çš„çƒå¿ƒåœ¨$(c_x,c_y,c_z)$, é‚£ä¹ˆè¿™ä¸ªå¼å­å°±ä¼šå˜å¾—ä¸‘é™‹ä¸€äº›:
$$
(xâˆ’c_x)2+(yâˆ’c_y)^2+(zâˆ’c_z)^2=R^2
$$


åœ¨å›¾å½¢å­¦ä¸­, ä½ æ€»å¸Œæœ›ä½ æ–¹ç¨‹é‡Œé¢æ‰€æœ‰ä¸œè¥¿éƒ½æ˜¯ç”¨å‘é‡è¡¨è¾¾çš„, è¿™æ ·æˆ‘ä»¬å°±èƒ½ç”¨`vec3`è¿™ä¸ªç±»æ¥å­˜å‚¨æ‰€æœ‰çš„è¿™äº›xyzç›¸å…³çš„ä¸œè¥¿äº†ã€‚ä½ ä¹Ÿè®¸ä¼šæ„è¯†åˆ°, å¯¹äºåˆ°çƒé¢ä¸Šçš„ç‚¹$P=(x,y,z)$åˆ°çƒå¿ƒ$c=(c_x,c_y,c_z)$çš„è·ç¦»å¯ä»¥ä½¿ç”¨å‘é‡è¡¨ç¤ºä¸º$(pâˆ’c)$, äºæ˜¯å°±æœ‰
$$
(pâˆ’c)â‹…(pâˆ’c)=(xâˆ’c_x)2+(yâˆ’c_y)2+(zâˆ’c_z)2
$$


> So the equation of the sphere in vector form is:

äºæ˜¯æˆ‘ä»¬å°±èƒ½å¾—åˆ°çƒé¢æ–¹ç¨‹çš„å‘é‡å½¢å¼:
$$
(pâˆ’c)â‹…(pâˆ’c)=R^2
$$


> We can read this as â€œany point **ğ** that satisfies this equation is on the sphereâ€. We want to know if our ray $P(t)=A+tb$ ever hits the sphere anywhere. If it does hit the sphere, there is some ğ‘¡t for which $P(t) $satisfies the sphere equation. So we are looking for any ğ‘¡t where this is true:

æˆ‘ä»¬å¯ä»¥å°†å…¶è§£è¯»ä¸ºâ€æ»¡è¶³æ–¹ç¨‹ä¸Šè¿°æ–¹ç¨‹çš„ä»»æ„ä¸€ç‚¹$p$ä¸€å®šä½äºçƒé¢ä¸Šâ€ã€‚æˆ‘ä»¬è¿˜è¦çŸ¥é“å°„çº¿$p(t)=a+t \vec b$æ˜¯å¦ä¸çƒä½“ç›¸äº¤ã€‚å¦‚æœè¯´å®ƒç›¸äº¤äº†, é‚£ä¹ˆè‚¯å®šæœ‰ä¸€ä¸ª$t$ä½¿ç›´çº¿ä¸Šçš„ç‚¹$p(t)$æ»¡è¶³çƒé¢æ–¹ç¨‹ã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆæ¥è®¡ç®—æ»¡è¶³æ¡ä»¶çš„ä»»æ„$t$å€¼:
$$
(p(t)âˆ’c)â‹…(p(t)âˆ’c)=R^2
$$


> or expanding the full form of the ray $P(t)$:

æˆ–è€…å°†$p(t)$å±•å¼€ä¸ºå°„çº¿æ–¹ç¨‹:
$$
(a+t \vec bâˆ’c)â‹…(a+t \vec bâˆ’c)=R^2
$$


> The rules of vector algebra are all that we would want here. If we expand that equation and move all the terms to the left hand side we get:

å¥½å•¦, æˆ‘ä»¬éœ€è¦çš„ä»£æ•°éƒ¨åˆ†å°±åˆ°è¿™é‡Œã€‚ç°åœ¨æˆ‘ä»¬æ¥å±•å¼€è¡¨è¾¾å¼å¹¶ç§»é¡¹, å¾—:
$$
t^2 \vec bâ‹…\vec b+2t \vec bâ‹…\vec{(aâˆ’c)}+ \vec{(aâˆ’c)}â‹…\vec {(aâˆ’c)}âˆ’R^2=0
$$


> The vectors and ğ‘Ÿr in that equation are all constant and known. The unknown is ğ‘¡, and the equation is a quadratic, like you probably saw in your high school math class. You can solve for ğ‘¡t and there is a square root part that is either positive (meaning two real solutions), negative (meaning no real solutions), or zero (meaning one real solution). In graphics, the algebra almost always relates very directly to the geometry. What we have is:

æ–¹ç¨‹ä¸­çš„å‘é‡å’ŒåŠå¾„Réƒ½æ˜¯å·²çŸ¥çš„å¸¸é‡, å”¯ä¸€çš„æœªçŸ¥æ•°å°±æ˜¯t, å¹¶ä¸”è¿™ä¸ªç­‰å¼æ˜¯å…³äºtçš„ä¸€ä¸ªä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹, å°±åƒä½ åœ¨é«˜ä¸­æ•°å­¦è¯¾ä¸Šã€ï¼Ÿï¼Ÿï¼Ÿã€‘å­¦åˆ°çš„é‚£æ ·ã€‚ä½ å¯ä»¥ç”¨æ±‚æ ¹å…¬å¼æ¥åˆ¤åˆ«äº¤ç‚¹ä¸ªæ•°, ä¸ºæ­£åˆ™2ä¸ªäº¤ç‚¹, ä¸ºè´Ÿåˆ™1ä¸ªäº¤ç‚¹, ä¸º0åˆ™æ²¡æœ‰äº¤ç‚¹ã€‚åœ¨å›¾å½¢å­¦ä¸­, ä»£æ•°ä¸å‡ ä½•å¾€å¾€å¯†åˆ‡ç›¸å…³, ä½ çœ‹å›¾:



![](https://tva1.sinaimg.cn/large/008i3skNgy1gqb12l4m1vj30ig0cb3yw.jpg)



> If we take that math and hard-code it into our program, we can test it by coloring red any pixel that hits a small sphere we place at âˆ’1 on the z-axis:

å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä»£ç æ¥æ±‚è§£, å¹¶ä½¿ç”¨çº¢è‰²æ¥è¡¨ç¤ºå°„çº¿å‡»ä¸­æˆ‘ä»¬æ”¾åœ¨$(0,0,-1)$çš„å°çƒ:

```diff
bool hit_sphere(const vec3& center, double radius, const ray& r) {
    vec3 oc = r.origin() - center;
    auto a = dot(r.direction(), r.direction());
    auto b = 2.0 * dot(oc, r.direction());
    auto c = dot(oc, oc) - radius*radius;
    auto discriminant = b*b - 4*a*c;
    return (discriminant > 0);
}

+vec3 ray_color(const ray& r) {
+    if (hit_sphere(vec3(0,0,-1), 0.5, r))
+        return vec3(1, 0, 0);
    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```



æˆ‘ä»¬ä¼šå¾—åˆ°:

![](https://tva1.sinaimg.cn/large/008i3skNgy1gqb1d0g7fuj30b40690sm.jpg)

ç°åœ¨æˆ‘ä»¬å•¥éƒ½ç¼º: ä¾‹å¦‚å…‰ç…§, åå°„, åŠ å…¥æ›´å¤šçš„ç‰©ä½“, ä½†æ˜¯æˆ‘ä»¬ç¦»æˆåŠŸåˆè¿‘äº†ä¸€æ­¥!ç°åœ¨ä½ è¦æ³¨æ„æˆ‘ä»¬å…¶å®æ±‚çš„æ˜¯ç›´çº¿ä¸çƒç›¸äº¤çš„è§£, $t<0$çš„é‚£äº›æƒ…å†µä¹Ÿè®¡ç®—è¿›å»äº†, è€Œæˆ‘ä»¬åªæƒ³è¦ç›´çº¿ä¸­ä¸€æ®µå°„çº¿çš„è§£ã€‚å¦‚æœä½ å°†ä½ çš„çƒå¿ƒè®¾ç½®åœ¨$(0,0,1)$ä½ ä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„ç»“æœã€‚è¿™ä¸æ˜¯ä¸€ä¸ªç‰¹æ€§(feature)!ã€åæ§½: ç›´æ¥è¯´itâ€™s a bugå˜›ã€‘æˆ‘ä»¬ä¼šåœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¿®å¤è¿™ä¸ªbugã€‚



### é¢æ³•çº¿ä¸å¤æ•°ç‰©ä½“

> First, letâ€™s get ourselves a surface normal so we can shade. This is a vector that is perpendicular to the surface at the point of intersection. There are two design decisions to make for normals. The first is whether these normals are unit length. That is convenient for shading so I will say yes, but I wonâ€™t enforce that in the code. This could allow subtle bugs, so be aware this is personal preference as are most design decisions like that. For a sphere, the outward normal is in the direction of the hit point minus the center:



ä¸ºäº†æ¥ç»™çƒä½“ç€è‰², é¦–å…ˆæˆ‘ä»¬æ¥å®šä¹‰ä¸€ä¸‹é¢æ³•çº¿ã€‚é¢æ³•çº¿åº”è¯¥æ˜¯ä¸€ç§å‚ç›´äºäº¤ç‚¹æ‰€åœ¨å¹³é¢çš„ä¸‰ç»´å‘é‡ã€‚å…³äºé¢æ³•çº¿æˆ‘ä»¬å­˜åœ¨ä¸¤ä¸ªè®¾è®¡æŠ‰æ‹©ã€‚é¦–å…ˆæ˜¯æ˜¯å¦å°†å…¶è®¾è®¡ä¸ºå•ä½å‘é‡, è¿™æ ·å¯¹äºç€è‰²å™¨æ¥è¯´, æ‰€ä»¥æˆ‘ä¼šè¯´â€yes!â€ä½†æ˜¯æˆ‘å¹¶æ²¡æœ‰åœ¨ä»£ç é‡Œè¿™ä¹ˆåš, è¿™éƒ¨åˆ†å·®å¼‚å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æ½œåœ¨çš„bugã€‚æ‰€ä»¥è®°ä½, è¿™ä¸ªæ˜¯ä¸ªäººå–œå¥½, å¤§å¤šæ•°çš„äººå–œå¥½ä½¿ç”¨å•ä½æ³•çº¿ã€‚å¯¹äºçƒä½“æ¥è¯´, æœå¤–çš„æ³•çº¿æ˜¯ç›´çº¿ä¸çƒçš„äº¤ç‚¹å‡å»çƒå¿ƒ:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.05-sphere-normal.jpg)



> On the earth, this implies that the vector from the earthâ€™s center to you points straight up. Letâ€™s throw that into the code now, and shade it. We donâ€™t have any lights or anything yet, so letâ€™s just visualize the normals with a color map. A common trick used for visualizing normals (because itâ€™s easy and somewhat intuitive to assume **ğ§** is a unit length vector â€” so each component is between âˆ’1 and 1) is to map each component to the interval from 0 to 1, and then map x/y/z to r/g/b. For the normal, we need the hit point, not just whether we hit or not. We only have one sphere in the scene, and it's directly in front of the camera, so we won't worry about negative values of ğ‘¡t yet. We'll just assume the closest hit point (smallest ğ‘¡t). These changes in the code let us compute and visualize **ğ§**:



è¯´åˆ°åº•, å…¶å®å°±æ˜¯ä»çƒå¿ƒåˆ°äº¤ç‚¹å†å‘å¤–å»¶ä¼¸çš„é‚£ä¸ªæ–¹å‘ã€‚è®©æˆ‘ä»¬æŠŠè¿™éƒ¨åˆ†è½¬å˜æˆä»£ç å¹¶å¼€å§‹ç€è‰²ã€‚æˆ‘ä»¬æš‚æ—¶è¿˜æ²¡æœ‰å…‰æºè¿™æ ·çš„ä¸œè¥¿, æ‰€ä»¥è®©æˆ‘ä»¬ç›´æ¥å°†æ³•çº¿å€¼ä½œä¸ºé¢œè‰²è¾“å‡ºå§ã€‚å¯¹äºæ³•çº¿å¯è§†åŒ–æ¥è¯´, æˆ‘ä»¬å¸¸å¸¸å°†xyzåˆ†é‡çš„å€¼å…ˆæ˜ å°„åˆ°0åˆ°1çš„èŒƒå›´(å‡å®š`vecN`æ˜¯ä¸€ä¸ªå•ä½å‘é‡, å®ƒçš„å–å€¼èŒƒå›´æ˜¯-1åˆ°1çš„),å†æŠŠå®ƒèµ‹å€¼ç»™rgbã€‚å¯¹äºæ³•çº¿æ¥è¯´, å…‰èƒ½åˆ¤æ–­å°„çº¿æ˜¯å¦ä¸çƒä½“ç›¸äº¤æ˜¯ä¸å¤Ÿçš„, æˆ‘ä»¬è¿˜éœ€æ±‚å‡ºäº¤ç‚¹çš„åæ ‡ã€‚åœ¨æœ‰ä¸¤ä¸ªäº¤ç‚¹çš„æƒ…å†µä¸‹, æˆ‘ä»¬é€‰å–æœ€è¿‘çš„äº¤ç‚¹smallest(t)ã€‚è®¡ç®—ä¸å¯è§†åŒ–çƒçš„æ³•å‘é‡çš„ä»£ç å¦‚ä¸‹:

```diff
+double hit_sphere(const vec3& center, double radius, const ray& r) {
    vec3 oc = r.origin() - center;
    auto a = dot(r.direction(), r.direction());
    auto b = 2.0 * dot(oc, r.direction());
    auto c = dot(oc, oc) - radius*radius;
    auto discriminant = b*b - 4*a*c;
+    if (discriminant < 0) {
+        return -1.0;
+    } else {
+        return (-b - sqrt(discriminant) ) / (2.0*a);
+    }
}

vec3 ray_color(const ray& r) {
+    auto t = hit_sphere(vec3(0,0,-1), 0.5, r);
+    if (t > 0.0) {
+        vec3 N = unit_vector(r.at(t) - vec3(0,0,-1));
+        return 0.5*vec3(N.x()+1, N.y()+1, N.z()+1);
+    }
    vec3 unit_direction = unit_vector(r.direction());
+    t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```



è¿™ä¼šå¾—åˆ°ä¸‹é¢çš„ç»“æœ:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.04-normals-sphere.png)



è¿˜æœ‰ä¸€ç§åŸºäºè®¡ç®—è¿‡ç¨‹å¯¹ä»£ç çš„ç®€åŒ–ï¼Œæˆ‘ä¹Ÿå†™å‡ºæ¥ä¾›å‚è€ƒï¼Œä½†è¿™ä¸ªç®€åŒ–æ•ˆç‡æå‡ä¸å¤šï¼Œæ‰€ä»¥ä»£ç ä¿ç•™ä¹‹å‰å¥½ç†è§£çš„ã€‚

> Letâ€™s revisit the ray-sphere equation:


æˆ‘ä»¬å†æ¥å›é¡¾ä¸Šé¢çš„ç›´çº¿æ–¹ç¨‹:

```c++
vec3 oc = r.origin() - center;
auto a = dot(r.direction(), r.direction());
auto b = 2.0 * dot(oc, r.direction());
auto c = dot(oc, oc) - radius*radius;
auto discriminant = b*b - 4*a*c;
```



> First, recall that a vector dotted with itself is equal to the squared length of that vector.
>
> Second, notice how the equation for `b` has a factor of two in it. Consider what happens to the quadratic equation if ğ‘=2â„:

é¦–å…ˆ, å›æƒ³ä¸€ä¸‹ä¸€ä¸ªå‘é‡ä¸è‡ªå·±çš„ç‚¹ç§¯å°±æ˜¯å®ƒçš„é•¿åº¦çš„å¹³æ–¹(éƒ½æ˜¯$x^2+y^2+z^2$)

å…¶æ¬¡, æ³¨æ„å…¶å®æˆ‘ä»¬çš„`b`æœ‰ä¸€ä¸ªç³»æ•°2, æˆ‘ä»¬è®¾`b=2h`, æœ‰:

$\frac{-b +/- \sqrt{b^2-4ac}}{2a} = \frac{-2h +/- \sqrt{(2h)^2-4ac}}{2a} = \frac{-h +/- \sqrt{h^2-ac}}{a}$



> Using these observations, we can now simplify the sphere-intersection code to this:

æ‰€ä»¥å°„çº¿ä¸çƒä½“æ±‚äº¤çš„ä»£ç å…¶å®å¯ä»¥ç®€åŒ–æˆä¸‹é¢è¿™æ ·:

```diff
vec3 oc = r.origin() - center;
-auto a = dot(r.direction(), r.direction());
-auto b = 2.0 * dot(oc, r.direction());
-auto c = dot(oc, oc) - radius*radius;
-auto discriminant = b*b - 4*a*c;
+auto a = r.direction().length_squared();
+auto half_b = dot(oc, r.direction());
+auto c = oc.length_squared() - radius*radius;
+auto discriminant = half_b*half_b - a*c;

if (discriminant < 0) {
    return -1.0;
} else {
-   return (-b - sqrt(discriminant) ) / (2.0*a);
+   return (-half_b - sqrt(discriminant) ) / a;
}
```



> Now, how about several spheres? While it is tempting to have an array of spheres, a very clean solution is the make an â€œabstract classâ€ for anything a ray might hit, and make both a sphere and a list of spheres just something you can hit. What that class should be called is something of a quandary â€” calling it an â€œobjectâ€ would be good if not for â€œobject orientedâ€ programming. â€œSurfaceâ€ is often used, with the weakness being maybe we will want volumes. â€œhittableâ€ emphasizes the member function that unites them. I donâ€™t love any of these, but I will go with â€œhittableâ€.

å¥½å•¦, é‚£ä¹ˆæ€ä¹ˆåœ¨åœºæ™¯ä¸­æ¸²æŸ“ä¸æ­¢ä¸€ä¸ªçƒå‘¢? å¾ˆç›´æ¥çš„æˆ‘ä»¬æƒ³åˆ°ä½¿ç”¨ä¸€ä¸ªsphereæ•°ç»„, è¿™é‡Œæœ‰ä¸ªå¾ˆç®€æ´çš„å¥½æ–¹æ³•: ä½¿ç”¨ä¸€ä¸ªæŠ½è±¡ç±», ä»»ä½•å¯èƒ½ä¸å…‰çº¿æ±‚äº¤çš„ä¸œè¥¿å®ç°æ—¶éƒ½ç»§æ‰¿è¿™ä¸ªç±», å¹¶ä¸”è®©çƒä»¥åŠçƒåˆ—è¡¨ä¹Ÿéƒ½ç»§æ‰¿è¿™ä¸ªç±»ã€‚æˆ‘ä»¬è¯¥ç»™è¿™ä¸ªç±»èµ·ä¸ªä»€ä¹ˆæ ·çš„åå­—å‘¢? å«å®ƒ`object`å¥½åƒä¸é”™ä½†ç°åœ¨æˆ‘ä»¬ä½¿ç”¨é¢å‘å¯¹è±¡ç¼–ç¨‹(oop)ã€‚`suface`æ˜¯æ—¶å¸¸è¢«ç¿»ç‰Œ, ä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä½“ç§¯ä½“(volumes)çš„è¯å°±ä¸å¤ªé€‚åˆäº†ã€‚`hittable`åˆè¿‡äºå¼ºè°ƒäº†è‡ªå·±çš„æˆå‘˜å‡½æ•°`hit`ã€‚æ‰€ä»¥æˆ‘å“ªä¸ªéƒ½ä¸å–œæ¬¢, ä½†æ˜¯æ€»å¾—ç»™å®ƒä¸ªåå­—çš„å˜›, é‚£æˆ‘å°±å«å®ƒ`hittable`ã€‚



> This `hittable` abstract class will have a hit function that takes in a ray. Most ray tracers have found it convenient to add a valid interval for hits ğ‘¡ğ‘šğ‘–ğ‘›tmin to ğ‘¡ğ‘šğ‘ğ‘¥tmax, so the hit only â€œcountsâ€ if $ğ‘¡_{ğ‘šğ‘–ğ‘›}<ğ‘¡<ğ‘¡_{ğ‘šğ‘ğ‘¥}$. For the initial rays this is positive ğ‘¡t, but as we will see, it can help some details in the code to have an interval $ğ‘¡_{ğ‘šğ‘–ğ‘›}$ to $ğ‘¡_{ğ‘šğ‘ğ‘¥}$. One design question is whether to do things like compute the normal if we hit something. We might end up hitting something closer as we do our search, and we will only need the normal of the closest thing. I will go with the simple solution and compute a bundle of stuff I will store in some structure. Hereâ€™s the abstract class:

`hittable`ç±»ç†åº”æœ‰ä¸ªæ¥å—å°„çº¿ä¸ºå‚æ•°çš„å‡½æ•°, è®¸å¤šå…‰çº¿è¿½è¸ªå™¨ä¸ºäº†ä¾¿åˆ©, åŠ å…¥äº†ä¸€ä¸ªåŒºé—´$t_{min}<t<t_{max}$æ¥åˆ¤æ–­ç›¸äº¤æ˜¯å¦æœ‰æ•ˆã€‚å¯¹äºä¸€å¼€å§‹çš„å…‰çº¿æ¥è¯´, è¿™ä¸ªtå€¼æ€»æ˜¯æ­£çš„, ä½†åŠ å…¥è¿™éƒ¨åˆ†å¯¹ä»£ç å®ç°çš„ä¸€äº›ç»†èŠ‚æœ‰ç€ä¸é”™çš„å¸®åŠ©ã€‚ç°åœ¨æœ‰ä¸ªè®¾è®¡ä¸Šçš„é—®é¢˜:æˆ‘ä»¬æ˜¯å¦åœ¨æ¯æ¬¡è®¡ç®—æ±‚äº¤çš„æ—¶å€™éƒ½è¦å»è®¡ç®—æ³•ç›¸?ä½†å…¶å®æˆ‘ä»¬åªéœ€è¦è®¡ç®—ç¦»å°„çº¿åŸç‚¹æœ€è¿‘çš„é‚£ä¸ªäº¤ç‚¹çš„æ³•ç›¸å°±è¡Œäº†, åé¢çš„ä¸œè¥¿ä¼šè¢«é®æŒ¡ã€‚æ¥ä¸‹æ¥æˆ‘ä¼šç»™å‡ºæˆ‘çš„ä»£ç , å¹¶å°†ä¸€äº›è®¡ç®—çš„ç»“æœå­˜åœ¨ä¸€ä¸ªç»“æ„ä½“é‡Œ, æ¥çœ‹, è¿™å°±æ˜¯é‚£ä¸ªæŠ½è±¡ç±»:

```c++
#ifndef HITTABLE_H
#define HITTABLE_H

#include "ray.h"

struct hit_record {
    vec3 p;
    vec3 normal;
    double t;
};

class hittable {
    public:
        virtual bool hit(const ray& r, double t_min, double t_max, hit_record& rec) const = 0;
};

#endif
```



> And hereâ€™s the sphere:

```c++
#ifndef SPHERE_H
#define SPHERE_H

#include "hittable.h"
#include "vec3.h"

class sphere: public hittable {
    public:
        sphere() {}
        sphere(vec3 cen, double r) : center(cen), radius(r) {};

        virtual bool hit(const ray& r, double tmin, double tmax, hit_record& rec) const;

    public:
        vec3 center;
        double radius;
};

bool sphere::hit(const ray& r, double t_min, double t_max, hit_record& rec) const {
    vec3 oc = r.origin() - center;
    auto a = r.direction().length_squared();
    auto half_b = dot(oc, r.direction());
    auto c = oc.length_squared() - radius*radius;
    auto discriminant = half_b*half_b - a*c;

    if (discriminant > 0) {
        auto root = sqrt(discriminant);
        auto temp = (-half_b - root)/a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
            rec.normal = (rec.p - center) / radius;
            return true;
        }
        temp = (-half_b + root) / a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
            rec.normal = (rec.p - center) / radius;
            return true;
        }
    }
    return false;
}


#endif
```



> The second design decision for normals is whether they should always point out. At present, the normal found will always be in the direction of the center to the intersection point (the normal points out). If the ray intersects the sphere from the outside, the normal points against the ray. If the ray intersects the sphere from the inside, the normal (which always points out) points with the ray. Alternatively, we can have the normal always point against the ray. If the ray is outside the sphere, the normal will point outward, but if the ray is inside the sphere, the normal will point inward.



å¥½äº†, è®©æˆ‘ä»¬æ¥è°ˆè°ˆç¬¬äºŒä¸ªå…³äºé¢æ³•ç›¸è®¾è®¡ä¸Šçš„é—®é¢˜å§ï¼Œ é‚£å°±æ˜¯é¢æ³•ç›¸çš„æœå‘é—®é¢˜ã€‚å¯¹äºç°åœ¨æ¥è¯´, å¦‚æœå…‰çº¿ä»çƒä½“å¤–éƒ¨å‡»ä¸­çƒä½“, é‚£ä¹ˆæ³•ç›¸ä¹Ÿæ˜¯æœå¤–çš„, ä¸å°„çº¿çš„æ–¹å‘ç›¸å(ä¸æ˜¯æ•°å­¦æ„ä¹‰ä¸Šçš„ä¸¥æ ¼ç›¸å, åªæ˜¯å¤§è‡´é€†ç€)ã€‚å¦‚æœå…‰çº¿ä»å†…éƒ¨å°„å‘çƒé¢æ—¶, æ­¤æ—¶çš„é¢æ³•ç›¸ä¾ç„¶æœå¤–, ä¸å°„çº¿æ–¹å‘ç›¸åŒã€‚ç›¸å¯¹çš„, æˆ‘ä»¬ä¹Ÿå¯ä»¥æ€»æ˜¯è®©æ³•ç›¸å‘é‡ä¸å°„çº¿æ–¹å‘ç›¸å, å³å°„çº¿ä»å¤–éƒ¨å°„å‘çƒé¢, æ³•å‘é‡æœå¤–, å°„çº¿ä»å†…éƒ¨å°„å‘çƒé¢, æ³•å‘é‡å‘ç€çƒå¿ƒã€‚

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.06-normal-sides.jpg)



> We need to choose one of these possibilities because we will eventually want to determine which side of the surface that the ray is coming from. This is important for objects that are rendered differently on each side, like the text on a two-sided sheet of paper, or for objects that have an inside and an outside, like glass balls.

åœ¨æˆ‘ä»¬ç€è‰²å‰, æˆ‘ä»¬éœ€è¦ä»”ç»†è€ƒè™‘ä¸€ä¸‹é‡‡ç”¨ä¸Šé¢å“ªç§æ–¹å¼, è¿™å¯¹äºåŒé¢æè´¨æ¥è¯´è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ä¸€å¼ åŒé¢æ‰“å°çš„A4çº¸, æˆ–è€…ç»ç’ƒçƒè¿™æ ·çš„åŒæ—¶å…·æœ‰å†…è¡¨é¢å’Œå¤–è¡¨é¢çš„ç‰©ä½“ã€‚



> If we decide to have the normals always point out, then we will need to determine which side the ray is on when we color it. We can figure this out by comparing the ray with the normal. If the ray and the normal face in the same direction, the ray is inside the object, if the ray and the normal face in the opposite direction, then the ray is outside the object. This can be determined by taking the dot product of the two vectors, where if their dot is positive, the ray is inside the sphere.

å¦‚æœæˆ‘ä»¬å†³å®šè®©æ³•ç›¸æ°¸è¿œæœå¤–, é‚£åœ¨æˆ‘ä»¬å°±å¾—åœ¨å°„å…¥çš„æ—¶å€™åˆ¤æ–­æ˜¯ä»è¡¨é¢çš„å“ªä¸€ä¾§å°„å…¥çš„, æˆ‘ä»¬å¯ä»¥ç®€å•çš„å°†å…‰çº¿ä¸æ³•ç›¸åšç‚¹ä¹˜æ¥åˆ¤æ–­ã€‚å¦‚æœæ³•ç›¸ä¸å…‰çº¿æ–¹å‘ç›¸åŒ, é‚£å°±æ˜¯ä»å†…éƒ¨å‡»ä¸­å†…è¡¨é¢, å¦‚æœç›¸ååˆ™æ˜¯ä»å¤–éƒ¨å‡»ä¸­å¤–è¡¨é¢ã€‚ã€è¯‘æ³¨: $dot(a,b)=cosÎ¸|a||b|$ã€‘

```c++
if (dot(ray_direction, outward_normal) > 0.0) {
    // ray is inside the sphere
    ...
} else {
    // ray is outside the sphere
    ...
}
```



> If we decide to have the normals always point against the ray, we won't be able to use the dot product to determine which side of the surface the ray is on. Instead, we would need to store that information:

å¦‚æœæˆ‘ä»¬æ°¸è¿œè®©æ³•ç›¸ä¸å…¥å°„æ–¹å‘ç›¸å, æˆ‘ä»¬å°±ä¸ç”¨å»ç”¨ç‚¹ä¹˜æ¥åˆ¤æ–­å°„å…¥é¢æ˜¯å†…ä¾§è¿˜æ˜¯å¤–ä¾§äº†, ä½†ç›¸å¯¹çš„, æˆ‘ä»¬éœ€è¦ç”¨ä¸€ä¸ªå˜é‡å‚¨å­˜æ‘„å…¥é¢çš„ä¿¡æ¯:



```c++
bool front_face;
if (dot(ray_direction, outward_normal) > 0.0) {
    // ray is inside the sphere
    normal = -outward_normal;
    front_face = false;
}
else {
    // ray is outside the sphere
    normal = outward_normal;
    front_face = true;
}
```



> We can set things up so that normals always point â€œoutwardâ€ from the surface, or always point against the incident ray. This decision is determined by whether you want to determine the side of the surface at the time of geometry intersection or at the time of coloring. In this book we have more material types than we have geometry types, so we'll go for less work and put the determination at geometry time. This is simply a matter of preference, and you'll see both implementations in the literature.
>
> We add the `front_face` bool to the `hit_record` struct. We'll also add a function to solve this calculation for us.

å…¶å®é‡‡å–å“ªç§ç­–ç•¥, å…³é”®åœ¨äºä½ æƒ³æŠŠè¿™éƒ¨åˆ†æ”¾åœ¨ç€è‰²é˜¶æ®µè¿˜æ˜¯å‡ ä½•æ±‚äº¤çš„é˜¶æ®µã€‚ã€è¯‘æ³¨:åæ­£éƒ½è¦ç®—çš„, v2.0çš„æ—¶å€™æ˜¯åœ¨ç€è‰²é˜¶æ®µåˆ¤åˆ«çš„, v3.0æŠŠå®ƒæ”¾åœ¨äº†æ±‚äº¤é˜¶æ®µã€‘ã€‚åœ¨æœ¬ä¹¦ä¸­æˆ‘ä»¬æˆ‘ä»¬çš„æè´¨ç±»å‹ä¼šæ¯”æˆ‘ä»¬çš„å‡ ä½•å›¾å…ƒç±»å‹å¤š, æ‰€ä»¥ä¸ºäº†æœ‰æ›´å°‘çš„ä»£ç é‡, æˆ‘ä»¬ä¼šåœ¨å‡ ä½•éƒ¨åˆ†å…ˆåˆ¤åˆ«å°„å…¥é¢æ˜¯å†…ä¾§è¿˜æ˜¯å¤–ä¾§ã€‚è¿™å½“ç„¶ä¹Ÿæ˜¯ä¸€ç§ä¸ªäººå–œå¥½ã€‚

æˆ‘ä»¬åœ¨ç»“æ„ä½“`hit_record`ä¸­åŠ å…¥`front_face`å˜é‡, æˆ‘ä»¬æ¥ä¸‹æ¥è¿˜ä¼šå¼„ä¸€äº›åŠ¨æ€æ¨¡ç³Šç›¸å…³çš„äº‹æƒ…(Book2 chapter1),æ‰€ä»¥æˆ‘è¿˜ä¼šåŠ å…¥ä¸€ä¸ªæ—¶é—´å˜é‡:

```diff
ifndef HITTABLE_H
#define HITTABLE_H

#include "ray.h"

struct hit_record {
    vec3 p;
    vec3 normal;
+    double t;
+    bool front_face;

+    inline void set_face_normal(const ray& r, const vec3& outward_normal) {
        front_face = dot(r.direction(), outward_normal) < 0;
        normal = front_face ? outward_normal :-outward_normal;
    }
};

class hittable {
    public:
        virtual bool hit(const ray& r, double t_min, double t_max, hit_record& rec) const = 0;
};
#endif
```



> And then we add the surface side determination to the class:

æ¥ä¸‹æ¥æˆ‘ä»¬åœ¨æ±‚äº¤æ—¶åŠ å…¥å°„å…¥é¢çš„åˆ¤åˆ«:

```diff
bool sphere::hit(const ray& r, double t_min, double t_max, hit_record& rec) const {
    vec3 oc = r.origin() - center;
    auto a = r.direction().length_squared();
    auto half_b = dot(oc, r.direction());
    auto c = oc.length_squared() - radius*radius;
    auto discriminant = half_b*half_b - a*c;

    if (discriminant > 0) {
        auto root = sqrt(discriminant);
        auto temp = (-half_b - root)/a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
+            vec3 outward_normal = (rec.p - center) / radius;
+            rec.set_face_normal(r, outward_normal);
            return true;
        }
        temp = (-half_b + root) / a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
+            vec3 outward_normal = (rec.p - center) / radius;
+            rec.set_face_normal(r, outward_normal);
            return true;
        }
    }
    return false;
}
```



æˆ‘ä»¬åŠ å…¥å­˜æ”¾ç‰©ä½“çš„åˆ—è¡¨ï¼š

```c++
#ifndef HITTABLE_LIST_H
#define HITTABLE_LIST_H

#include "hittable.h"
#include <memory>
#include <vector>

using std::shared_ptr;
using std::make_shared;

class hittable_list: public hittable {
    public:
        hittable_list() {}
        hittable_list(shared_ptr<hittable> object) { add(object); }

        void clear() { objects.clear(); }
        void add(shared_ptr<hittable> object) { objects.push_back(object); }

        virtual bool hit(const ray& r, double tmin, double tmax, hit_record& rec) const;

    public:
        std::vector<shared_ptr<hittable>> objects;
};

bool hittable_list::hit(const ray& r, double t_min, double t_max, hit_record& rec) const {
    hit_record temp_rec;
    bool hit_anything = false;
    auto closest_so_far = t_max;

    for (const auto& object : objects) {
        if (object->hit(r, t_min, closest_so_far, temp_rec)) {
            hit_anything = true;
            closest_so_far = temp_rec.t;
            rec = temp_rec;
        }
    }

    return hit_anything;
}

#endif
```







#### ä¸€äº›C++çš„æ–°ç‰¹æ€§

> The `hittable_list` class code uses two C++ features that may trip you up if you're not normally a C++ programmer: `vector` and `shared_ptr`.

`hittable_list`ç±»ä½¿ç”¨äº†ä¸¤ç§C++çš„ç‰¹æ€§:`vector`å’Œ`shared_ptr`, å¦‚æœä½ å¹¶ä¸ç†Ÿæ‚‰C++, ä½ å¯èƒ½ä¼šæ„Ÿåˆ°æœ‰äº›å›°æƒ‘ã€‚



##### shared_ptr

> `shared_ptr<type>` is a pointer to some allocated type, with reference-counting semantics. Every time you assign its value to another shared pointer (usually with a simple assignment), the reference count is incremented. As shared pointers go out of scope (like at the end of a block or function), the reference count is decremented. Once the count goes to zero, the object is deleted.

`shared_ptr<type>`æ˜¯æŒ‡å‘ä¸€äº›å·²åˆ†é…å†…å­˜çš„ç±»å‹çš„æŒ‡é’ˆã€‚æ¯å½“ä½ å°†å®ƒçš„å€¼èµ‹å€¼ç»™å¦ä¸€ä¸ªæ™ºèƒ½æŒ‡é’ˆæ—¶, ç‰©ä½“çš„å¼•ç”¨è®¡æ•°å™¨å°±ä¼š+1ã€‚å½“æ™ºèƒ½æŒ‡é’ˆç¦»å¼€å®ƒæ‰€åœ¨çš„ç”Ÿå­˜èŒƒå›´(ä¾‹å¦‚ä»£ç å—æˆ–è€…å‡½æ•°å¤–), ç‰©ä½“çš„å¼•ç”¨è®¡æ•°å™¨å°±ä¼š-1ã€‚ä¸€æ—¦å¼•ç”¨è®¡æ•°å™¨ä¸º0, å³æ²¡æœ‰ä»»ä½•æ™ºèƒ½æŒ‡é’ˆæŒ‡å‘è¯¥ç‰©ä½“æ—¶, è¯¥ç‰©ä½“å°±ä¼šè¢«é”€æ¯ã€‚



> Typically, a shared pointer is first initialized with a newly-allocated object, something like this:

ä¸€èˆ¬æ¥è¯´, æ™ºèƒ½æŒ‡é’ˆé¦–å…ˆç”±ä¸€ä¸ªåˆšåˆšæ–°åˆ†é…å¥½å†…å­˜çš„ç‰©ä½“æ¥åˆå§‹åŒ–:

```c++
shared_ptr<double> double_ptr = make_shared<double>(0.37);
shared_ptr<vec3>   vec3_ptr   = make_shared<vec3>(1.414214, 2.718281, 1.618034);
shared_ptr<sphere> sphere_ptr = make_shared<sphere>(vec3(0,0,0), 1.0);
```



> `make_shared<thing>(thing_constructor_params ...)` allocates a new instance of type `thing`, using the constructor parameters. It returns a `shared_ptr<thing>`.
>
> Since the type can be automatically deduced by the return type of `make_shared<type>(...)`, the above lines can be more simply expressed using C++'s `auto` type specifier:

`make_shared<thing>(thing_constructor_params ...)`ä¸ºæŒ‡å®šçš„ç±»å‹åˆ†é…ä¸€æ®µå†…å­˜, ä½¿ç”¨ä½ æŒ‡å®šçš„æ„é€ å‡½æ•°ä¸å‚æ•°æ¥åˆ›å»ºè¿™ä¸ªç±», å¹¶è¿”å›ä¸€ä¸ªæ™ºèƒ½æŒ‡é’ˆ`shared_ptr<thing>`

ä½¿ç”¨C++çš„`auto`ç±»å‹å…³é”®å­—, å¯ä»¥è‡ªåŠ¨æ¨æ–­`make_shared<type>`è¿”å›çš„æ™ºèƒ½æŒ‡é’ˆç±»å‹, äºæ˜¯æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„ä»£ç ç®€åŒ–ä¸º:

```c++
auto double_ptr = make_shared<double>(0.37);
auto vec3_ptr   = make_shared<vec3>(1.414214, 2.718281, 1.618034);
auto sphere_ptr = make_shared<sphere>(vec3(0,0,0), 1.0);
```



> We'll use shared pointers in our code, because it allows multiple geometries to share a common instance (for example, a bunch of spheres that all use the same texture map material), and because it makes memory management automatic and easier to reason about.
>
> `std::shared_ptr` is included with the `<memory>` header.



æˆ‘ä»¬åœ¨ä»£ç ä¸­ä½¿ç”¨æ™ºèƒ½æŒ‡é’ˆçš„ç›®çš„æ˜¯ä¸ºäº†èƒ½è®©å¤šä¸ªå‡ ä½•å›¾å…ƒå…±äº«ä¸€ä¸ªå®ä¾‹(ä¸¾ä¸ªæ —å­, ä¸€å †ä¸åŒçƒä½“ä½¿ç”¨åŒä¸€ä¸ªçº¹ç†æè´¨), å¹¶ä¸”è¿™æ ·å†…å­˜ç®¡ç†æ¯”èµ·æ™®é€šçš„æŒ‡é’ˆæ›´åŠ çš„ç®€å•æ–¹ä¾¿ã€‚

`std::shared_ptr`åœ¨å¤´æ–‡ä»¶`<memory>`ä¸­:

```c++
#include <memory>
```





##### vector

> The second C++ feature you may be unfamiliar with is `std::vector`. This is a generic array-like collection of an arbitrary type. Above, we use a collection of pointers to `hittable`. `std::vector` automatically grows as more values are added: `objects.push_back(object)` adds a value to the end of the `std::vector` member variable `objects`.

ç¬¬äºŒä¸ªä½ å¯èƒ½ä¸å¤ªç†Ÿæ‚‰çš„C++ç‰¹æ€§æ˜¯`std::vector`ã€‚è¿™æ˜¯ä¸€ä¸ªç±»ä¼¼æ•°ç»„çš„ç»“æ„ç±»å‹, å¯ä»¥å­˜å‚¨ä»»æ„æŒ‡å®šçš„ç±»å‹ã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­, æˆ‘ä»¬å°†hittableç±»å‹çš„æ™ºèƒ½æŒ‡é’ˆå­˜å…¥`vector`ä¸­, éšç€`objects.push_back(object)`çš„è°ƒç”¨, `object`è¢«å­˜å…¥`vector`çš„æœ«å°¾, åŒæ—¶`vector`çš„å‚¨å­˜ç©ºé—´ä¼šè‡ªåŠ¨å¢åŠ ã€‚



> `std::vector` is included with the `<vector>` header.

`std::vector`åœ¨å¤´æ–‡ä»¶`<vector>`ä¸­

```c++
#include <vector>
```



> Finally, the `using` statements in [listing 20](https://raytracing.github.io/books/RayTracingInOneWeekend.html#listing_hittable-list-initial) tell the compiler that we'll be getting `shared_ptr` and `make_shared` from the `std` library, so we don't need to prefex these with `std::` every time we reference them.

æœ€å, ä½äº`hittable_list.h`æ–‡ä»¶å¼€å¤´éƒ¨åˆ†çš„`using`è¯­å¥å‘Šè¯‰ç¼–è¯‘å™¨, `shared_ptr`ä¸`make_shared`æ˜¯æ¥è‡ª`std`åº“çš„ã€‚è¿™æ ·æˆ‘ä»¬åœ¨ä½¿ç”¨å®ƒä»¬ä¹‹å‰å°±ä¸ç”¨æ¯æ¬¡éƒ½åŠ ä¸Šå‰ç¼€`std::`ã€‚





#### å¸¸ç”¨çš„å¸¸æ•°ä¸å·¥å…·å‡½æ•°

æˆ‘ä»¬éœ€è¦åœ¨å¤´æ–‡ä»¶ä¸­å®šä¹‰ä¸€äº›å¸¸ç”¨çš„å¸¸æ•°ã€‚ç›®å‰ä¸ºæ­¢æˆ‘ä»¬åªéœ€è¦å®šä¹‰æ— ç©·ã€‚ä½†æ˜¯æˆ‘ä»¬å…ˆæŠŠpiåœ¨è¿™é‡Œå®šä¹‰å¥½, ä¹‹åè¦ç”¨çš„ã€‚å¯¹äºpiæ¥è¯´å¹¶æ²¡æœ‰ä»€ä¹ˆè·¨å¹³å°çš„æ ‡å‡†å®šä¹‰ã€è¯‘æ³¨: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ä¹‹å‰ç‰ˆæœ¬ä¸­`M_PI`å®å®šä¹‰çš„åŸå› ã€‘, æ‰€ä»¥æˆ‘ä»¬è‡ªå·±æ¥å®šä¹‰ä¸€ä¸‹ã€‚æˆ‘ä»¬åœ¨`rtweekend.h`ä¸­ç»™å‡ºäº†ä¸€äº›æœªæ¥å¸¸ç”¨çš„å¸¸æ•°å’Œå‡½æ•°:

```c++
#ifndef RTWEEKEND_H
#define RTWEEKEND_H

#include <cmath>
#include <cstdlib>
#include <limits>
#include <memory>


// Usings

using std::shared_ptr;
using std::make_shared;

// Constants

const double infinity = std::numeric_limits<double>::infinity();
const double pi = 3.1415926535897932385;

// Utility Functions

inline double degrees_to_radians(double degrees) {
    return degrees * pi / 180;
}

inline double ffmin(double a, double b) { return a <= b ? a : b; }
inline double ffmax(double a, double b) { return a >= b ? a : b; }

// Common Headers

#include "ray.h"
#include "vec3.h"

#endif
```



ä»¥åŠè¿™æ˜¯æ›´æ–°åçš„mainå‡½æ•°:

```diff
#include "rtweekend.h"
#include "hittable_list.h"
#include "sphere.h"
#include <iostream>

vec3 ray_color(const ray& r, const hittable& world) {
+    hit_record rec;
+    if (world.hit(r, 0, infinity, rec)) {
+        return 0.5 * (rec.normal + vec3(1,1,1));
+    }    

     vec3 unit_direction = unit_vector(r.direction());
+    auto t = 0.5*(unit_direction.y() + 1.0);    
     return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}

int main() {    
    const int image_width = 200;    
    const int image_height = 100;    

    std::cout << "P3\n" << image_width << ' ' << image_height << "\n255\n";    
    vec3 lower_left_corner(-2.0, -1.0, -1.0);    
    vec3 horizontal(4.0, 0.0, 0.0);    
    vec3 vertical(0.0, 2.0, 0.0);    
    vec3 origin(0.0, 0.0, 0.0);
+    hittable_list world;
+    world.add(make_shared<sphere>(vec3(0,0,-1), 0.5));
+    world.add(make_shared<sphere>(vec3(0,-100.5,-1), 100));    
    
    for (int j = image_height-1; j >= 0; --j) {        
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;        
        for (int i = 0; i < image_width; ++i) {            
            auto u = double(i) / image_width;            
            auto v = double(j) / image_height;            
            ray r(origin, lower_left_corner + u*horizontal + v*vertical);
+           vec3 color = ray_color(r, world);            
            color.write_color(std::cout);        
        }    
    }    
    std::cerr << "\nDone.\n";
}
```



è¿™æ ·æˆ‘ä»¬å°±ä¼šå¾—åˆ°ä¸€å¼ ä½¿ç”¨æ³•å‘ä½œä¸ºçƒä½“é¢œè‰²å€¼çš„å›¾ç‰‡ã€‚å½“ä½ æƒ³æŸ¥çœ‹æ¨¡å‹çš„ç‰¹å¾ç»†èŠ‚ä¸ç‘•ç–µæ—¶, è¾“å‡ºé¢æ³•å‘ä½œä¸ºé¢œè‰²å€¼ä¸å¤±ä¸ºä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ã€‚



### åèµ°æ ·

> When a real camera takes a picture, there are usually no jaggies along edges because the edge pixels are a blend of some foreground and some background. We can get the same effect by averaging a bunch of samples inside each pixel. We will not bother with stratification. This is controversial, but is usual for my programs. For some ray tracers it is critical, but the kind of general one we are writing doesnâ€™t benefit very much from it and it makes the code uglier. We abstract the camera class a bit so we can make a cooler camera later.

çœŸå®ä¸–ç•Œä¸­çš„æ‘„åƒæœºæ‹æ‘„å‡ºæ¥çš„ç…§ç‰‡æ˜¯æ²¡æœ‰åƒç´ çŠ¶çš„é”¯é½¿çš„ã€‚å› ä¸ºè¾¹ç¼˜åƒç´ æ˜¯ç”±èƒŒæ™¯å’Œå‰æ™¯æ··åˆè€Œæˆçš„ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ç¨‹åºä¸­ç®€å•çš„å¯¹æ¯ä¸ªè¾¹ç¼˜åƒç´ å¤šæ¬¡é‡‡æ ·å–å¹³å‡è¾¾åˆ°ç±»ä¼¼çš„æ•ˆæœã€‚æˆ‘ä»¬è¿™é‡Œä¸ä¼šä½¿ç”¨åˆ†å±‚é‡‡æ ·ã€‚å°½ç®¡æˆ‘è‡ªå·±å¸¸å¸¸åœ¨æˆ‘çš„ç¨‹åºé‡Œä½¿ç”¨è¿™ç§æœ‰äº‰è®®çš„æ–¹æ³•ã€‚å¯¹æŸäº›å…‰çº¿è¿½è¸ªå™¨æ¥è¯´åˆ†å±‚é‡‡æ ·æ˜¯å¾ˆå…³é”®çš„éƒ¨åˆ†, ä½†æ˜¯å¯¹äºæˆ‘ä»¬å†™çš„è¿™ä¸ªå°å…‰çº¿è¿½è¸ªå™¨å¹¶ä¸ä¼šæœ‰ä»€ä¹ˆå¾ˆå¤§çš„æå‡, åªä¼šè®©ä»£ç æ›´åŠ ä¸‘é™‹ã€‚æˆ‘ä»¬ä¼šåœ¨è¿™é‡Œå°†æ‘„åƒæœºç±»æŠ½è±¡ä¸€ä¸‹, ä»¥ä¾¿äºåç»­èƒ½æœ‰ä¸€ä¸ªæ›´é…·çš„æ‘„åƒæœºã€‚

> When a real camera takes a picture, there are usually no jaggies along edges because the edge pixels are a blend of some foreground and some background. We can get the same effect by averaging a bunch of samples inside each pixel. We will not bother with stratification. This is controversial, but is usual for my programs. For some ray tracers it is critical, but the kind of general one we are writing doesnâ€™t benefit very much from it and it makes the code uglier. We abstract the camera class a bit so we can make a cooler camera later.

æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿè¿”å›çœŸéšæœºæ•°çš„ä¸€ä¸ªéšæœºæ•°ç”Ÿæˆå™¨ã€‚é»˜è®¤æ¥è¯´è¿™ä¸ªå‡½æ•°åº”è¯¥è¿”å›0â‰¤r<1çš„éšæœºæ•°ã€‚æ³¨æ„è¿™ä¸ªèŒƒå›´å–ä¸åˆ°1æ˜¯å¾ˆé‡è¦çš„ã€‚æœ‰æ—¶å€™æˆ‘ä»¬èƒ½ä»è¿™ä¸ªç‰¹æ€§ä¸­è·å¾—å¥½å¤„ã€‚



> A simple approach to this is to use the `rand()` function that can be found in `<cstdlib>`. This function returns a random integer in the range 0 and `RAND_MAX`. Hence we can get a real random number as desired with the following code snippet, added to `rtweekend.h`:

ä¸€ä¸ªç®€å•çš„å®ç°æ–¹æ³•æ˜¯, ä½¿ç”¨`<cstdlib>`ä¸­çš„`rand()`å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°ä¼šè¿”å›0åˆ°RAND_MAXä¸­çš„ä¸€ä¸ªä»»æ„æ•´æ•°ã€‚æˆ‘ä»¬å°†ä¸‹é¢çš„ä¸€å°æ®µä»£ç åŠ åˆ°`rtweekend.h`ä¸­, å°±èƒ½å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„éšæœºå‡½æ•°äº†:

```c++
#include <cstdlib>
...

inline double random_double() {
    // Returns a random real in [0,1).
    return rand() / (RAND_MAX + 1.0);
}

inline double random_double(double min, double max) {
    // Returns a random real in [min,max).
    return min + (max-min)*random_double();
}
```



> C++ did not traditionally have a standard random number generator, but newer versions of C++ have addressed this issue with the `<random>` header (if imperfectly according to some experts). If you want to use this, you can obtain a random number with the conditions we need as follows:

ä¼ ç»ŸC++å¹¶æ²¡æœ‰éšæœºæ•°ç”Ÿæˆå™¨, ä½†æ˜¯æ–°ç‰ˆC++ä¸­çš„å¤´å®ç°äº†è¿™ä¸ªåŠŸèƒ½(æŸäº›ä¸“å®¶è§‰å¾—è¿™ç§æ–¹æ³•ä¸å¤ªå®Œç¾)ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨è¿™ç§æ–¹æ³•, ä½ å¯ä»¥å‚ç…§ä¸‹é¢çš„ä»£ç :

```c++
#include <functional>
#include <random>

inline double random_double() {
    static std::uniform_real_distribution<double> distribution(0.0, 1.0);
    static std::mt19937 generator;
    static std::function<double()> rand_generator =
        std::bind(distribution, generator);
    return rand_generator();
}
```



> For a given pixel we have several samples within that pixel and send rays through each of the samples. The colors of these rays are then averaged:

å¯¹äºç»™å®šçš„åƒç´ , æˆ‘ä»¬å‘å°„å¤šæ¡å°„çº¿è¿›è¡Œå¤šæ¬¡é‡‡æ ·ã€‚ç„¶åæˆ‘ä»¬å¯¹é¢œè‰²ç»“æœæ±‚ä¸€ä¸ªå¹³å‡å€¼:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.07-pixel-samples.jpg)



> Now's a good time to create a `camera` class to manage our virtual camera and the related tasks of scene scampling. The following class implements a simple camera using the axis-aligned camera from before:

ç»¼ä¸Š, æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„ç®€å•çš„è½´å¯¹é½æ‘„åƒæœºç±»è¿›è¡Œäº†ä¸€æ¬¡å°è£…:

```c++
#ifndef CAMERA_H
#define CAMERA_H

#include "rtweekend.h"

class camera {
    public:
        camera() {
            lower_left_corner = vec3(-2.0, -1.0, -1.0);
            horizontal = vec3(4.0, 0.0, 0.0);
            vertical = vec3(0.0, 2.0, 0.0);
            origin = vec3(0.0, 0.0, 0.0);
        }

        ray get_ray(double u, double v) {
            return ray(origin, lower_left_corner + u*horizontal + v*vertical - origin);
        }

    public:
        vec3 origin;
        vec3 lower_left_corner;
        vec3 horizontal;
        vec3 vertical;
};
#endif
```



> To handle the multi-sampled color computation, we'll update the `write_color()` function. Rather than adding in a fractional contribution each time we accumulate more light to the color, just add the full color each iteration, and then perform a single divide at the end (by the number of samples) when writing out the color. In addition, we'll add a handy utility function to the `rtweekend.h` utility header: `clamp(x,min,max)`, which clamps the value `x` to the range [min,max]:

ä¸ºäº†å¯¹å¤šé‡é‡‡æ ·çš„é¢œè‰²å€¼è¿›è¡Œè®¡ç®—, æˆ‘ä»¬å‡çº§äº†`vec3::write_color()`å‡½æ•°ã€‚æˆ‘ä»¬ä¸ä¼šåœ¨æ¯æ¬¡å‘å‡ºå°„çº¿é‡‡æ ·æ—¶éƒ½è®¡ç®—ä¸€ä¸ª0-1ä¹‹é—´çš„é¢œè‰²å€¼, è€Œæ˜¯ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰çš„é¢œè‰²éƒ½åŠ åœ¨ä¸€èµ·, ç„¶åæœ€ååªéœ€è¦ç®€å•çš„ä¸€é™¤(é™¤ä»¥é‡‡æ ·ç‚¹ä¸ªæ•°)ã€‚å¦å¤–, æˆ‘ä»¬ç»™å¤´æ–‡ä»¶`rtweekend.h`åŠ å…¥äº†ä¸€ä¸ªæ–°å‡½æ•°`clamp(x,min,max)`, ç”¨æ¥å°†`x`é™åˆ¶åœ¨[min,max]åŒºé—´ä¹‹ä¸­:

```c++
inline double clamp(double x, double min, double max) {
    if (x < min) return min;
    if (x > max) return max;
    return x;
}
```



```c++
void write_color(std::ostream &out, int samples_per_pixel) {
    // Divide the color total by the number of samples.
    auto scale = 1.0 / samples_per_pixel;
    auto r = scale * e[0];
    auto g = scale * e[1];
    auto b = scale * e[2];

    // Write the translated [0,255] value of each color component.
    out << static_cast<int>(256 * clamp(r, 0.0, 0.999)) << ' '
        << static_cast<int>(256 * clamp(g, 0.0, 0.999)) << ' '
        << static_cast<int>(256 * clamp(b, 0.0, 0.999)) << '\n';
}
```



mainå‡½æ•°ä¹Ÿå‘ç”Ÿäº†å˜åŒ–:

```c++
#include "camera.h"

......

int main() {
    const int image_width = 200;
    const int image_height = 100;
    const int samples_per_pixel = 100;

    std::cout << "P3\n" << image_width << " " << image_height << "\n255\n";

    hittable_list world;
    world.add(make_shared<sphere>(vec3(0,0,-1), 0.5));
    world.add(make_shared<sphere>(vec3(0,-100.5,-1), 100));
    camera cam;
    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            vec3 color(0, 0, 0);
            for (int s = 0; s < samples_per_pixel; ++s) {
                auto u = (i + random_double()) / image_width;
                auto v = (j + random_double()) / image_height;
                ray r = cam.get_ray(u, v);
                color += ray_color(r, world);
            }
            color.write_color(std::cout, samples_per_pixel);
        }
    }

    std::cerr << "\nDone.\n";
}
```



è¿™é‡Œçš„åèµ°æ ·ç®—æ³•ä¸»è¦æ˜¯**SSAAï¼ˆSupersampling Anti-Aliasingï¼‰**ã€‚SSAAå¯ä»¥è¯´æ˜¯å›¾å½¢å­¦ä¸­æœ€ç®€å•ç²—æš´çš„åèµ°æ ·æ–¹æ³•ï¼Œä½†åŒæ—¶ä¹Ÿæœ€æœ‰æ•ˆï¼Œå®ƒå”¯ä¸€ä¹Ÿæ˜¯è‡´å‘½çš„ç¼ºç‚¹æ˜¯æ€§èƒ½å¤ªå·®ã€‚ä»»ä½•ç±»å‹çš„èµ°æ ·å½’æ ¹ç»“åº•éƒ½æ˜¯å› ä¸ºæ¬ é‡‡æ ·ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦å¢åŠ é‡‡æ ·æ•°ï¼Œå°±å¯ä»¥å‡è½»èµ°æ ·ç°è±¡ã€‚è¿™å°±æ˜¯SSAAï¼Œæ‰€ä»¥SSAAç®€å•çš„æ¥è¯´å¯ä»¥åˆ†ä¸‰æ­¥ï¼š

1. åœ¨ä¸€ä¸ªåƒç´ å†…å–è‹¥å¹²ä¸ªå­é‡‡æ ·ç‚¹
2. å¯¹å­åƒç´ ç‚¹è¿›è¡Œé¢œè‰²è®¡ç®—ï¼ˆé‡‡æ ·ï¼‰
3. æ ¹æ®å­åƒç´ çš„é¢œè‰²å’Œä½ç½®ï¼Œåˆ©ç”¨ä¸€ä¸ªç§°ä¹‹ä¸ºresolveçš„åˆæˆé˜¶æ®µï¼Œè®¡ç®—å½“å‰åƒç´ çš„æœ€ç»ˆé¢œè‰²è¾“å‡º

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/v2-940e167efe0b74854cf56082d084fbeb_720w.jpg)



> Zooming into the image that is produced, we can see the difference in edge pixels.

åœ, æ”¾å¤§æ”¾å¤§å†æ”¾å¤§, çœ‹å•Š, æ¯ä¸€ä¸ªåƒç´ éƒ½æ˜¯èƒŒæ™¯å’Œå‰æ™¯çš„æ··åˆ:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.06-antialias-before-after.png)





### æ¼«åå°„æè´¨

> Now that we have objects and multiple rays per pixel, we can make some realistic looking materials. Weâ€™ll start with diffuse (matte) materials. One question is whether we mix and match geometry and materials (so we can assign a material to multiple spheres, or vice versa) or if geometry and material are tightly bound (that could be useful for procedural objects where the geometry and material are linked). Weâ€™ll go with separate â€” which is usual in most renderers â€” but do be aware of the limitation.

æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†ç‰©ä½“çš„ç±»å’Œå¤šé‡é‡‡æ ·, æˆ‘ä»¬ä¸å¦¨å†åŠ å…¥ä¸€äº›é€¼çœŸçš„æè´¨å§ã€‚æˆ‘ä»¬å…ˆä»æ¼«åå°„æè´¨å¼€å§‹ã€‚è®¾è®¡ä¸Šçš„é—®é¢˜åˆæ¥äº†:æˆ‘ä»¬æ˜¯æŠŠæè´¨å’Œç‰©ä½“è®¾è®¡æˆä¸¤ä¸ªç±», è¿™æ ·å°±å¯ä»¥å°†æè´¨èµ‹å€¼ç»™ç‰©ä½“ç±»çš„æˆå‘˜å˜é‡, è¿˜æ˜¯è¯´è®©å®ƒä»¬ç´§å¯†ç»“åˆ,è¿™å¯¹äºä½¿ç”¨å‡ ä½•ä¿¡æ¯æ¥ç”Ÿæˆçº¹ç†çš„ç¨‹åºæ¥è¯´æ˜¯å¾ˆä¾¿åˆ©çš„ ã€‚æˆ‘ä»¬ä¼šé‡‡å–å°†å…¶åˆ†å¼€çš„åšæ³•â€”â€”â€”â€”å®é™…ä¸Šå¤§å¤šæ•°çš„æ¸²æŸ“å™¨éƒ½æ˜¯è¿™æ ·åšçš„â€”â€”â€”â€”ä½†æ˜¯è®°å¾—æ³¨æ„çš„ç¡®æ˜¯æœ‰ä¸¤ç§è®¾è®¡æ–¹æ³•çš„ã€‚



> Diffuse objects that donâ€™t emit light merely take on the color of their surroundings, but they modulate that with their own intrinsic color. Light that reflects off a diffuse surface has its direction randomized. So, if we send three rays into a crack between two diffuse surfaces they will each have different random behavior:

æ¼«åå°„æè´¨ä¸ä»…ä»…æ¥å—å…¶å‘¨å›´ç¯å¢ƒçš„å…‰çº¿, è¿˜ä¼šåœ¨æ•£å°„æ—¶ä½¿å…‰çº¿å˜æˆè‡ªå·±æœ¬èº«çš„é¢œè‰²ã€‚å…‰çº¿å°„å…¥æ¼«åå°„æè´¨å, å…¶åå°„æ–¹å‘æ˜¯éšæœºçš„ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬ä¸ºä¸‹é¢è¿™ä¸¤ä¸ªæ¼«å‘å°„çš„çƒå°„å…¥ä¸‰æ¡å…‰çº¿, å…‰çº¿éƒ½ä¼šæœ‰ä¸åŒçš„åå°„è§’åº¦:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.08-light-bounce.jpg)





> They also might be absorbed rather than reflected. The darker the surface, the more likely absorption is. (Thatâ€™s why it is dark!) Really any algorithm that randomizes direction will produce surfaces that look matte. One of the simplest ways to do this turns out to be exactly correct for ideal diffuse surfaces. (I used to do it as a lazy hack that approximates mathematically ideal Lambertian.)

å¹¶ä¸”å¤§éƒ¨åˆ†çš„å…‰çº¿éƒ½ä¼šè¢«å¸æ”¶, è€Œä¸æ˜¯è¢«åå°„ã€‚è¡¨é¢è¶Šæš—, å¸æ”¶å°±è¶Šæœ‰å¯èƒ½å‘ç”Ÿã€‚æˆ‘ä»¬ä½¿ç”¨ä»»æ„çš„ç®—æ³•ç”Ÿæˆéšæœºçš„åå°„æ–¹å‘, å°±èƒ½è®©å…¶çœ‹ä¸Šå»åƒä¸€ä¸ªç²—ç³™ä¸å¹³çš„æ¼«åå°„æè´¨ã€‚è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨æœ€ç®€å•çš„ç®—æ³•å°±èƒ½å¾—åˆ°ä¸€ä¸ªç†æƒ³çš„æ¼«åå°„è¡¨é¢(å…¶å®æ˜¯æ‡’å¾—å†™lambertianæ‰€ä»¥ç”¨äº†ä¸€ä¸ªæ•°å­¦ä¸Šè¿‘ä¼¼çš„æ–¹æ³•)ã€‚



> (Reader Vassillen Chizhov proved that the lazy hack is indeed just a lazy hack and is inaccurate. The correct representation of ideal Lambertian isn't much more work, and is presented at the end of the chapter.)

(è¯»è€…Vassillen Chizhov æä¾›äº†è¿™ä¸ªæ–¹æ³•, è™½ç„¶å¹¶ä¸æ˜¯å¾ˆç²¾ç¡®ã€‚æˆ‘ä»¬ä¼šåœ¨ç« èŠ‚æœ€åæå‡†ç¡®çš„lambertianè¡¨è¾¾å¼, è€Œä¸”å…¶å¹¶ä¸ä¼šå¾ˆå¤æ‚)



> There are two unit radius spheres tangent to the hit point ğ‘p of a surface. These two spheres have a center of (**ğ**+**ğ§**) and (**ğ**âˆ’**ğ§**), where **ğ§**n is the normal of the surface. The sphere with a center at (**ğ**âˆ’**ğ§**) is considered *inside* the surface, whereas the sphere with center (**ğ**+**ğ§**) is considered *outside* the surface. Select the tangent unit radius sphere that is on the same side of the surface as the ray origin. Pick a random point **ğ’** inside this unit radius sphere and send a ray from the hit point **ğ** to the random point **ğ’** (this is the vector (**ğ’**âˆ’**ğ**)):

å¥½, ç°åœ¨æœ‰ä¸¤ä¸ªå•ä½çƒç›¸åˆ‡äºç‚¹p, è¿™ä¸¤ä¸ªçƒä½“çš„çƒå¿ƒä¸º$(p+\vec N)$å’Œ$(p-\vec N)$, $\vec N$æ˜¯çƒä½“è¡¨é¢çš„æ³•å‘é‡ã€‚çƒå¿ƒä¸º$(p-\vec N)$çš„é‚£ä¸ªçƒåœ¨è¡¨é¢çš„å†…éƒ¨, çƒå¿ƒä¸º$(p+ \vec N)$çš„çƒåœ¨è¡¨é¢çš„å¤–éƒ¨ã€‚é€‰æ‹©å’Œå…‰çº¿åŸç‚¹ä½äºè¡¨é¢åŒä¸€ä¾§çš„é‚£ä¸ªå•ä½çƒ, å¹¶ä»çƒä¸­éšæœºé€‰å–ä¸€ç‚¹s, å‘é‡$(sâˆ’p)$å°±æ˜¯æˆ‘ä»¬è¦æ±‚çš„åå°„å…‰çº¿çš„æ–¹å‘:



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.09-rand-vec.jpg)



> We need a way to pick a random point in a unit radius sphere. Weâ€™ll use what is usually the easiest algorithm: a rejection method. First, pick a random point in the unit cube where x, y, and z all range from âˆ’1 to +1. Reject this point and try again if the point is outside the sphere.

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç®—æ³•æ¥ç”Ÿæˆçƒä½“å†…çš„éšæœºç‚¹ã€‚æˆ‘ä»¬ä¼šé‡‡ç”¨æœ€ç®€å•çš„åšæ³•:å¦å®šæ³•(rejection method)ã€‚é¦–å…ˆ, åœ¨ä¸€ä¸ªxyzå–å€¼èŒƒå›´ä¸º-1åˆ°+1çš„å•ä½ç«‹æ–¹ä½“ä¸­é€‰å–ä¸€ä¸ªéšæœºç‚¹, å¦‚æœè¿™ä¸ªç‚¹åœ¨çƒå¤–å°±é‡æ–°ç”Ÿæˆç›´åˆ°è¯¥ç‚¹åœ¨çƒå†…ã€‚

åœ¨`vec3.h`ä¸­æ·»åŠ ï¼š

```c++
class vec3 {
  public:
    ...
    inline static vec3 random() {
        return vec3(random_double(), random_double(), random_double());
    }

    inline static vec3 random(double min, double max) {
        return vec3(random_double(min,max), random_double(min,max), random_double(min,max));
    }
}  
 
......
  
inline vec3 random_in_unit_sphere() {
    while (true) {
        auto p = vec3::random(-1,1);
        if (p.length_squared() >= 1) continue;
        return p;
    }
}  
```



> Then update the `ray_color()` function to use the new random direction generator:

ç„¶åä½¿ç”¨æˆ‘ä»¬æ–°çš„ç”Ÿæˆéšæœºéšæœºåå°„æ–¹å‘çš„å‡½æ•°æ¥æ›´æ–°ä¸€ä¸‹æˆ‘ä»¬çš„`ray_color()`å‡½æ•°:

```c++
vec3 ray_color(const ray& r, const hittable& world) {
    hit_record rec;

    if (world.hit(r, 0, infinity, rec)) {
        vec3 target = rec.p + rec.normal + random_in_unit_sphere();
        return 0.5 * ray_color(ray(rec.p, target - rec.p), world);
    }

    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```



> There's one potential problem lurking here. Notice that the `ray_color` function is recursive. When will it stop recursing? When it fails to hit anything. In some cases, however, that may be a long time â€” long enough to blow the stack. To guard against that, let's limit the maximum recursion depth, returning no light contribution at the maximum depth:



è¿™é‡Œè¿˜æœ‰ä¸ªæ½œåœ¨çš„é—®é¢˜: æ³¨æ„`ray_color`å‡½æ•°æ˜¯ä¸€ä¸ªé€’å½’å‡½æ•°ã€‚é‚£ä¹ˆé€’å½’ç»ˆæ­¢çš„æ¡ä»¶æ˜¯ä»€ä¹ˆå‘¢?å½“å®ƒæ²¡æœ‰å‡»ä¸­ä»»ä½•ä¸œè¥¿ã€‚ä½†æ˜¯, åœ¨æŸäº›æ¡ä»¶ä¸‹, è¾¾åˆ°è¿™ä¸ªç»ˆæ­¢æ¡ä»¶çš„æ—¶é—´ä¼šéå¸¸é•¿, é•¿åˆ°è¶³å¤Ÿçˆ†äº†å‡½æ•°æ ˆã€è¯‘æ³¨:æƒ³è±¡ä¸€ä¸‹ä¸€æ¡å…‰çº¿åœ¨ä¸€ä¸ªé•œå­æè´¨çš„å¯†å°çš„ç›’å­(å¹¶ä¸å¸æ”¶å…‰çº¿)ä¸­åå¤æŠ˜å°„, æ°¸æ— å°½å¤´ã€‘ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µçš„å‘ç”Ÿ, æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå˜é‡`depth`é™åˆ¶é€’å½’å±‚æ•°ã€‚å½“é€’å½’å±‚æ•°è¾¾åˆ°é™åˆ¶å€¼æ—¶æˆ‘ä»¬ç»ˆæ­¢é€’å½’, è¿”å›é»‘è‰²:ã€è¯‘æ³¨: å¯ä»¥è¯•è¯•è¿”å›çº¯çº¢(1,0,0), ç„¶åæ¸²æŸ“ä¸€ä¸‹, å¤§è‡´çœ‹ä¸€ä¸‹æ˜¯å“ªé‡Œåœ¨ä¸åœçš„å‘ç”Ÿæ•£å°„ã€‘

```diff
+vec3 ray_color(const ray& r, const hittable& world, int depth) {
+    hit_record rec;

+    // If we've exceeded the ray bounce limit, no more light is gathered.
+    if (depth <= 0)
+        return vec3(0,0,0);

+    if (world.hit(r, 0, infinity, rec)) {
+        vec3 target = rec.p + rec.normal + random_in_unit_sphere();
+        return 0.5 * ray_color(ray(rec.p, target - rec.p), world, depth-1);
+   }

    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
...
int main() {
    const int image_width = 200;
    const int image_height = 100;
    const int samples_per_pixel = 100;
+   const int max_depth = 50;

    ...
    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            vec3 color(0, 0, 0);
            for (int s = 0; s < samples_per_pixel; ++s) {
                auto u = (i + random_double()) / image_width;
                auto v = (j + random_double()) / image_height;
                ray r = cam.get_ray(u, v);
+               color += ray_color(r, world, max_depth);
            }
            color.write_color(std::cout, samples_per_pixel);
        }
    }

    std::cerr << "\nDone.\n";
}
```



> This gives us:

æˆ‘ä»¬ä¼šå¾—åˆ°:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.07-first-diffuse.png)





> Note the shadowing under the sphere. This picture is very dark, but our spheres only absorb half the energy on each bounce, so they are 50% reflectors. If you canâ€™t see the shadow, donâ€™t worry, we will fix that now. These spheres should look pretty light (in real life, a light grey). The reason for this is that almost all image viewers assume that the image is â€œgamma correctedâ€, meaning the 0 to 1 values have some transform before being stored as a byte. There are many good reasons for that, but for our purposes we just need to be aware of it. To a first approximation, we can use â€œgamma 2â€ which means raising the color to the power 1/ğ‘”ğ‘ğ‘šğ‘šğ‘, or in our simple case Â½, which is just square-root:

æ³¨æ„çƒä¸‹é¢æ˜¯æœ‰å½±å­çš„ã€‚è¿™ä¸ªå›¾ç‰‡éå¸¸çš„æš—, ä½†æ˜¯æˆ‘ä»¬çš„çƒåœ¨æ•£å°„çš„æ—¶å€™åªå¸æ”¶äº†ä¸€åŠçš„èƒ½é‡ã€‚å¦‚æœä½ çœ‹ä¸è§è¿™ä¸ªé˜´å½±, åˆ«æ‹…å¿ƒ, æˆ‘ä»¬ç°åœ¨æ¥ä¿®å¤ä¸€ä¸‹ã€‚ç°å®ä¸–ç•Œä¸­çš„è¿™ä¸ªçƒæ˜æ˜¾æ˜¯åº”è¯¥æ›´åŠ äº®ä¸€äº›çš„ã€‚è¿™æ˜¯å› ä¸ºæ‰€æœ‰çš„çœ‹å›¾è½¯ä»¶éƒ½é»˜è®¤å›¾åƒå·²ç»ç»è¿‡äº†ä¼½é©¬æ ¡æ­£(gamma corrected)ã€‚å³åœ¨å›¾ç‰‡å­˜å…¥å­—èŠ‚ä¹‹å‰, é¢œè‰²å€¼å‘ç”Ÿäº†ä¸€æ¬¡è½¬åŒ–ã€‚è¿™ä¹ˆåšæœ‰è®¸å¤šå¥½å¤„, ä½†è¿™å¹¶ä¸æ˜¯æˆ‘ä»¬è¿™é‡Œæ‰€è®¨è®ºçš„é‡ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨â€gamma 2â€ç©ºé—´, å°±æ„å‘³ç€æœ€ç»ˆçš„é¢œè‰²å€¼è¦åŠ ä¸ŠæŒ‡æ•°1/gamma, åœ¨æˆ‘ä»¬çš„ä¾‹å­é‡Œå°±æ˜¯ Â½, å³å¼€å¹³æ–¹æ ¹:

```c++
void write_color(std::ostream &out, int samples_per_pixel) {
    // Divide the color total by the number of samples and gamma-correct
    // for a gamma value of 2.0.
    auto scale = 1.0 / samples_per_pixel;
    auto r = sqrt(scale * e[0]);
    auto g = sqrt(scale * e[1]);
    auto b = sqrt(scale * e[2]);

    // Write the translated [0,255] value of each color component.
    out << static_cast<int>(256 * clamp(r, 0.0, 0.999)) << ' '
        << static_cast<int>(256 * clamp(g, 0.0, 0.999)) << ' '
        << static_cast<int>(256 * clamp(b, 0.0, 0.999)) << '\n';
}
```



> That yields light grey, as we desire:

å¥½äº†, ç°åœ¨çœ‹ä¸Šå»æ›´ç°äº†, å¦‚æˆ‘ä»¬æ‰€æ„¿:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.08-gamma-correct.png)



> Thereâ€™s also a subtle bug in there. Some of the reflected rays hit the object they are reflecting off of not at exactly ğ‘¡=0, but instead at ğ‘¡=âˆ’0.0000001 or ğ‘¡=0.00000001 or whatever floating point approximation the sphere intersector gives us. So we need to ignore hits very near zero:

è¿™é‡Œè¿˜æœ‰ä¸ªä¸å¤ªé‡è¦çš„æ½œåœ¨bugã€‚æœ‰äº›ç‰©ä½“åå°„çš„å…‰çº¿ä¼šåœ¨t=0æ—¶å†æ¬¡å‡»ä¸­è‡ªå·±ã€‚ç„¶è€Œç”±äºç²¾åº¦é—®é¢˜, è¿™ä¸ªå€¼å¯èƒ½æ˜¯t=âˆ’0.000001æˆ–è€…æ˜¯t=0.0000000001æˆ–è€…ä»»æ„æ¥è¿‘0çš„æµ®ç‚¹æ•°ã€‚æ‰€ä»¥æˆ‘ä»¬è¦å¿½ç•¥æ‰0é™„è¿‘çš„ä¸€éƒ¨åˆ†èŒƒå›´, é˜²æ­¢ç‰©ä½“å‘å‡ºçš„å…‰çº¿å†æ¬¡ä¸è‡ªå·±ç›¸äº¤ã€‚ã€è¯‘æ³¨: å°å¿ƒè‡ªç›¸äº¤é—®é¢˜ã€‘

```c++
if (world.hit(r, 0.001, infinity, rec)) {
```



> This gets rid of the shadow acne problem. Yes it is really called that.

è¿™æ ·æˆ‘ä»¬å°±èƒ½é¿å…é˜´å½±ç—¤ç–®(shadow ance)çš„äº§ç”Ÿã€‚æ˜¯æ»´, è¿™ç§ç°è±¡çš„ç¡®æ˜¯å«è¿™ä¸ªåå­—ã€‚



> The rejection method presented here produces random points in the unit ball offset along the surface normal. This corresponds to picking directions on the hemisphere with high probability close to the normal, and a lower probability of scattering rays at grazing angles. This distribution scales by the cos3(ğœ™) where ğœ™ is the angle from the normal. This is useful since light arriving at shallow angles spreads over a larger area, and thus has a lower contribution to the final color.

æ‹’ç»æ³•ç”Ÿæˆçš„ç‚¹æ˜¯å•ä½çƒä½“ç§¯å†…çš„çš„éšæœºç‚¹, è¿™æ ·ç”Ÿæˆçš„å‘é‡å¤§æ¦‚ç‡ä¸Šä¼šå’Œæ³•çº¿æ–¹å‘ç›¸è¿‘, å¹¶ä¸”æå°æ¦‚ç‡ä¼šæ²¿ç€å…¥å°„æ–¹å‘åå°„å›å»ã€‚è¿™ä¸ªåˆ†å¸ƒå¾‹çš„è¡¨è¾¾å¼æœ‰ä¸€ä¸ªcos3(Ï•)cos3â¡(Ï•)çš„ç³»æ•°, å…¶ä¸­ Ï•Ï• æ˜¯åå°„å…‰çº¿è·ç¦»æ³•å‘é‡çš„å¤¹è§’ã€‚è¿™æ ·å½“å…‰çº¿ä»ä¸€ä¸ªç¦»è¡¨é¢å¾ˆå°çš„è§’åº¦å°„å…¥æ—¶, ä¹Ÿä¼šæ•£å°„åˆ°ä¸€ç‰‡å¾ˆå¤§çš„åŒºåŸŸ, å¯¹æœ€ç»ˆé¢œè‰²å€¼çš„å½±å“ä¹Ÿä¼šæ›´ä½ã€‚



> However, we are interested in a Lambertian distribution, which has a distribution of cos(ğœ™). True Lambertian has the probability higher for ray scattering close to the normal, but the distribution is more uniform. This is achieved by picking random points on the surface of the unit sphere, offset along the surface normal. Picking random points on the unit sphere can be achieved by picking random points *in* the unit sphere, and then normalizing those.

ç„¶è€Œ, äº‹å®ä¸Šçš„lambertiançš„åˆ†å¸ƒå¾‹å¹¶ä¸æ˜¯è¿™æ ·çš„, å®ƒçš„ç³»æ•°æ˜¯cos(Ï•)cosâ¡(Ï•)ã€‚çœŸæ­£çš„lambertianæ•£å°„åçš„å…‰çº¿è·ç¦»æ³•ç›¸æ¯”è¾ƒè¿‘çš„æ¦‚ç‡ä¼šæ›´é«˜, ä½†æ˜¯åˆ†å¸ƒå¾‹ä¼šæ›´åŠ å‡è¡¡ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬é€‰å–çš„æ˜¯å•ä½çƒé¢ä¸Šçš„ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å•ä½çƒå†…é€‰å–ä¸€ä¸ªéšæœºç‚¹, ç„¶åå°†å…¶å•ä½åŒ–æ¥è·å¾—è¯¥ç‚¹ã€‚ã€è¯‘æ³¨: ç„¶è€Œä¸‹é¢çš„ä»£ç å´ç”¨äº†æåæ ‡çš„å½¢å¼ã€‘

```c++
vec3 random_unit_vector() {
    auto a = random_double(0, 2*pi);
    auto z = random_double(-1, 1);
    auto r = sqrt(1 - z*z);
    return vec3(r*cos(a), r*sin(a), z);
}
```



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.10-rand-unitvec.png)



> This `random_unit_vector()` is a drop-in replacement for the existing `random_in_unit_sphere()` function.

æˆ‘ä»¬ä½¿ç”¨æ–°å‡½æ•°`random_unit_vector()`æ›¿æ¢ç°å­˜çš„`random_unit_sphere()`:

```c++
vec3 ray_color(const ray& r, const hittable& world, int depth) {
    hit_record rec;

    // If we've exceeded the ray bounce limit, no more light is gathered.
    if (depth <= 0)
        return vec3(0,0,0);

    if (world.hit(r, 0.001, infinity, rec)) {
        vec3 target = rec.p + rec.normal + random_unit_vector();
        return 0.5 * ray_color(ray(rec.p, target - rec.p), world, depth-1);
    }

    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```



> After rendering we get a similar image:

æˆ‘ä»¬ä¼šå¾—åˆ°è¿™æ ·çš„å›¾ç‰‡, å’Œä¹‹å‰å¾ˆç›¸åƒ:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.09-correct-lambertian.png)



> It's hard to tell the difference between these two diffuse methods, given that our scene of two spheres is so simple, but you should be able to notice two important visual differences:
>
> 1. The shadows are less pronounced after the change
> 2. Both spheres are lighter in appearance after the change
>
> Both of these changes are due to the more uniform scattering of the light rays, fewer rays are scattering toward the normal. This means that for diffuse objects, they will appear *lighter* because more light bounces toward the camera. For the shadows, less light bounces straight-up, so the parts of the larger sphere directly underneath the smaller sphere are brighter.

æˆ‘ä»¬çš„åœºæ™¯å¤ªç®€å•, åŒºåˆ†è¿™ä¸¤ç§æ–¹æ³•æ˜¯æ¯”è¾ƒéš¾çš„ã€‚ä½†ä½ åº”è¯¥èƒ½å¤Ÿæ³¨æ„åˆ°è§†è§‰ä¸Šçš„ä¸€äº›å·®å¼‚:

1.é˜´å½±éƒ¨åˆ†å°‘äº†
2.å¤§çƒå’Œå°çƒéƒ½å˜äº®äº†

è¿™äº›å˜åŒ–éƒ½æ˜¯ç”±æ•£å°„å…‰çº¿çš„å•ä½è§„æ•´åŒ–å¼•èµ·çš„, ç°åœ¨æ›´å°‘çš„å…‰çº¿ä¼šæœç€å‘ç°æ–¹å‘æ•£å°„ã€‚å¯¹äºæ¼«å‘å°„çš„ç‰©ä½“æ¥è¯´, ä»–ä»¬ä¼šå˜å¾—æ›´äº®ã€‚å› ä¸ºæ›´å¤šå…‰çº¿æœç€æ‘„åƒæœºåå°„ã€‚å¯¹äºé˜´å½±éƒ¨åˆ†æ¥è¯´, æ›´å°‘çš„å…‰çº¿æœä¸Šåå°„, æ‰€ä»¥å°çƒä¸‹æ–¹çš„å¤§çƒåŒºåŸŸä¼šå˜å¾—æ›´åŠ æ˜äº®ã€‚





> The initial hack presented in this book lasted a long time before it was proven to be an incorrect approximation of ideal Lambertian diffuse. A big reason that the error persisted for so long is that it can be difficult to:
>
> 1. Mathematically prove that the probability distribution is incorrect
> 2. Intuitively explain why a cos(ğœ™) distribution is desirable (and what it would look like)
>
> Not a lot of common, everyday objects are perfectly diffuse, so our visual intuition of how these objects behave under light can be poorly formed.

è¿™æœ¬ä¹¦å¾ˆé•¿ä¸€æ®µæ—¶é—´éƒ½é‡‡ç”¨çš„æ˜¯å…ˆå‰çš„ç‰ˆæœ¬, ç›´åˆ°åæ¥æœ‰ä¸€å¤©å¤§å®¶å‘ç°å®ƒå…¶å®åªæ˜¯ç†æƒ³lambertianæ¼«å‘å°„çš„è¿‘ä¼¼, å…¶å¹¶ä¸æ­£ç¡®ã€‚è¿™ä¸ªé”™è¯¯åœ¨æœ¬ä¹¦ä¸­ç•™å­˜äº†é‚£ä¹ˆé•¿æ—¶é—´, ä¸»è¦æ˜¯å› ä¸º:

1.æ¦‚ç‡åˆ†å¸ƒçš„æ•°å­¦è¯æ˜ç®—é”™äº†
2.è§†è§‰ä¸Šæ¥è¯´, å¹¶ä¸èƒ½ç›´æ¥çœ‹å‡ºcos(Ï•)çš„æ¦‚ç‡åˆ†é…æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„

å› ä¸ºå¤§å®¶æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç‰©ä½“éƒ½æ˜¯å‘ç”Ÿäº†å®Œç¾çš„æ¼«åå°„, æ‰€ä»¥æˆ‘ä»¬å¾ˆéš¾å…»æˆå¯¹å…‰ç…§ä¸‹ç‰©ä½“æ˜¯å¦‚ä½•è¡¨ç°çš„è§†è§‰ç›´è§‰ã€‚



> In the interest of learning, we are including an intuitive and easy to understand diffuse method. For the two methods above we had a random vector, first of random length and then of unit length, offset from the hit point by the normal. It may not be immediately obvious why the vectors should be displaced by the normal.

ä¸ºäº†ä¾¿äºå¤§å®¶ç†è§£, ç®€å•æ¥è¯´ä¸¤ç§æ–¹æ³•éƒ½é€‰å–äº†ä¸€ä¸ªéšæœºæ–¹å‘çš„å‘é‡, ä¸è¿‡ä¸€ç§æ˜¯ä»å•ä½çƒä½“å†…å–çš„, å…¶é•¿åº¦æ˜¯éšæœºçš„, å¦ä¸€ç§æ˜¯ä»å•ä½çƒé¢ä¸Šå–çš„, é•¿åº¦å›ºå®šä¸ºå•ä½å‘é‡é•¿åº¦ã€‚ä¸ºä»€ä¹ˆè¦é‡‡å–å•ä½çƒé¢å¹¶ä¸æ˜¯èƒ½å¾ˆç›´è§‚çš„ä¸€çœ¼çœ‹å‡ºã€‚



> A more intuitive approach is to have a uniform scatter direction for all angles away from the hit point, with no dependence on the angle from the normal. Many of the first raytracing papers used this diffuse method (before adopting Lambertian diffuse).

å¦ä¸€ç§å…·æœ‰å¯å‘æ€§çš„æ–¹æ³•æ˜¯, ç›´æ¥ä»å…¥å°„ç‚¹å¼€å§‹é€‰å–ä¸€ä¸ªéšæœºçš„æ–¹å‘, ç„¶åå†åˆ¤æ–­æ˜¯å¦åœ¨æ³•å‘é‡æ‰€åœ¨çš„é‚£ä¸ªåŠçƒã€‚åœ¨ä½¿ç”¨lambertianæ¼«å‘å°„æ¨¡å‹å‰, æ—©æœŸçš„å…‰çº¿è¿½è¸ªè®ºæ–‡ä¸­å¤§éƒ¨åˆ†ä½¿ç”¨çš„éƒ½æ˜¯è¿™ä¸ªæ–¹æ³•:

```c++
vec3 random_in_hemisphere(const vec3& normal) {
    vec3 in_unit_sphere = random_in_unit_sphere();
    if (dot(in_unit_sphere, normal) > 0.0) // In the same hemisphere as the normal
        return in_unit_sphere;
    else
        return -in_unit_sphere;
}
```





å°†æˆ‘ä»¬çš„æ–°å‡½æ•°å¥—å…¥`ray_color()`å‡½æ•°:

```c++
vec3 ray_color(const ray& r, const hittable& world, int depth) {
    hit_record rec;

    // If we've exceeded the ray bounce limit, no more light is gathered.
    if (depth <= 0)
        return vec3(0,0,0);

    if (world.hit(r, 0.001, infinity, rec)) {
        vec3 target = rec.p + random_in_hemisphere(rec.normal);
        return 0.5 * ray_color(ray(rec.p, target - rec.p), world, depth-1);
    }

    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```



> Gives us the following image:

æˆ‘ä»¬ä¼šå¾—åˆ°å¦‚ä¸‹çš„å›¾ç‰‡:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.10-rand-hemispherical.png)



> Scenes will become more complicated over the course of the book. You are encouraged to switch between the different diffuse renderers presented here. Most scenes of interest will contain a disproportionate amount of diffuse materials. You can gain valuable insight by understanding the effect of different diffuse methods on the lighting of the scene.

æˆ‘ä»¬çš„åœºæ™¯ä¼šéšç€æœ¬ä¹¦çš„æ·±å…¥ä¼šå˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚è¿™é‡Œé¼“åŠ±å¤§å®¶åœ¨ä¹‹åéƒ½è¯•ä¸€ä¸‹è¿™å‡ ç§ä¸åŒçš„æ¼«åå°„æ¸²æŸ“æ³•ã€‚å¤§å¤šæ•°åœºæ™¯éƒ½ä¼šæœ‰è®¸å¤šçš„æ¼«åå°„æè´¨ã€‚ä½ å¯ä»¥ä»ä¸­åŸ¹å…»å‡ºä½ å¯¹è¿™å‡ ç§æ–¹æ³•çš„æ•æ„Ÿç¨‹åº¦ã€‚





### é‡‘å±æè´¨

> If we want different objects to have different materials, we have a design decision. We could have a universal material with lots of parameters and different material types just zero out some of those parameters. This is not a bad approach. Or we could have an abstract material class that encapsulates behavior. I am a fan of the latter approach. For our program the material needs to do two things:
>
> 1. Produce a scattered ray (or say it absorbed the incident ray).
> 2. If scattered, say how much the ray should be attenuated.
>
> This suggests the abstract class:

å¦‚æœæˆ‘ä»¬æƒ³è®©ä¸åŒçš„ç‰©ä½“èƒ½æ‹¥æœ‰ä¸åŒçš„æè´¨, æˆ‘ä»¬åˆé¢ä¸´ç€ä¸€ä¸ªè®¾è®¡ä¸Šçš„æŠ‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªå®‡å®™æ— æ•Œå¤§æè´¨, è¿™ä¸ªæè´¨é‡Œé¢æœ‰æ•°ä¸èƒœæ•°çš„å‚æ•°å’Œæè´¨ç±»å‹å¯ä¾›é€‰æ‹©ã€‚è¿™æ ·å…¶å®ä¹Ÿä¸é”™, ä½†æˆ‘ä»¬è¿˜å¯ä»¥è®¾è®¡å¹¶å°è£…ä¸€ä¸ªæŠ½è±¡çš„æè´¨ç±»ã€‚æˆ‘åæ­£å–œæ¬¢åé¢ä¸€ç§, å¯¹äºæˆ‘ä»¬çš„ç¨‹åºæ¥è¯´, ä¸€ä¸ªæè´¨ç±»åº”è¯¥å°è£…ä¸¤ä¸ªåŠŸèƒ½è¿›å»:

1.ç”Ÿæˆæ•£å°„åçš„å…‰çº¿(æˆ–è€…è¯´å®ƒå¸æ”¶äº†å…¥å°„å…‰çº¿)
2.å¦‚æœå‘ç”Ÿæ•£å°„, å†³å®šå…‰çº¿ä¼šå˜æš—å¤šå°‘(attenuate)

ä¸‹é¢æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªæŠ½è±¡ç±»:

```c++
class material {
    public:
        virtual bool scatter(
            const ray& r_in, const hit_record& rec, vec3& attenuation, ray& scattered
        ) const = 0;
};
```



> The `hit_record` is to avoid a bunch of arguments so we can stuff whatever info we want in there. You can use arguments instead; itâ€™s a matter of taste. Hittables and materials need to know each other so there is some circularity of the references. In C++ you just need to alert the compiler that the pointer is to a class, which the â€œclass materialâ€ in the hittable class below does:



æˆ‘ä»¬åœ¨å‡½æ•°ä¸­ä½¿ç”¨hit_recordä½œä¸ºä¼ å…¥å‚æ•°, å°±å¯ä»¥ä¸ç”¨ä¼ å…¥ä¸€å¤§å †å˜é‡äº†ã€‚å½“ç„¶å¦‚æœä½ æƒ³ä¼ ä¸€å †å˜é‡è¿›å»çš„è¯ä¹Ÿè¡Œã€‚è¿™ä¹Ÿæ˜¯ä¸ªäººå–œå¥½ã€‚å½“ç„¶ç‰©ä½“å’Œæè´¨è¿˜è¦èƒ½å¤Ÿè”ç³»åœ¨ä¸€èµ·ã€‚åœ¨C++ä¸­ä½ åªè¦å‘Šè¯‰ç¼–è¯‘å™¨, æˆ‘ä»¬åœ¨`hit_record`é‡Œé¢å­˜äº†ä¸ªæè´¨çš„æŒ‡é’ˆã€‚

```c++
#ifndef HITTABLE_H
#define HITTABLE_H

#include "rtweekend.h"
#include "ray.h"
class material;

struct hit_record {
    vec3 p;
    vec3 normal;
    shared_ptr<material> mat_ptr;
    double t;
    bool front_face;


    inline void set_face_normal(const ray& r, const vec3& outward_normal) {
        front_face = dot(r.direction(), outward_normal) < 0;
        normal = front_face ? outward_normal :-outward_normal;
    }
};

class hittable {
    public:
        virtual bool hit(const ray& r, double t_min, double t_max, hit_record& rec) const = 0;
};

#endif
```



> What we have set up here is that material will tell us how rays interact with the surface. `hit_record` is just a way to stuff a bunch of arguments into a struct so we can send them as a group. When a ray hits a surface (a particular sphere for example), the material pointer in the `hit_record` will be set to point at the material pointer the sphere was given when it was set up in `main()` when we start. When the `ray_color()` routine gets the `hit_record` it can call member functions of the material pointer to find out what ray, if any, is scattered.
>
> To achieve this, we must have a reference to the material for our sphere class to returned within `hit_record`. See the highlighted lines below:

å…‰çº¿ä¼šå¦‚ä½•ä¸è¡¨é¢äº¤äº’æ˜¯ç”±å…·ä½“çš„æè´¨æ‰€å†³å®šçš„ã€‚`hit_record`åœ¨è®¾è®¡ä¸Šå°±æ˜¯ä¸ºäº†æŠŠä¸€å †è¦ä¼ çš„å‚æ•°ç»™æ‰“åŒ…åœ¨äº†ä¸€èµ·ã€‚å½“å…‰çº¿å°„å…¥ä¸€ä¸ªè¡¨é¢(æ¯”å¦‚ä¸€ä¸ªçƒä½“), `hit_record`ä¸­çš„æè´¨æŒ‡é’ˆä¼šè¢«çƒä½“çš„æè´¨æŒ‡é’ˆæ‰€èµ‹å€¼, è€Œçƒä½“çš„æè´¨æŒ‡é’ˆæ˜¯åœ¨`main()`å‡½æ•°ä¸­æ„é€ æ—¶ä¼ å…¥çš„ã€‚å½“`color()`å‡½æ•°è·å–åˆ°`hit_record`æ—¶, ä»–å¯ä»¥æ‰¾åˆ°è¿™ä¸ªæè´¨çš„æŒ‡é’ˆ, ç„¶åç”±æè´¨çš„å‡½æ•°æ¥å†³å®šå…‰çº¿æ˜¯å¦å‘ç”Ÿæ•£å°„, æ€ä¹ˆæ•£å°„ã€‚

æ‰€ä»¥æˆ‘ä»¬å¿…é¡»åœ¨çƒä½“çš„æ„é€ å‡½æ•°å’Œå˜é‡åŒºåŸŸä¸­åŠ å…¥æè´¨æŒ‡é’ˆ, ä»¥ä¾¿ä¹‹åä¼ ç»™`hit_record`ã€‚è§ä¸‹é¢é«˜äº®çš„ä»£ç è¡Œ:

```diff
class sphere: public hittable {
    public:
        sphere() {}
+       sphere(vec3 cen, double r, shared_ptr<material> m)
+           : center(cen), radius(r), mat_ptr(m) {};

        virtual bool hit(const ray& r, double tmin, double tmax, hit_record& rec) const;

    public:
        vec3 center;
        double radius;
+       shared_ptr<material> mat_ptr;
};

bool sphere::hit(const ray& r, double t_min, double t_max, hit_record& rec) const {
    vec3 oc = r.origin() - center;
    auto a = r.direction().length_squared();
    auto half_b = dot(oc, r.direction());
    auto c = oc.length_squared() - radius*radius;
    auto discriminant = half_b*half_b - a*c;

    if (discriminant > 0) {
        auto root = sqrt(discriminant);
        auto temp = (-half_b - root)/a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
            vec3 outward_normal = (rec.p - center) / radius;
            rec.set_face_normal(r, outward_normal);
            rec.mat_ptr = mat_ptr;
            return true;
        }
        temp = (-half_b + root) / a;
        if (temp < t_max && temp > t_min) {
            rec.t = temp;
            rec.p = r.at(rec.t);
            vec3 outward_normal = (rec.p - center) / radius;                
            rec.set_face_normal(r, outward_normal);
+           rec.mat_ptr = mat_ptr;
            return true;
        }
    }
    return false;
}
```



> For the Lambertian (diffuse) case we already have, it can either scatter always and attenuate by its reflectance ğ‘…R, or it can scatter with no attenuation but absorb the fraction 1âˆ’ğ‘…1âˆ’R of the rays, or it could be a mixture of those strategies. For Lambertian materials we get this simple class:

å¯¹äºæˆ‘ä»¬ä¹‹å‰å†™è¿‡çš„Lambertian(æ¼«åå°„)æè´¨æ¥è¯´, è¿™é‡Œæœ‰ä¸¤ç§ç†è§£æ–¹æ³•, è¦ä¹ˆæ˜¯å…‰çº¿æ°¸è¿œå‘ç”Ÿæ•£å°„, æ¯æ¬¡æ•£å°„è¡°å‡è‡³R, è¦ä¹ˆæ˜¯å…‰çº¿å¹¶ä¸è¡°å‡, è½¬è€Œç‰©ä½“å¸æ”¶(1-R)çš„å…‰çº¿ã€‚ä½ ä¹Ÿå¯ä»¥å½“æˆæ˜¯è¿™ä¸¤ç§çš„ç»“åˆã€‚äºæ˜¯æˆ‘ä»¬å¯ä»¥å†™å‡ºLambertiançš„æè´¨ç±»:

```c++
class lambertian : public material {
    public:
        lambertian(const vec3& a) : albedo(a) {}

        virtual bool scatter(
            const ray& r_in, const hit_record& rec, vec3& attenuation, ray& scattered
        ) const {
            vec3 scatter_direction = rec.normal + random_unit_vector();
            scattered = ray(rec.p, scatter_direction);
            attenuation = albedo;
            return true;
        }

    public:
        vec3 albedo;
};
```



> Note we could just as well only scatter with some probability ğ‘ and have attenuation be ğ‘ğ‘™ğ‘ğ‘’ğ‘‘ğ‘œ/ğ‘. Your choice.
>
> If you read the code above carefully, you'll notice a small chance of mischief. If the random unit vector we generate is exactly opposite the normal vector, the two will sum to zero, which will result in a zero scatter direction vector. This leads to bad scenarios later on (infinities and NaNs), so we need to intercept the condition before we pass it on.
>
> For smooth metals the ray wonâ€™t be randomly scattered. The key math is: how does a ray get reflected from a metal mirror? Vector math is our friend here:

æ³¨æ„æˆ‘ä»¬ä¹Ÿå¯ä»¥è®©å…‰çº¿æ ¹æ®ä¸€å®šçš„æ¦‚ç‡på‘ç”Ÿæ•£å°„ã€è¯‘æ³¨: è‹¥åˆ¤æ–­æ²¡æœ‰æ•£å°„, å…‰çº¿ç›´æ¥æ¶ˆå¤±ã€‘, å¹¶ä½¿å…‰çº¿çš„è¡°å‡ç‡(ä»£ç ä¸­çš„attenuation)ä¸ºalbedo/pã€‚éšä½ çš„å–œå¥½æ¥ã€‚

å¯¹äºå…‰æ»‘çš„é‡‘å±æè´¨æ¥è¯´, å…‰çº¿æ˜¯ä¸ä¼šåƒæ¼«åå°„é‚£æ ·éšæœºæ•£å°„çš„, è€Œæ˜¯äº§ç”Ÿåå°„ã€‚å…³é”®æ˜¯:å¯¹äºä¸€ä¸ªé‡‘å±çŠ¶çš„é•œå­, å…‰çº¿å…·ä½“æ˜¯æ€ä¹ˆåå°„çš„å‘¢?å‘é‡æ•°å­¦æ˜¯æˆ‘ä»¬çš„å¥½æœ‹å‹:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.11-reflection.jpg)



> The reflected ray direction in red is just $v+2b$. In our design, **ğ§** is a unit vector, but **ğ¯** may not be. The length of **ğ›** should be $vâ‹…n$. Because **ğ¯** points in, we will need a minus sign, yielding:

åå°„æ–¹å‘çš„å‘é‡å¦‚å›¾æ‰€ç¤ºä¸º$\vec V + 2\vec B$, å…¶ä¸­æˆ‘ä»¬è§„å®šå‘é‡$\vec N$æ˜¯å•ä½å‘é‡, ä½†$\vec V$ä¸ä¸€å®šæ˜¯ã€‚å‘é‡Bçš„é•¿åº¦åº”ä¸º$\vec V \cdot \vec N$, å› ä¸ºå‘é‡$V$ä¸å‘é‡$N$çš„æ–¹å‘ç›¸å, è¿™é‡Œæˆ‘ä»¬éœ€è¦å†åŠ ä¸Šä¸€ä¸ªè´Ÿå·, äºæ˜¯æœ‰:

```c++
vec3 reflect(const vec3& v, const vec3& n) {
    return v - 2*dot(v,n)*n;
}
```



> The metal material just reflects rays using that formula:

é‡‘å±æè´¨ä½¿ç”¨ä¸Šé¢çš„å…¬å¼æ¥è®¡ç®—åå°„æ–¹å‘:

```c++
class metal : public material {
    public:
        metal(const vec3& a) : albedo(a) {}

        virtual bool scatter(
            const ray& r_in, const hit_record& rec, vec3& attenuation, ray& scattered
        ) const {
            vec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);
            scattered = ray(rec.p, reflected);
            attenuation = albedo;
            return (dot(scattered.direction(), rec.normal) > 0);
        }

    public:
        vec3 albedo;
};
```



> We need to modify the `ray_color()` function to use this:

æˆ‘ä»¬è¿˜éœ€è¦ä¿®æ”¹ä¸€ä¸‹colorå‡½æ•°:

```diff
vec3 ray_color(const ray& r, const hittable& world, int depth) {
    hit_record rec;

    // If we've exceeded the ray bounce limit, no more light is gathered.
    if (depth <= 0)
        return vec3(0,0,0);

    if (world.hit(r, 0.001, infinity, rec)) {
+       ray scattered;
+       vec3 attenuation;
+       if (rec.mat_ptr->scatter(r, rec, attenuation, scattered))
+           return attenuation * ray_color(scattered, world, depth-1);
+       return vec3(0,0,0);
    }

    vec3 unit_direction = unit_vector(r.direction());
    auto t = 0.5*(unit_direction.y() + 1.0);
    return (1.0-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);
}
```





ç°åœ¨æˆ‘ä»¬ç»™åœºæ™¯åŠ å…¥ä¸€äº›é‡‘å±çƒ:

```diff
int main() {
    const int image_width = 200;
    const int image_height = 100;
    const int samples_per_pixel = 100;
    const int max_depth = 50;

    std::cout << "P3\n" << image_width << " " << image_height << "\n255\n";

+   hittable_list world;

+   world.add(make_shared<sphere>(
+       vec3(0,0,-1), 0.5, make_shared<lambertian>(vec3(0.7, 0.3, 0.3))));
+
+   world.add(make_shared<sphere>(
+       vec3(0,-100.5,-1), 100, make_shared<lambertian>(vec3(0.8, 0.8, 0.0))));

+   world.add(make_shared<sphere>(vec3(1,0,-1), 0.5, make_shared<metal>(vec3(0.8, 0.6, 0.2))));
+   world.add(make_shared<sphere>(vec3(-1,0,-1), 0.5, make_shared<metal>(vec3(0.8, 0.8, 0.8))));

    camera cam;
    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            vec3 color(0, 0, 0);
            for (int s = 0; s < samples_per_pixel; ++s) {
                auto u = (i + random_double()) / image_width;
                auto v = (j + random_double()) / image_height;
                ray r = cam.get_ray(u, v);
                color += ray_color(r, world, max_depth);
            }
            color.write_color(std::cout, samples_per_pixel);
        }
    }

    std::cerr << "\nDone.\n";
}
```



æˆ‘ä»¬å°±èƒ½å¾—åˆ°è¿™æ ·çš„å›¾ç‰‡:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.11-metal-shiny.png)

> We can also randomize the reflected direction by using a small sphere and choosing a new endpoint for the ray:

æˆ‘ä»¬è¿˜å¯ä»¥ç»™åå°„æ–¹å‘åŠ å…¥ä¸€ç‚¹ç‚¹éšæœºæ€§, åªè¦åœ¨ç®—å‡ºåå°„å‘é‡å, åœ¨å…¶ç»ˆç‚¹ä¸ºçƒå¿ƒçš„çƒå†…éšæœºé€‰å–ä¸€ä¸ªç‚¹ä½œä¸ºæœ€ç»ˆçš„ç»ˆç‚¹:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.12-reflect-fuzzy.jpg)



> The bigger the sphere, the fuzzier the reflections will be. This suggests adding a fuzziness parameter that is just the radius of the sphere (so zero is no perturbation). The catch is that for big spheres or grazing rays, we may scatter below the surface. We can just have the surface absorb those.

å½“ç„¶è¿™ä¸ªçƒè¶Šå¤§, é‡‘å±çœ‹ä¸Šå»å°±æ›´åŠ æ¨¡ç³Š(fuzzy, æˆ–è€…è¯´ç²—ç³™)ã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œå¼•å…¥ä¸€ä¸ªå˜é‡æ¥è¡¨ç¤ºæ¨¡ç³Šçš„ç¨‹åº¦(fuzziness)(æ‰€ä»¥å½“fuzz=0æ—¶ä¸ä¼šäº§ç”Ÿæ¨¡ç³Š)ã€‚å¦‚æœfuzz, ä¹Ÿå°±æ˜¯éšæœºçƒçš„åŠå¾„å¾ˆå¤§, å…‰çº¿å¯èƒ½ä¼šæ•£å°„åˆ°ç‰©ä½“å†…éƒ¨å»ã€‚è¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥è®¤ä¸ºç‰©ä½“å¸æ”¶äº†å…‰çº¿ã€‚

```diff
class metal : public material {
    public:
+       metal(const vec3& a, double f) : albedo(a), fuzz(f < 1 ? f : 1) {}

        virtual bool scatter(
            const ray& r_in, const hit_record& rec, vec3& attenuation, ray& scattered
        ) const {
            vec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);
+           scattered = ray(rec.p, reflected + fuzz*random_in_unit_sphere());
            attenuation = albedo;
            return (dot(scattered.direction(), rec.normal) > 0);//dot<0æˆ‘ä»¬è®¤ä¸ºå¸æ”¶
        }

    public:
        vec3 albedo;
+       double fuzz;
};
```





æˆ‘ä»¬å¯ä»¥å°†æ¨¡ç³Šå€¼è®¾ç½®ä¸º0.3å’Œ1.0, å›¾ç‰‡ä¼šå˜æˆè¿™æ ·:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.12-metal-fuzz.png)







### ç»ç¼˜ä½“æè´¨

> Clear materials such as water, glass, and diamonds are dielectrics. When a light ray hits them, it splits into a reflected ray and a refracted (transmitted) ray. Weâ€™ll handle that by randomly choosing between reflection or refraction, and only generating one scattered ray per interaction.

é€æ˜çš„ææ–™, ä¾‹å¦‚æ°´, ç»ç’ƒ, å’Œé’»çŸ³éƒ½æ˜¯ç»ç¼˜ä½“ã€‚å½“å…‰çº¿å‡»ä¸­è¿™ç±»ææ–™æ—¶, ä¸€æ¡å…‰çº¿ä¼šåˆ†æˆä¸¤æ¡, ä¸€æ¡å‘ç”Ÿåå°„, ä¸€æ¡å‘ç”ŸæŠ˜å°„ã€‚æˆ‘ä»¬ä¼šé‡‡å–è¿™æ ·çš„ç­–ç•¥: æ¯æ¬¡å…‰çº¿ä¸ç‰©ä½“ç›¸äº¤æ—¶, è¦ä¹ˆåå°„è¦ä¹ˆæŠ˜å°„, ä¸€æ¬¡åªå‘ç”Ÿä¸€ç§æƒ…å†µ,éšæœºé€‰å–ã€‚åæ­£æœ€åé‡‡æ ·æ¬¡æ•°å¤š, ä¼šç»™è¿™äº›ç»“æœå–ä¸ªå¹³å‡å€¼ã€‚

æŠ˜å°„éƒ¨åˆ†æ˜¯æœ€éš¾å»debugçš„éƒ¨åˆ†ã€‚æˆ‘å¸¸å¸¸ä¸€å¼€å§‹è®©æ‰€æœ‰çš„å…‰çº¿åªå‘ç”ŸæŠ˜å°„æ¥è°ƒè¯•ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­, æˆ‘åŠ å…¥äº†ä¸¤ä¸ªè¿™æ ·çš„ç»ç’ƒçƒ, å¹¶ä¸”å¾—åˆ°ä¸‹å›¾(æˆ‘è¿˜æ²¡æ•™ä½ æ€ä¹ˆå¼„å‡ºè¿™æ ·çš„ç»ç’ƒçƒ, ä½ å…ˆå¾€ä¸‹è¯», ä¸€ä¼šå„¿ä½ å°±çŸ¥é“äº†):

<img src="https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.13-glass-first.png" style="zoom: 200%;" />

> Is that right? Glass balls look odd in real life. But no, it isnâ€™t right. The world should be flipped upside down and no weird black stuff. I just printed out the ray straight through the middle of the image and it was clearly wrong. That often does the job.

è¿™å›¾çœ‹ä¸Šå»æ˜¯å¯¹çš„ä¹ˆ? ç»ç’ƒçƒåœ¨ç°å®ä¸–ç•Œä¸­çœ‹ä¸Šå»å’Œè¿™å·®ä¸å¤šã€‚ä½†æ˜¯, å…¶å®è¿™å›¾ä¸å¯¹ã€‚ç»ç’ƒçƒåº”è¯¥ä¼šç¿»è½¬ä¸Šä¸‹, ä¹Ÿä¸ä¼šæœ‰è¿™ç§å¥‡æ€ªçš„é»‘åœˆã€‚æˆ‘è¾“å‡ºäº†å›¾ç‰‡ä¸­å¿ƒçš„ä¸€æ¡å…‰çº¿æ¥debug, å‘ç°å®ƒå®Œå…¨é”™äº†, ä½ è°ƒè¯•çš„æ—¶å€™ä¹Ÿå¯ä»¥è¿™æ ·æ¥ã€‚



> The refraction is described by Snellâ€™s law:
>
> $ğœ‚â‹…sinğœƒ=ğœ‚â€²â‹…sinğœƒâ€²$
>
> Where ğœƒ and ğœƒâ€² are the angles from the normal, and ğœ‚ and ğœ‚â€² (pronounced â€œetaâ€ and â€œeta primeâ€) are the refractive indices (typically air = 1.0, glass = 1.3â€“1.7, diamond = 2.4). The geometry is:

æŠ˜å°„æ³•åˆ™æ˜¯ç”±Snellæ³•åˆ™å®šä¹‰çš„ï¼š

$ğœ‚â‹…sinğœƒ=ğœ‚^â€²â‹…sinğœƒ^â€²$â€‹

Î¸ä¸Î¸â€²æ˜¯å…¥å°„å…‰çº¿ä¸æŠ˜å°„å…‰çº¿è·ç¦»æ³•ç›¸çš„å¤¹è§’,Î·ä¸Î·â€²(è¯»ä½œetaå’Œeta prime)æ˜¯ä»‹è´¨çš„æŠ˜å°„ç‡(è§„å®šç©ºæ°”ä¸º1.0, ç»ç’ƒä¸º1.3-1.7,é’»çŸ³ä¸º2.4), å¦‚å›¾:



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.13-refraction.jpg)





> In order to determine the direction of the refracted ray, we have to solve for sinğœƒâ€²:
>
> $sinğœƒâ€²= \frac{ğœ‚}{ğœ‚â€²}â‹…sinğœƒ$

ä¸ºäº†è§£å‡ºæŠ˜å°„å…‰çº¿çš„æ–¹å‘, æˆ‘ä»¬éœ€è¦è§£å‡ºsinÎ¸:

$sinğœƒâ€²= \frac{ğœ‚}{ğœ‚â€²}â‹…sinğœƒ$



> On the refracted side of the surface there is a refracted ray **ğ‘**â€² and a normal **ğ§**â€², and there exists an angle, ğœƒâ€², between them. We can split **ğ‘**â€² into the parts of the ray that are perpendicular to **ğ§**â€² and parallel to **ğ§**â€²:
>
> $ \mathbf{Râ€™} = \mathbf{Râ€™}_{\bot} + \mathbf{Râ€™}_{\parallel}$â€‹
>
> If we solve for **ğ‘**â€²âŠ¥ and **ğ‘**â€²âˆ¥ we get:
>
> $ \mathbf{Râ€™}_{\bot} = \frac{ğœ‚}{ğœ‚â€²}(\mathbf{R} + \cos \theta)\mathbf{n}$
>
> $ \mathbf{Râ€™}_{\parallel} = -\sqrt{1 - |\mathbf{Râ€™}_{\bot}|^2} \mathbf{n} $

åœ¨æŠ˜å°„ä»‹è´¨éƒ¨åˆ†æœ‰å°„çº¿å…‰çº¿Râ€²ä¸æ³•å‘é‡Nâ€², å®ƒä»¬çš„å¤¹è§’ä¸ºÎ¸â€²ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå…‰çº¿Râ€²åˆ†è§£æˆå‚ç›´å’Œæ°´å¹³ä¸æ³•å‘é‡Nâ€²çš„ä¸¤ä¸ªå‘é‡:

$ \mathbf{Râ€™} = \mathbf{Râ€™}_{\bot} + \mathbf{Râ€™}_{\parallel}$

å¦‚æœè¦è§£å‡ºè¿™ä¸¤ä¸ªå‘é‡, æœ‰:

$ \mathbf{Râ€™}_{\bot} = \frac{ğœ‚}{ğœ‚â€²}(\mathbf{R} + \cos \theta)\mathbf{n}$

$ \mathbf{Râ€™}_{\parallel} = -\sqrt{1 - |\mathbf{Râ€™}_{\bot}|^2} \mathbf{n} $



> You can go ahead and prove this for yourself if you want, but we will treat it as fact and move on. The rest of the book will not require you to understand the proof.

ä½ å¯ä»¥è‡ªå·±æ¨å¯¼,è¯æ˜ã€‚æˆ‘ä»¬è¿™é‡Œå…ˆç›´æ¥æ‹¿æ¥å½“ç»“è®ºç”¨äº†ã€‚è¿™æœ¬ä¹¦æœ‰äº›åˆ«çš„åœ°æ–¹ä¹Ÿæ˜¯, å¹¶ä¸éœ€è¦ä½ å®Œå…¨ä¼šè¯æ˜ã€‚ã€è¯‘æ³¨: è‡ªå·±æ¨æ¨ä¹Ÿæ²¡åå¤„ã€‘



> We still need to solve for cosğœƒcosâ¡Î¸. It is well known that the dot product of two vectors can be explained in terms of the cosine of the angle between them:
>
> $aâ‹…b=|a||b|\cosÎ¸$
>
> If we restrict **ğš** and **ğ›** to be unit vectors:
>
> $aâ‹…b=\cosğœƒ$â€‹â€‹
>
> We can now rewrite **ğ‘**â€²âŠ¥ in terms of known quantities:
>
> $ğ‘^â€²_âŠ¥=\frac{ğœ‚}{ğœ‚â€²}(ğ‘+(âˆ’ğ‘â‹…ğ§)ğ§)$

ç„¶åæˆ‘ä»¬æ¥è§£cosÎ¸, ä¸‹é¢æ˜¯è‘—åçš„ç‚¹ä¹˜çš„å…¬å¼å®šä¹‰:

$aâ‹…b=|a||b|\cosÎ¸$â€‹â€‹

å¦‚æœæˆ‘ä»¬å°† $a$ ä¸ $b$ å½’ä¸€åŒ–ä¸ºå•ä½å‘é‡:

$aâ‹…b=\cosğœƒ$â€‹â€‹

äºæ˜¯æˆ‘ä»¬å¯ä»¥è¿™æ ·è¡¨è¾¾å‚ç›´çš„é‚£ä¸ªå‘é‡:

$ğ‘^â€²_âŠ¥=\frac{ğœ‚}{ğœ‚â€²}(ğ‘+(âˆ’ğ‘â‹…ğ§)ğ§)$

æ ¹æ®ä¸Šè¿°å…¬å¼, æˆ‘ä»¬å°±èƒ½å†™å‡ºè®¡ç®—æŠ˜å°„å…‰çº¿Râ€²çš„å‡½æ•°:

```c++
vec3 refract(const vec3& uv, const vec3& n, double etai_over_etat) {
    auto cos_theta = dot(-uv, n);
    vec3 r_out_parallel = etai_over_etat * (uv + cos_theta * n);
    vec3 r_out_perp = - sqrt(fabs(1.0 - r_out_parallel.length_squared())) * n;
    return r_out_parallel + r_out_perp;
}
```



å›åˆ° `main.cpp` ä¸­æ›´æ”¹ï¼š

```diff
    // World
    hittable_list world;

    world.add(make_shared<sphere>(
            vec3(0,0,-1), 0.5, make_shared<lambertian>(vec3(0.7, 0.3, 0.3))));
    world.add(make_shared<sphere>(
            vec3(0,-100.5,-1), 100, make_shared<lambertian>(vec3(0.8, 0.8, 0.0))));

+   world.add(make_shared<sphere>(
+           vec3(1,0,-1), 0.5, make_shared<dielectric>(1.5)));
    
+   world.add(make_shared<sphere>(
+           vec3(-1,0,-1), 0.5, make_shared<dielectric>(1.5)));
```





![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.14-glass-always-refract.png)





> That definitely doesn't look right. One troublesome practical issue is that when the ray is in the material with the higher refractive index, there is no real solution to Snellâ€™s law, and thus there is no refraction possible. If we refer back to Snell's law and the derivation of sinğœƒâ€²:
>
> $\sinğœƒ^â€²=\frac{ğœ‚}{ğœ‚â€²}â‹…\sinğœƒ$â€‹â€‹
>
> If the ray is inside glass and outside is air (ğœ‚=1.5 and ğœ‚â€²=1.0):
>
> $\sinğœƒ^â€²=\frac{1.5}{1.0}â‹…\sinğœƒ$â€‹
>
> The value of sinğœƒâ€²sinâ¡Î¸â€² cannot be greater than 1. So, if,
>
> $\frac{1.5}{1.0}â‹…\sinğœƒ>1.0$â€‹
>
> the equality between the two sides of the equation is broken, and a solution cannot exist. If a solution does not exist, the glass cannot refract, and therefore must reflect the ray:

ç°åœ¨çœ‹ä¸Šå»å›¾å¥½åƒä¸å¤ªå¯¹, è¿™æ˜¯å› ä¸ºå½“å…‰çº¿ä»é«˜æŠ˜å°„å¾‹ä»‹è´¨å°„å…¥ä½æŠ˜å°„ç‡ä»‹è´¨æ—¶, å¯¹äºä¸Šè¿°çš„Snellæ–¹ç¨‹å¯èƒ½æ²¡æœ‰å®è§£ã€sinÎ¸>1ã€‘ã€‚è¿™æ—¶å€™å°±ä¸ä¼šå‘ç”ŸæŠ˜å°„, æ‰€ä»¥å°±ä¼šå‡ºç°è®¸å¤šå°é»‘ç‚¹ã€‚æˆ‘ä»¬å›å¤´çœ‹ä¸€ä¸‹snellæ³•åˆ™çš„å¼å­:

$\sinğœƒ^â€²=\frac{ğœ‚}{ğœ‚â€²}â‹…\sinğœƒ$â€‹â€‹

å¦‚æœå…‰çº¿ä»ç»ç’ƒ(Î·=1.5)å°„å…¥ç©ºæ°”(Î·=1.0)

$\sinğœƒ^â€²=\frac{1.5}{1.0}â‹…\sinğœƒ$â€‹

åˆå› ä¸ºsinÎ¸â€²sinâ¡Î¸â€²æ˜¯ä¸å¯èƒ½æ¯”1å¤§çš„,æ‰€ä»¥ä¸€æ—¦è¿™ç§æƒ…å†µå‘ç”Ÿäº†:

$\frac{1.5}{1.0}â‹…\sinğœƒ>1.0$â€‹

é‚£å°±å®Œè›‹äº†, æ–¹ç¨‹æ— è§£äº†ã€‚æ‰€ä»¥æˆ‘ä»¬è®¤ä¸ºå…‰çº¿æ— æ³•å‘ç”ŸæŠ˜å°„çš„æ—¶å€™, å®ƒå‘ç”Ÿäº†åå°„:

```c++
if(etai_over_etat * sin_theta > 1.0) {
    // Must Reflect
    ...
}
else {
    // Can Refract
    ...
}
```



> Here all the light is reflected, and because in practice that is usually inside solid objects, it is called â€œtotal internal reflectionâ€. This is why sometimes the water-air boundary acts as a perfect mirror when you are submerged.
>
> We can solve for `sin_theta` using the trigonometric qualities:
>
> $\sinÎ¸=\sqrt{1âˆ’\cos{2Î¸}}$
>
> and
>
> $\cosÎ¸ = R â‹… N$



è¿™é‡Œæ‰€æœ‰çš„å…‰çº¿éƒ½ä¸å‘ç”ŸæŠ˜å°„, è½¬è€Œå‘ç”Ÿäº†åå°„ã€‚å› ä¸ºè¿™ç§æƒ…å†µå¸¸å¸¸åœ¨å®å¿ƒç‰©ä½“çš„å†…éƒ¨å‘ç”Ÿ, æ‰€ä»¥æˆ‘ä»¬ç§°è¿™ç§æƒ…å†µè¢«ç§°ä¸ºâ€å…¨å†…åå°„â€ã€‚è¿™ä¹Ÿå½“ä½ æµ¸å…¥æ°´ä¸­æ—¶, ä½ å‘ç°æ°´ä¸ç©ºæ°”çš„äº¤ç•Œå¤„çœ‹ä¸Šå»åƒä¸€é¢é•œå­çš„åŸå› ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä¸‰è§’å‡½æ•°è§£å‡º `sin_theta`ï¼š

$\sinÎ¸=\sqrt{1âˆ’\cos{2Î¸}}$â€‹

å…¶ä¸­çš„ `cos_theta` ä¸ºï¼š

$\cosÎ¸ = R â‹… N$

```c++
double cos_theta = ffmin(dot(-unit_direction, rec.normal), 1.0);
double sin_theta = sqrt(1.0 - cos_theta*cos_theta);
if(etai_over_etat * sin_theta > 1.0) {
    // Must Reflect
    ...
}
else {
    // Can Refract
    ...
}
```



> And the dielectric material that always refracts (when possible) is:

ä¸€ä¸ªåœ¨å¯ä»¥åæŠ˜çš„æƒ…å†µä¸‹æ€»æ˜¯åæŠ˜, å…¶ä½™æƒ…å†µå‘ç”Ÿåå°„çš„ç»ç¼˜ä½“æè´¨ä¸º:

```diff
class dielectric : public material {
public:
    dielectric(double ri) : ref_idx(ri) {}

    virtual bool scatter (
            const ray& r_in,const hit_record& rec,vec3& attenuation,ray& scattered
    ) const {
        double etai_over_etat;
        attenuation = vec3(1.0, 1.0, 1.0);

        if (rec.front_face) {
            etai_over_etat = 1 / ref_idx;
        } else {
            etai_over_etat = ref_idx;
        }

        vec3 unit_direction = unit_vector(r_in.direction());
+       double cos_theta = ffmin(dot(-unit_direction, rec.normal), 1.0);
+       double sin_theta = sqrt(1.0 - cos_theta * cos_theta);

+       if (etai_over_etat * sin_theta > 1.0) {
+           vec3 reflected = reflect(unit_direction, rec.normal);
+           scattered = ray(rec.p, reflected);
+           return true;
+       }

        vec3 refracted = refract(unit_direction, rec.normal, etai_over_etat);
        scattered = ray(rec.p, refracted);

        return true;
    }
public:
    double ref_idx;
};
```



> Attenuation is always 1 â€” the glass surface absorbs nothing. If we try that out with these parameters:

è¿™é‡Œçš„å…‰çº¿è¡°å‡ç‡ä¸º1â€”â€”å°±æ˜¯ä¸è¡°å‡, ç»ç’ƒè¡¨é¢ä¸å¸æ”¶å…‰çš„èƒ½é‡ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„å‚æ•°:

```c++
    // World
    hittable_list world;

    world.add(make_shared<sphere>(
            vec3(0,0,-1), 0.5, make_shared<lambertian>(vec3(0.1, 0.2, 0.5))));
    world.add(make_shared<sphere>(
            vec3(0,-100.5,-1), 100, make_shared<lambertian>(vec3(0.8, 0.8, 0.0))));

    world.add(make_shared<sphere>(
           vec3(1,0,-1), 0.5, make_shared<metal>(vec3(0.8, 0.6, 0.2),0.0)));

    world.add(make_shared<sphere>(
            vec3(-1,0,-1), 0.5, make_shared<dielectric>(1.5)));

```



> We get:

æˆ‘ä»¬ä¼šå¾—åˆ°:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.15-glass-sometimes-refract.png)



> Now real glass has reflectivity that varies with angle â€” look at a window at a steep angle and it becomes a mirror. There is a big ugly equation for that, but almost everybody uses a cheap and surprisingly accurate polynomial approximation by Christophe Schlick. This yields our full glass material:

ç°å®ä¸–ç•Œä¸­çš„ç»ç’ƒ, å‘ç”ŸæŠ˜å°„çš„æ¦‚ç‡ä¼šéšç€å…¥å°„è§’è€Œæ”¹å˜â€”â€”ä»ä¸€ä¸ªå¾ˆç‹­çª„çš„è§’åº¦å»çœ‹ç»ç’ƒçª—, å®ƒä¼šå˜æˆä¸€é¢é•œå­ã€‚è¿™ä¸ªå¼å­åˆä¸‘åˆé•¿, å¥½åœ¨æˆ‘ä»¬æœ‰ä¸ªæ•°å­¦ä¸Šè¿‘ä¼¼çš„ç­‰å¼, å®ƒæ˜¯ç”±Christophe Schlickæå‡ºçš„:

```c++
double schlick(double cosine, double ref_idx) {
    auto r0 = (1-ref_idx) / (1+ref_idx);
    r0 = r0*r0;
    return r0 + (1-r0)*pow((1 - cosine),5);
}
```



è¿™é‡Œæœ‰ä¸ªç®€å•åˆå¥½ç”¨çš„trick, å¦‚æœä½ å°†çƒçš„åŠå¾„è®¾ä¸ºè´Ÿå€¼, å½¢çŠ¶çœ‹ä¸Šå»å¹¶æ²¡ä»€ä¹ˆå˜åŒ–, ä½†æ˜¯æ³•ç›¸å…¨éƒ½ç¿»è½¬åˆ°å†…éƒ¨å»äº†ã€‚æ‰€ä»¥å°±å¯ä»¥ç”¨è¿™ä¸ªç‰¹æ€§æ¥åšå‡ºä¸€ä¸ªé€šé€çš„ç»ç’ƒçƒ:ã€æŠŠä¸€ä¸ªå°çƒå¥—åœ¨å¤§çƒé‡Œ, å…‰çº¿å‘ç”Ÿä¸¤æ¬¡æŠ˜å°„, äºæ˜¯è´Ÿè´Ÿå¾—æ­£, ä¸Šä¸‹ä¸ä¼šé¢ å€’ã€‘:

```c++
world.add(make_shared<sphere>(vec3(0,0,-1), 0.5, make_shared<lambertian>(vec3(0.1, 0.2, 0.5))));
world.add(make_shared<sphere>(
    vec3(0,-100.5,-1), 100, make_shared<lambertian>(vec3(0.8, 0.8, 0.0))));
world.add(make_shared<sphere>(vec3(1,0,-1), 0.5, make_shared<metal>(vec3(0.8, 0.6, 0.2), 0.3)));
world.add(make_shared<sphere>(vec3(-1,0,-1), 0.5, make_shared<dielectric>(1.5)));
world.add(make_shared<sphere>(vec3(-1,0,-1), -0.45, make_shared<dielectric>(1.5)));
```









### å¯è‡ªå®šä¹‰ä½ç½®çš„æ‘„åƒæœº

> Cameras, like dielectrics, are a pain to debug. So I always develop mine incrementally. First, letâ€™s allow an adjustable field of view (*fov*). This is the angle you see through the portal. Since our image is not square, the fov is different horizontally and vertically. I always use vertical fov. I also usually specify it in degrees and change to radians inside a constructor â€” a matter of personal taste.

æ‘„åƒæœºæ€»æ˜¯å’Œç»ç¼˜ä½“ä¸€æ ·éš¾ä»¥debugã€‚æ‰€ä»¥æˆ‘æ€»æ˜¯ä¸€æ­¥æ­¥æ­å»ºæˆ‘çš„æ‘„åƒæœºç±»ã€‚é¦–å…ˆ, æˆ‘ä»¬ä½¿æ‘„åƒæœºèƒ½è°ƒæ•´å…¶è§†é‡èŒƒå›´(field of view, fov)ã€‚fovæ˜¯ä½ çš„è§†è§’ã€‚å› ä¸ºæˆ‘ä»¬çš„å›¾ç‰‡ä¸æ˜¯æ–¹çš„, æ‰€ä»¥å‚ç›´å’Œæ°´å¹³çš„fovå€¼æ˜¯ä¸åŒçš„ã€‚æˆ‘æ€»æ˜¯ä½¿ç”¨å‚ç›´æ–¹å‘çš„fovã€‚å¹¶ä¸”æˆ‘æ€»æ˜¯ä½¿ç”¨è§’åº¦åˆ¶æ¥ä¼ å‚, åœ¨æ„é€ å‡½æ•°ä¸­å†å°†å…¶è½¬åŒ–ä¸ºå¼§åº¦â€”â€”è¿™ä¹Ÿæ˜¯æˆ‘çš„ä¸ªäººå–œå¥½ã€‚



> I first keep the rays coming from the origin and heading to the ğ‘§=âˆ’1 plane. We could make it the ğ‘§=âˆ’2 plane, or whatever, as long as we made â„h a ratio to that distance. Here is our setup:

é¦–å…ˆæˆ‘è®©å°„çº¿ä»åŸç‚¹å°„å‘z=âˆ’1å¹³é¢ã€‚æˆ‘ä»¬å½“ç„¶ä¹Ÿå¯ä»¥è®©å…¶å°„å‘z=âˆ’2çš„å¹³é¢,æˆ–è€…å…¶ä»–çš„ä»€ä¹ˆå€¼éƒ½è¡Œ, åæ­£hå’Œè¿™ä¸ªè·ç¦»dæ˜¯æˆæ¯”ä¾‹çš„ã€‚

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.14-cam-view-geom.jpg)



> This implies $â„=\tan(\frac{ğœƒ}{2})$â€‹. Our camera now becomes:

æ˜¾ç„¶, $h=\tan(\frac{Î¸}{2})$ã€‚æˆ‘ä»¬çš„æ‘„åƒæœºç±»ç°åœ¨å˜æˆ:

```diff
#ifndef RAYTRACING_DEV_CAMERA_H
#define RAYTRACING_DEV_CAMERA_H

#include "rtweekend.h"

class camera {
public:
    camera() {
        lower_left_corner = vec3(-2.0,-1.0,-1.0);
        horizontal = vec3(4.0,0.0,0.0);
        vertical = vec3(0.0,2.0,0.0);
        origin = vec3(0.0,0.0,0.0);
    }

+   camera(double vfov, double aspect) { // top to bottom, in degress
+       origin = vec3(0.0, 0.0, 0.0);

+       auto theta = degree_to_radians(vfov);
+       auto half_height = tan(theta / 2);
+       auto half_width = aspect * half_height;

+       lower_left_corner = vec3(-half_width, -half_height, -1.0);

+       horizontal = vec3(2 * half_width, 0.0, 0.0);
+       vertical = vec3(0.0, 2 * half_height, 0.0);
+   }

    ray get_ray(double u,double v) {
        return ray(origin,lower_left_corner + u * horizontal + v * vertical - origin);
    }

public:
    vec3 origin;
    vec3 lower_left_corner;
    vec3 horizontal;
    vec3 vertical;
};

#endif //RAYTRACING_DEV_CAMERA_H
```



> When calling it with camera `cam(90, aspect_ratio)` and these spheres:

å½“æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª`cam(90, double(image_width)/image_height)`çš„æ‘„åƒæœºå»æ‹ä¸‹é¢çš„çƒ:

```c++
    // World
    auto R = cos(pi / 4);
    hittable_list world;

    world.add(make_shared<sphere>(
            vec3(-R, 0, -1), R, make_shared<lambertian>(vec3(0, 0, 1))));
    world.add(make_shared<sphere>(
            vec3(R, 0, -1), R, make_shared<lambertian>(vec3(1, 0, 0))));

```



æˆ‘ä»¬ä¼šå¾—åˆ°:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.17-wide-view.png)





> To get an arbitrary viewpoint, letâ€™s first name the points we care about. Weâ€™ll call the position where we place the camera *lookfrom*, and the point we look at *lookat*. (Later, if you want, you could define a direction to look in instead of a point to look at.)
>
> We also need a way to specify the roll, or sideways tilt, of the camera: the rotation around the lookat-lookfrom axis. Another way to think about it is that even if you keep `lookfrom` and `lookat` constant, you can still rotate your head around your nose. What we need is a way to specify an â€œupâ€ vector for the camera. This up vector should lie in the plane orthogonal to the view direction.

ä¸ºäº†èƒ½å°†æˆ‘ä»¬çš„æ‘„åƒæœºè®¾ç½®åœ¨ä»»æ„ä½ç½®, æˆ‘ä»¬å…ˆæ¥ç»™è¿™ä¸ªä½ç½®ç‚¹èµ·ä¸ªåå­—ã€‚æˆ‘ä»¬ç®¡æ‘„åƒæœºæ‰€åœ¨çš„è¿™ä¸ªä½ç½®å«åš `lookfrom` , æˆ‘ä»¬çœ‹å‘çš„ç‚¹å«åš`lookat`(å¦‚æœä½ ä¸æƒ³ç”¨ä¸–ç•Œåæ ‡ä¸‹çš„ç‚¹, æƒ³ç”¨å‘é‡æ¥è¡¨ç¤ºè¿™ä¸ªæ–¹å‘çš„è¯ä¹Ÿå®Œå…¨ok)ã€‚

æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå˜é‡å»æè¿°æ‘„åƒæœºçš„å€¾æ–œç¨‹åº¦, æˆ–è€…è¯´æ‘„åƒæœºç»•ç€è½´`lookfrom - lookat`æ—‹è½¬çš„è§’åº¦ã€æƒ³è±¡ä¸‹å›¾ä¸­çº¢è‰²å¹³é¢ç»•è¿™ä¸ªè½´æ—‹è½¬ã€‘ã€‚å°±å¥½æ¯”ä½ ç«™ç›´äº†, ä½†æ˜¯ä½ çš„å¤´è¿˜æ˜¯å¯ä»¥å·¦å³è½¬åŠ¨ã€‚ä¸ºäº†å»æè¿°è¿™ä¸ªå€¾æ–œç¨‹åº¦, æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‘é‡æ¥æŒ‡å®šæ‘„åƒæœºåæ ‡ç³»çš„æ­£ä¸Šæ–¹æ–¹å‘(up vector)ã€‚è¿™é‡Œæ³¨æ„:è¿™ä¸ªå‘é‡å°±åœ¨è§†çº¿æ–¹å‘æ­£äº¤æŠ•å½±è¿‡æ¥çš„é‚£ä¸ªå¹³é¢ä¸Š:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.15-cam-view-dir.jpg)



> We can actually use any up vector we want, and simply project it onto this plane to get an up vector for the camera. I use the common convention of naming a â€œview upâ€ (*vup*) vector. A couple of cross products, and we now have a complete orthonormal basis (ğ‘¢,ğ‘£,ğ‘¤) to describe our cameraâ€™s orientation.

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»æ„çš„æ–¹å‘å‘é‡, å°†å…¶æŠ•å½±åˆ°ä¸Šå›¾çš„å¹³é¢ä¸­æ¥è·å¾—æ‘„åƒæœºçš„up vectorã€‚æˆ‘è¿™é‡Œç»™ä»–èµ·åå«vupå‘é‡ã€‚ç»è¿‡ä¸€ç³»åˆ—çš„ç‚¹ä¹˜æ“ä½œ, æˆ‘ä»¬ä¼šæœ‰å®Œæ•´çš„u,v,wä¸‰ä¸ªå‘é‡æ¥æè¿°æ‘„åƒæœºçš„æ—‹å‘ã€è¿™é‡Œè¦ç»“åˆç€ä»£ç çœ‹ä¸ä¸‹é¢çš„å›¾ç‰‡çœ‹ã€‘ã€‚

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.16-cam-view-up.jpg)



> Remember that `vup`, `v`, and `w` are all in the same plane. Note that, like before when our fixed camera faced -Z, our arbitrary view camera faces -w. And keep in mind that we can â€” but we donâ€™t have to â€” use world up (0,1,0) to specify vup. This is convenient and will naturally keep your camera horizontally level until you decide to experiment with crazy camera angles.

æ³¨æ„`vup`,`v`,`w`å¤„äºåŒä¸€å¹³é¢å†…ã€‚å’Œå…ˆå‰æˆ‘ä»¬çš„æ‘„åƒæœºé¢å¯¹ç€-Zæ–¹å‘ä¸€æ ·, ä¿®æ”¹åçš„ä»»æ„è§†è§’æ‘„åƒæœºé¢å¯¹ç€-wæ–¹å‘ã€‚è®°å¾—ä½¿ç”¨ä¸–ç•Œåæ ‡ç³»çš„ä¸Šæ–¹å‘å‘é‡(0,1,0)(ä¸æ˜¯ä¸€å®šè¦ç”¨è¿™ä¸ªå‘é‡)æŒ‡å®švupã€‚è¿™æ ·ä¼šæ¯”è¾ƒæ–¹ä¾¿, å¹¶ä¸”ä½ çš„æ‘„åƒæœºé•œå¤´ä¼šä¿æŒæ°´å¹³ã€‚å¦‚æœä½ æƒ³è¯•è¯•é‚£äº›å¥‡æ€ªçš„æ‘„åƒè§’åº¦, ä½ å¯ä»¥æ”¾å¿ƒå¤§èƒ†çš„ä¼ å…¥åˆ«çš„å€¼ã€‚

```diff
class camera {
public:
    camera() {
        lower_left_corner = vec3(-2.0,-1.0,-1.0);
        horizontal = vec3(4.0,0.0,0.0);
        vertical = vec3(0.0,2.0,0.0);
        origin = vec3(0.0,0.0,0.0);
    }

+   camera(vec3 lookfrom, vec3 lookat, vec3 vup,
            double vfov, double aspect) { // top to bottom, in degress
        origin = lookfrom;

        auto theta = degree_to_radians(vfov);
        auto half_height = tan(theta / 2);
        auto half_width = aspect * half_height;
+       vec3 w = unit_vector(lookfrom - lookat);
+       vec3 u = unit_vector(cross(vup, w));
+       vec3 v = cross(w, u);

+       lower_left_corner = origin - half_width * u - half_height * v - w;

+       horizontal = 2 * half_width * u;
+       vertical = 2 * half_height * v;
    }

    ray get_ray(double u,double v) {
        return ray(origin,lower_left_corner + u * horizontal + v * vertical - origin);
    }

public:
    vec3 origin;
    vec3 lower_left_corner;
    vec3 horizontal;
    vec3 vertical;
};
```



> We'll change back to the prior scene, and use the new viewpoint:

ç°åœ¨æˆ‘ä»¬å°±å¯ä»¥æ”¹å˜æˆ‘ä»¬çš„è§†è§’äº†:

```diff
+   const auto aspect_ratio = double(image_width) / image_height;

		// World
    auto R = cos(pi / 4);
    hittable_list world;

-   world.add(make_shared<sphere>(
-           vec3(-R, 0, -1), R, make_shared<lambertian>(vec3(0, 0, 1))));
-   world.add(make_shared<sphere>(
-           vec3(R, 0, -1), R, make_shared<lambertian>(vec3(1, 0, 0))));

    world.add(make_shared<sphere>(
            vec3(0,0,-1), 0.5, make_shared<lambertian>(vec3(0.1, 0.2, 0.5))));
    world.add(make_shared<sphere>(
            vec3(0,-100.5,-1), 100, make_shared<lambertian>(vec3(0.8, 0.8, 0.0))));

    world.add(make_shared<sphere>(
           vec3(1,0,-1), 0.5, make_shared<metal>(vec3(0.8, 0.6, 0.2),0.0)));

    world.add(make_shared<sphere>(
            vec3(-1,0,-1), 0.5, make_shared<dielectric>(1.5)));

    // Camera
+   camera cam = camera(vec3(-2, 2, 1), vec3(0, 0, -1), vec3(0, 1, 0), 90, aspect_ratio);

```



> to get:

æˆ‘ä»¬ä¼šå¾—åˆ°:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.18-view-distant.png)





> And we can change field of view:



ç„¶åæˆ‘ä»¬åœ¨æ”¹å˜ä¸€ä¸‹fov:ã€è¿™é‡Œç¼©å°äº†fovã€‘

```c++
    // Camera
    camera cam = camera(vec3(-2, 2, 1), vec3(0, 0, -1), vec3(0, 1, 0), 20, aspect_ratio);

```



> to get:

ä¼šå¾—åˆ°:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.19-view-zoom.png)



### æ•£ç„¦æ¨¡ç³Š

>  Now our final feature: defocus blur. Note, all photographers will call it â€œdepth of fieldâ€ so be aware of only using â€œdefocus blurâ€ among friends.

ç»ˆäºåˆ°äº†æˆ‘ä»¬æœ€åçš„ç‰¹æ€§äº†: æ•£ç„¦æ¨¡ç³Š(defocus blur)ã€‚åŸºæœ¬ä¸Šæ‰€æœ‰çš„æ‘„å½±å¸ˆéƒ½å®ƒå«æ™¯æ·±(depth of field)ã€‚æ‰€ä»¥ä½ å’Œä½ æœ‹å‹èŠå¤©çš„æ—¶å€™å¯åˆ«æä»€ä¹ˆdefocus blurå•Šã€‚



>  The reason we defocus blur in real cameras is because they need a big hole (rather than just a pinhole) to gather light. This would defocus everything, but if we stick a lens in the hole, there will be a certain distance where everything is in focus. You can think of a lens this way: all light rays coming *from* a specific point at the focus distance â€” and that hit the lens â€” will be bent back *to* a single point on the image sensor.

ç°å®ä¸–ç•Œä¸­çš„æ‘„åƒæœºäº§ç”Ÿå¯¹ç„¦æ¨¡ç³Šçš„åŸå› æ˜¯å› ä¸ºä»–ä»¬éœ€è¦ä¸€ä¸ªå¾ˆå¤§çš„å­”, è€Œä¸æ˜¯ä¸€ä¸ªé’ˆçœ¼å¤§å°çš„å°å­”æ¥èšé›†å…‰çº¿ã€‚è¿™ä¼šå¯¼è‡´æ‰€æœ‰çš„ä¸œè¥¿éƒ½è¢«æ•£ç„¦äº†ã€‚ä½†å¦‚æœæˆ‘ä»¬åœ¨å­”å†…åŠ å…¥ä¸€å—é€é•œ, åœ¨ä¸€æ®µè·ç¦»å†…çš„æ‰€æœ‰ç‰©ä½“éƒ½ä¼šè¢«å¯¹ç„¦ã€‚ä½ å¯ä»¥è¿™æ ·æ¥æƒ³è±¡é€é•œ:æ‰€æœ‰çš„å…‰çº¿ä»åŒä¸€ç‚¹åˆ†æ•£å°„å‡º, å‡»ä¸­é€é•œååˆèšç„¦åœ¨å›¾åƒä¼ æ„Ÿå™¨ä¸Šçš„ä¸€ä¸ªç‚¹ä¸Šã€‚



> In a physical camera, the focus distance is controlled by the distance between the lens and the film/sensor. That is why you see the lens move relative to the camera when you change what is in focus (that may happen in your phone camera too, but the sensor moves). The â€œapertureâ€ is a hole to control how big the lens is effectively. For a real camera, if you need more light you make the aperture bigger, and will get more defocus blur. For our virtual camera, we can have a perfect sensor and never need more light, so we only have an aperture when we want defocus blur.



åœ¨ç°å®ä¸–ç•Œçš„ç›¸æœºä¸­, ç‰©ä½“åœ¨å“ªé‡Œè¢«èšç„¦æ˜¯ç”±é€é•œè·ç¦»æˆåƒå¹³é¢ä¸èšç„¦å¹³é¢è¿™ä¸¤ä¸ªå¹³é¢çš„è·ç¦»æ‰€å†³å®šçš„ã€‚å½“ä½ æ”¹å˜å¯¹ç„¦è®¾ç½®æ—¶,ç›¸æœºä¸­çš„è¿™ä¸ªé€é•œä½ç½®å°±ä¼šå‘ç”Ÿæ”¹å˜(ä½ æ‰‹æœºä¸Šçš„æ‘„åƒå¤´ä¹Ÿæ˜¯è¿™ä¸ªåŸç†, åªä¸è¿‡é€é•œä¸åŠ¨, æ”¹æˆäº†æˆåƒä¼ æ„Ÿå™¨åŠ¨)ã€‚å¿«é—¨å…‰åœˆ(aperture)æ˜¯ä¸€ä¸ªå­”, å®ƒæ§åˆ¶è¿™å—é€é•œåº”è¯¥å¤šå¤§æ¯”è¾ƒå¥½ã€‚å¦‚æœä½ éœ€è¦æ›´å¤šçš„å…‰çº¿, ä½ çš„è¿™ä¸ªå¿«é—¨å…‰åœˆå°±å¤§ä¸€ç‚¹, æ™¯æ·±ä¹Ÿä¼šéšä¹‹åŠ å¤§ã€‚å¯¹äºä¸€ä¸ªè™šæ‹Ÿçš„æ‘„åƒæœºæ¥è¯´, æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªä¼ æ„Ÿå™¨å°±å¤Ÿäº†ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ä¼ å…¥å¿«é—¨å…‰åœˆçš„å¤§å°å°±è¡Œã€å³é€é•œå¤§å°ã€‘ã€‚



> A real camera has a complicated compound lens. For our code we could simulate the order: sensor, then lens, then aperture. Then we could figure out where to send the rays, and flip the image after it's computed (the image is projected upside down on the film). Graphics people, however, usually use a thin lens approximation:

ç°å®ä¸–ç•Œä¸­çš„æ‘„åƒæœºçš„é€é•œç»„æ˜¯å¾ˆå¤æ‚çš„ã€‚ä½†å¯¹äºæˆ‘ä»¬å†™ä»£ç æ¥è¯´, æˆ‘ä»¬åªéœ€è¦æ¨¡æ‹Ÿä¸Šè¿°çš„é¡ºåº: å›¾åƒä¼ æ„Ÿå™¨, é€é•œ, å¿«é—¨, ç„¶åå°„å‡ºå…‰çº¿, æœ€åè®°å¾—ç¿»è½¬å›¾ç‰‡(è¿›è¿‡é€é•œæˆåƒä¼šè¢«ä¸Šä¸‹ç¿»è½¬)ã€‚å›¾å½¢å­¦ä¸­äººä»¬å¸¸å¸¸ä½¿ç”¨ä¸€å—è–„ç‰‡é€é•œè¿‘ä¼¼æ¨¡æ‹Ÿ:



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.17-cam-lens.jpg)



> We donâ€™t need to simulate any of the inside of the camera. For the purposes of rendering an image outside the camera, that would be unnecessary complexity. Instead, I usually start rays from the lens, and send them toward the focus plane (`focus_dist` away from the lens), where everything on that plane is in perfect focus.

ä½†æ˜¯æˆ‘ä»¬æ ¹æœ¬ä¸ç”¨æ¨¡æ‹Ÿä»»ä½•æ‘„åƒæœºå†…éƒ¨çš„ä¸œè¥¿, å¯¹äºæˆ‘ä»¬æ¸²æŸ“æ‘„åƒæœºå¤–çš„ç‰©ä½“æ¥è¯´, è¿™äº›éƒ½æ²¡å¿…è¦ã€‚æˆ‘ä»¬åªè¦ä»ä¸€ä¸ªè™šæ‹Ÿçš„é€é•œèŒƒå›´ä¸­å‘å°„å…‰çº¿åˆ°æˆ‘ä»¬çš„æ‘„åƒæœºå¹³é¢å°±èƒ½æ¨¡æ‹Ÿäº†,è¿™ä¸ªé€é•œä¸å¹³é¢çš„è·ç¦»æˆä¸ºç„¦è·(`focus_dist`)

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/fig-1.18-cam-film-plane.jpg)



> Normally, all scene rays originate from the `lookfrom` point. In order to accomplish defocus blur, generate random scene rays originating from inside a disk centered at the `lookfrom` point. The larger the radius, the greater the defocus blur. You can think of our original camera as having a defocus disk of radius zero (no blur at all), so all rays originated at the disk center (`lookfrom`).

ä¹‹å‰æˆ‘ä»¬æ‰€æœ‰çš„å…‰çº¿éƒ½æ˜¯ä»`lookfrom`å‘å‡ºçš„, ä½†ç°åœ¨åŠ å…¥äº†æ•£ç„¦æ¨¡ç³Š, æ‰€æœ‰å…‰çº¿éƒ½ä»å†…éƒ¨çš„ä¸€ä¸ªè™šæ‹Ÿé€é•œå‘å‡º, ç»è¿‡`lookfrom`ç‚¹, è¿™ä¸ªé€é•œçš„åŠå¾„è¶Šå¤§, å›¾åƒå°±è¶Šæ¨¡ç³Šã€‚ä½ å¯ä»¥è®¤ä¸ºä¹‹å‰çš„æ‘„åƒæœº, è¿™ä¸ªåŠå¾„ä¸º0ã€‚

```c++
vec3 random_in_unit_disk() {
    while (true) {
        auto p = vec3(random_double(-1,1), random_double(-1,1), 0);
        if (p.length_squared() >= 1) continue;
        return p;
    }
}
```





ä¸‹é¢ç»™å‡ºå®Œæ•´çš„ `camera.h`ï¼š

```diff
#ifndef RAYTRACING_DEV_CAMERA_H
#define RAYTRACING_DEV_CAMERA_H

#include "rtweekend.h"

class camera {
public:
    camera() {
        lower_left_corner = vec3(-2.0,-1.0,-1.0);
        horizontal = vec3(4.0,0.0,0.0);
        vertical = vec3(0.0,2.0,0.0);
        origin = vec3(0.0,0.0,0.0);
    }

    camera(vec3 lookfrom, vec3 lookat, vec3 vup,
+           double vfov, double aspect, double aperture, double focus_dist) { // top to bottom, in degress
        origin = lookfrom;
+       lens_radius = aperture / 2;

        auto theta = degree_to_radians(vfov);
        auto half_height = tan(theta / 2);
        auto half_width = aspect * half_height;
        w = unit_vector(lookfrom - lookat);
        u = unit_vector(cross(vup, w));
        v = cross(w, u);

-       lower_left_corner = origin - half_width * u - half_height * v - w;
+       lower_left_corner = origin
+                         - half_width * focus_dist * u
+                         - half_height * focus_dist * v
+                         - focus_dist * w;

-       horizontal = 2 * half_width * u;
-       vertical = 2 * half_height * v;
+       horizontal = 2 * half_width * focus_dist * u;
+       vertical = 2 * half_height * focus_dist * v;
    }

+   ray get_ray(double s, double t) {
+       vec3 rd = lens_radius * random_in_unit_disk();
+       vec3 offset = u * rd.x() + v * rd.y();

+       return ray(
+               origin + offset,
+               lower_left_corner + s * horizontal + t * vertical - origin - offset
+               );
+   }

public:
    vec3 origin;
    vec3 lower_left_corner;
    vec3 horizontal;
    vec3 vertical;
+   vec3 u, v, w;
+   double lens_radius;
};

#endif //RAYTRACING_DEV_CAMERA_H
```





> Using a big aperture:

æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¤§å¤§çš„å¿«é—¨å…‰åœˆ:

```c++
const auto aspect_ratio = double(image_width) / image_height;
...
vec3 lookfrom(3,3,2);
vec3 lookat(0,0,-1);
vec3 vup(0,1,0);
auto dist_to_focus = (lookfrom-lookat).length();
auto aperture = 2.0;

camera cam(lookfrom, lookat, vup, 20, aspect_ratio, aperture, dist_to_focus);
```





> We get:

We get:

![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.20-depth-of-field.png)



### æ¥ä¸‹æ¥å­¦ä»€ä¹ˆ

é¦–å…ˆæˆ‘ä»¬åœ¨ `main.cpp` æŠŠä¹¦çš„å°é¢å›¾â€”â€”è®¸å¤šè®¸å¤šçš„éšæœºçƒæ¸²æŸ“å‡ºæ¥:

```c++
hittable_list random_scene() {
    hittable_list world;

    world.add(make_shared<sphere>(
        vec3(0,-1000,0), 1000, make_shared<lambertian>(vec3(0.5, 0.5, 0.5))));

    int i = 1;
    for (int a = -11; a < 11; a++) {
        for (int b = -11; b < 11; b++) {
            auto choose_mat = random_double();
            vec3 center(a + 0.9*random_double(), 0.2, b + 0.9*random_double());
            if ((center - vec3(4, 0.2, 0)).length() > 0.9) {
                if (choose_mat < 0.8) {
                    // diffuse
                    auto albedo = vec3::random() * vec3::random();
                    world.add(
                        make_shared<sphere>(center, 0.2, make_shared<lambertian>(albedo)));
                } else if (choose_mat < 0.95) {
                    // metal
                    auto albedo = vec3::random(.5, 1);
                    auto fuzz = random_double(0, .5);
                    world.add(
                        make_shared<sphere>(center, 0.2, make_shared<metal>(albedo, fuzz)));
                } else {
                    // glass
                    world.add(make_shared<sphere>(center, 0.2, make_shared<dielectric>(1.5)));
                }
            }
        }
    }

    world.add(make_shared<sphere>(vec3(0, 1, 0), 1.0, make_shared<dielectric>(1.5)));

    world.add(
        make_shared<sphere>(vec3(-4, 1, 0), 1.0, make_shared<lambertian>(vec3(0.4, 0.2, 0.1))));

    world.add(
        make_shared<sphere>(vec3(4, 1, 0), 1.0, make_shared<metal>(vec3(0.7, 0.6, 0.5), 0.0)));

    return world;
}

int main() {
    ...
    auto world = random_scene();

    vec3 lookfrom(13,2,3);
    vec3 lookat(0,0,0);
    vec3 vup(0,1,0);
    auto dist_to_focus = 10.0;
    auto aperture = 0.1;

    camera cam(lookfrom, lookat, vup, 20, aspect_ratio, aperture, dist_to_focus);
    ...
}
```



![](https://cdn.jsdelivr.net/gh/Yousazoe/picgo-repo/img/img-1.21-book1-final.jpg)

ä½ å¯èƒ½ä¼šå‘ç°ç»ç’ƒçƒæ²¡æœ‰é˜´å½±, ä½¿å¾—ä»–ä»¬çœ‹ä¸Šå»åƒæ¼‚æµ®åœ¨ç©ºä¸­ä¼¼å¾—ã€‚è¿™ä¸æ˜¯bug(ä½ åœ¨ç°å®ä¸–ç•Œä¸­å¾ˆå°‘æœ‰æœºä¼šè§åˆ°çœŸæ­£çš„ç»ç’ƒçƒ, å®ƒä»¬çœ‹èµ·æ¥çš„ç¡®å°±æ˜¯è¿™æ ·çš„)ã€‚ç»ç’ƒçƒä¸‹çš„é‚£ä¸ªä½œä¸ºåœ°æ¿çš„å¤§çƒä»ç„¶èƒ½è¢«é‚£ä¹ˆå¤šå…‰çº¿å‡»ä¸­, ç»ç’ƒçƒä¸‹çš„é‚£ä¸ªä½œä¸ºåœ°æ¿çš„å¤§çƒä»ç„¶èƒ½è¢«é‚£ä¹ˆå¤šå…‰çº¿å‡»ä¸­, å› ä¸ºå…‰çº¿å¹¶ä¸ä¼šè¢«ç»ç’ƒçƒé˜»æŒ¡ï¼Œç»ç”±ç»ç’ƒçƒçš„æŠ˜å°„æœ€ç»ˆå°„å‘å¤©ç©ºã€‚ã€the sky is re-ordered rather than blocked. æ„Ÿè°¢è¯„è®ºåŒº[Kanichiyaoba](https://www.zhihu.com/people/iioo-95) çš„ç¿»è¯‘è§£ç­”ã€‘

ç°åœ¨ä½ æ‹¥æœ‰ä¸€ä¸ªcooooolæ¯™äº†çš„å…‰çº¿è¿½è¸ªå™¨äº†! é‚£æ¥ä¸‹æ¥æˆ‘è¯¥ä½•å»ä½•ä»å‘¢?ã€æ ‡*ä¸º[ä¸‹æœ¬ä¹¦](https://oxine.github.io/Graphic/Ray-tracing-the-next-week/)ä¸­çš„å†…å®¹ã€‘

1. å…‰ç…§ã€‚ä½ å¯ä»¥ä½¿ç”¨é˜´å½±å…‰çº¿æ¥æ˜¾å¼å®ç°è¿™éƒ¨åˆ†, ä¹Ÿå¯ä»¥ä½¿ç”¨äº§ç”Ÿå…‰çº¿çš„æè´¨æ¥éšå¼å®ç°*ã€‚

2. åç§»æ•£å°„å…‰çº¿, ç„¶åé™ä½è¿™äº›å…‰çº¿çš„æƒé‡æ¥æ¶ˆé™¤åç§»ã€‚è¿™ä¸¤ç§éƒ½è¡Œã€‚ç¡¬è¦è¯´çš„è¯, æˆ‘åå‘åè€…ä¸€ç‚¹ç‚¹ã€‚ã€æˆ‘çŒœè¿™å¥è¯æ˜¯åœ¨è¯´æ¶ˆé™¤è‡ªç›¸äº¤æ‰€å¯¼è‡´çš„é˜´å½± å³Shadow Ance, å¦‚æœæœ‰äººçŸ¥é“è¿™æ˜¯åœ¨è¯´ä»€ä¹ˆè¯·æ•™æ•™æˆ‘å§ï¼ã€‘

3. åŠ å…¥ä¸‰è§’å½¢ã€‚å¤§éƒ¨åˆ†æ¨¡å‹éƒ½æ˜¯ä¸‰è§’ç½‘æ ¼ã€‚æ¨¡å‹çš„IOéƒ¨åˆ†æ˜¯æœ€æ¶å¿ƒçš„, åŸºæœ¬ä¸Šæ‰€æœ‰äººéƒ½ä¸æƒ³è‡ªå·±å†™, éƒ½å»æ‰¾åˆ«äººçš„ä»£ç ç”¨ã€‚

4. è¡¨é¢çº¹ç†*ã€‚è¿™å¯ä»¥è®©ä½ åƒè´´å¢™çº¸ä¸€æ ·æŠŠå›¾ç‰‡è´´åˆ°ç‰©ä½“ä¸Šå»ã€‚å®ç°èµ·æ¥ä¹Ÿå¾ˆç®€å•ã€‚

5. å›ºä½“çº¹ç†*ã€‚å¯ä»¥å‚è§Ken Perlinçš„åœ¨çº¿ä»£ç , Andrew Kenslerçš„blogä¸­ä¹Ÿæœ‰å…³äºè¿™éƒ¨åˆ†çš„ä¿¡æ¯ã€‚

6. ä½“ç§¯ä½“(volumes å³é›¾ç­‰)*ä¸å…¶ä»–ä»‹è´¨ã€‚å¾ˆCool, ä½†æ˜¯ä¼šæ”¹å˜ä½ çš„ä»£ç æ„ç­‘ã€‚æˆ‘å–œæ¬¢æŠŠä½“ç§¯ä½“ä¹Ÿè®¾è®¡æˆhittableçš„å­ç±», æ ¹æ®å…¶å¯†åº¦æ¥éšæœºå†³å®šå…‰çº¿æ˜¯å¦ä¸å…¶ç›¸äº¤ã€‚ä½¿ç”¨è¿™ä¸ªæ–¹æ³•, ä½ çš„æ¸²æŸ“å™¨ç”šè‡³ä¸ç”¨çŸ¥é“ä½ æ¸²çš„æ˜¯ä½“ç§¯ä½“å°±æ¸²å‡ºæ¥äº†ã€‚

7. å¹¶è¡Œä¼˜åŒ–ã€‚ä½¿ç”¨ä¸åŒçš„éšæœºç§å­, æŠŠä½ çš„ä»£ç å¤åˆ¶ä¸ŠNä»½è·‘åœ¨Nä¸ªæ ¸å¿ƒä¸Š,ç„¶åå†æ±‚å¹³å‡å€¼ã€‚ä½ å¯ä»¥åˆ†å±‚æ¥å®Œæˆè¿™éƒ¨åˆ†å·¥ä½œ, æ¯”å¦‚åˆ†æˆN/2å¯¹, æ¯æ¬¡å¹³å‡æ±‚å‡ºN/4ã€ä¸ºä»€ä¹ˆæ˜¯N/4å•Šï¼Ÿï¼Ÿè¿™ç¿»è¯‘ç¿»ä¸ä¸‹å»äº†ï¼ã€‘çš„å›¾ç‰‡, ç„¶ååœ¨å¯¹è¿™äº›å¯¹ä¹‹é—´æ±‚å¹³å‡å€¼ã€‚è¿™åº”è¯¥ç”¨ä¸äº†å¤šå°‘ä»£ç ã€è¯•è¯•CUDAå§ã€‘ã€‚

è®°å¾—æŠŠä½ æ¸²æŸ“å‡ºçš„ç‚«é…·å›¾ç‰‡å‘ç»™æˆ‘!ç¥ä½ æ„‰å¿«!

